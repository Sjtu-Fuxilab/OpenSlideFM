{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# OP_FM — Script 1: Environment, Paths, and Compute-Passport\n",
    "\n",
    "import os, sys, json, time, math, platform, shutil, socket, datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "# ------------------------------- USER PATHS -------------------------------\n",
    "# READ-ONLY: your TCGA WSI root (no writes will ever be performed here)\n",
    "WSI_ROOT = Path(r\"D:\\个人文件夹\\Sanwal\\DL_V2\\Histo slides 20k\")\n",
    "\n",
    "# WORKSPACE: all pipeline outputs go here (and only here)\n",
    "WORKSPACE = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\")\n",
    "\n",
    "# ----------------------------- SUBFOLDER LAYOUT ---------------------------\n",
    "SUBDIRS = {\n",
    "    \"compute\": WORKSPACE / \"compute\",\n",
    "    \"logs\": WORKSPACE / \"logs\",\n",
    "    \"figures\": WORKSPACE / \"figures\",\n",
    "    \"qc\": WORKSPACE / \"qc\",\n",
    "    \"tiles\": WORKSPACE / \"tiles\",\n",
    "    \"features\": WORKSPACE / \"features\",\n",
    "    \"embeddings\": WORKSPACE / \"embeddings\",\n",
    "    \"attn\": WORKSPACE / \"attn\",\n",
    "    \"leak_audit\": WORKSPACE / \"leak_audit\",\n",
    "    \"preanalytics\": WORKSPACE / \"preanalytics\",\n",
    "    \"artifacts\": WORKSPACE / \"artifacts\",\n",
    "    \"manifests\": WORKSPACE / \"manifests\",\n",
    "    \"hashes\": WORKSPACE / \"hashes\",\n",
    "    \"ckpt\": WORKSPACE / \"ckpt\",\n",
    "}\n",
    "\n",
    "# Create folders in workspace (and only workspace)\n",
    "for name, p in SUBDIRS.items():\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------- IMPORTS ----------------------------------\n",
    "OPENS = None\n",
    "PIL_Image = None\n",
    "TORCH = None\n",
    "\n",
    "def safe_imports():\n",
    "    \"\"\"Import optional deps gracefully; keep the notebook runnable.\"\"\"\n",
    "    global OPENS, PIL_Image, TORCH\n",
    "    try:\n",
    "        import openslide\n",
    "        OPENS = openslide\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] openslide-python not available:\", e)\n",
    "        OPENS = None\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        PIL_Image = Image\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Pillow (PIL) not available:\", e)\n",
    "        PIL_Image = None\n",
    "    try:\n",
    "        import torch\n",
    "        TORCH = torch\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] PyTorch not available:\", e)\n",
    "        TORCH = None\n",
    "\n",
    "safe_imports()\n",
    "\n",
    "# ------------------------------- HELPERS ----------------------------------\n",
    "def now_iso() -> str:\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def gb(nbytes: int) -> float:\n",
    "    return round(nbytes / (1024**3), 2)\n",
    "\n",
    "def safe_write_json(path: Path, obj: Dict[str, Any]) -> None:\n",
    "    \"\"\"Atomic-ish write for JSON (write .tmp then replace).\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def list_wsi_files(root: Path):\n",
    "    \"\"\"Find slides by typical extensions (case-insensitive).\"\"\"\n",
    "    if not root.exists():\n",
    "        return []\n",
    "    exts = (\".svs\", \".tif\", \".tiff\", \".ndpi\", \".mrxs\", \".scn\")\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files.extend(root.rglob(f\"*{ext}\"))\n",
    "        files.extend(root.rglob(f\"*{ext.upper()}\"))\n",
    "    return sorted(set(files))\n",
    "\n",
    "def open_and_probe_wsi(path: Path):\n",
    "    \"\"\"\n",
    "    Return (width, height, mpp_x, mpp_y, vendor, thumb_path or None).\n",
    "    Reads WSI read-only; writes thumbnail only under WORKSPACE/figures.\n",
    "    \"\"\"\n",
    "    if OPENS is None:\n",
    "        return None\n",
    "    slide = OPENS.OpenSlide(str(path))\n",
    "    props = slide.properties\n",
    "    w, h = slide.dimensions\n",
    "    mpp_x = props.get('openslide.mpp-x') or props.get('aperio.MPP') or None\n",
    "    mpp_y = props.get('openslide.mpp-y') or props.get('aperio.MPP') or None\n",
    "    vendor = props.get('openslide.vendor') or props.get('aperio.AppMag') or 'unknown'\n",
    "\n",
    "    # Write a small thumbnail into WORKSPACE/figures (proof-of-life)\n",
    "    thumb_path = None\n",
    "    try:\n",
    "        if hasattr(slide, \"get_thumbnail\") and PIL_Image is not None:\n",
    "            max_side = 768\n",
    "            scale = max(w, h) / max_side if max(w, h) > max_side else 1.0\n",
    "            tw, th = int(w/scale), int(h/scale)\n",
    "            thumb = slide.get_thumbnail((tw, th))\n",
    "            thumb_path = SUBDIRS[\"figures\"] / f\"sample_thumb_{path.stem}.jpg\"\n",
    "            thumb.save(str(thumb_path), \"JPEG\", quality=90)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not write thumbnail for {path.name}: {e}\")\n",
    "        thumb_path = None\n",
    "    finally:\n",
    "        slide.close()\n",
    "\n",
    "    return (w, h, mpp_x, mpp_y, vendor, thumb_path)\n",
    "\n",
    "# ----------------------------- ENV SUMMARY --------------------------------\n",
    "def get_env_summary() -> Dict[str, Any]:\n",
    "    info = {\n",
    "        \"timestamp\": now_iso(),\n",
    "        \"host\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "        \"workspace\": str(WORKSPACE),\n",
    "        \"wsi_root\": str(WSI_ROOT),\n",
    "        \"paths_note\": \"All writes occur ONLY under 'workspace'. 'wsi_root' is read-only.\",\n",
    "    }\n",
    "    # Disk at workspace\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(WORKSPACE)\n",
    "        info.update({\n",
    "            \"disk_total_gb\": gb(total),\n",
    "            \"disk_used_gb\": gb(used),\n",
    "            \"disk_free_gb\": gb(free),\n",
    "        })\n",
    "    except Exception as e:\n",
    "        info[\"disk_error\"] = str(e)\n",
    "\n",
    "    # Torch / CUDA\n",
    "    if TORCH is not None:\n",
    "        info[\"torch_version\"] = TORCH.__version__\n",
    "        info[\"cuda_available\"] = TORCH.cuda.is_available()\n",
    "        if TORCH.cuda.is_available():\n",
    "            try:\n",
    "                dev = TORCH.cuda.current_device()\n",
    "                prop = TORCH.cuda.get_device_properties(dev)\n",
    "                info[\"cuda_device\"] = {\n",
    "                    \"index\": dev,\n",
    "                    \"name\": prop.name,\n",
    "                    \"total_vram_gb\": round(prop.total_memory / (1024**3), 2),\n",
    "                    \"multi_processor_count\": getattr(prop, \"multi_processor_count\", None),\n",
    "                }\n",
    "                info[\"cuda_runtime_version\"] = TORCH.version.cuda\n",
    "                info[\"cudnn_version\"] = TORCH.backends.cudnn.version()\n",
    "            except Exception as e:\n",
    "                info[\"cuda_error\"] = str(e)\n",
    "    else:\n",
    "        info[\"torch_version\"] = None\n",
    "\n",
    "    # OpenSlide\n",
    "    info[\"openslide_version\"] = getattr(OPENS, \"__version__\", None) if OPENS else None\n",
    "    return info\n",
    "\n",
    "# ------------------------------ RUNTIME START -----------------------------\n",
    "print(\"== OP_FM Script 1: Environment & Compute-Passport ==\")\n",
    "print(f\"[{now_iso()}] Workspace: {WORKSPACE}\")\n",
    "print(f\"[{now_iso()}] WSI Root (read-only): {WSI_ROOT}\")\n",
    "\n",
    "if not WSI_ROOT.exists():\n",
    "    print(f\"[WARN] WSI_ROOT does not exist yet: {WSI_ROOT}\")\n",
    "\n",
    "# Save environment summary & compute-passport init\n",
    "env = get_env_summary()\n",
    "\n",
    "compute_passport = {\n",
    "    \"run_id\": datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"created_at\": now_iso(),\n",
    "    \"workspace\": str(WORKSPACE),\n",
    "    \"wsi_root\": str(WSI_ROOT),\n",
    "    \"environment\": env,\n",
    "    \"stages\": [],   # subsequent scripts will append stage entries here\n",
    "}\n",
    "\n",
    "compute_path = SUBDIRS[\"compute\"] / \"compute_passport.json\"\n",
    "safe_write_json(compute_path, compute_passport)\n",
    "print(f\"[OK] Compute-Passport initialized at: {compute_path}\")\n",
    "\n",
    "# ----------------------- SANITY: FIND & PROBE ONE WSI ---------------------\n",
    "wsi_files = list_wsi_files(WSI_ROOT)\n",
    "print(f\"[INFO] Detected {len(wsi_files)} WSI files in WSI_ROOT.\")\n",
    "\n",
    "sample_report: Dict[str, Any] = {}\n",
    "if wsi_files and OPENS is not None:\n",
    "    # deterministic sample (first sorted) for reproducibility\n",
    "    sample_path = wsi_files[0]\n",
    "    try:\n",
    "        probe = open_and_probe_wsi(sample_path)\n",
    "        if probe:\n",
    "            w, h, mpp_x, mpp_y, vendor, thumb_path = probe\n",
    "            sample_report = {\n",
    "                \"slide_path\": str(sample_path),\n",
    "                \"width\": w, \"height\": h,\n",
    "                \"mpp_x\": mpp_x, \"mpp_y\": mpp_y,\n",
    "                \"vendor\": vendor,\n",
    "                \"thumbnail\": str(thumb_path) if thumb_path else None,\n",
    "            }\n",
    "            print(\"\\n== Sample WSI Probe ==\")\n",
    "            print(\" Path   :\", sample_report[\"slide_path\"])\n",
    "            print(\" Size   :\", f\"{w} x {h}\")\n",
    "            print(\" MPP    :\", f\"x={mpp_x}  y={mpp_y}\")\n",
    "            print(\" Vendor :\", vendor)\n",
    "            print(\" Thumb  :\", sample_report['thumbnail'] or \"(not created)\")\n",
    "        else:\n",
    "            print(\"[WARN] OpenSlide not available; skipping probe.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not open sample slide: {e}\")\n",
    "else:\n",
    "    if not wsi_files:\n",
    "        print(\"[INFO] No WSI files detected (check WSI_ROOT path).\")\n",
    "    if OPENS is None:\n",
    "        print(\"[WARN] openslide-python missing; install:\\n  pip install openslide-python\\nand system OpenSlide libs.\")\n",
    "\n",
    "# ----------------------- LOG SUMMARY TO WORKSPACE -------------------------\n",
    "log = {\n",
    "    \"timestamp\": now_iso(),\n",
    "    \"env\": env,\n",
    "    \"sample_probe\": sample_report,\n",
    "}\n",
    "log_path = SUBDIRS[\"logs\"] / \"env_summary.json\"\n",
    "safe_write_json(log_path, log)\n",
    "print(f\"[OK] Environment summary written to: {log_path}\")\n",
    "\n",
    "# Human-readable TXT for Methods appendix\n",
    "txt_path = SUBDIRS[\"logs\"] / \"env_summary.txt\"\n",
    "with txt_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"OP_FM — Environment Summary\\n\")\n",
    "    f.write(f\"Timestamp: {now_iso()}\\n\\n\")\n",
    "    for k, v in env.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "    if sample_report:\n",
    "        f.write(\"\\nSample WSI Probe:\\n\")\n",
    "        for k, v in sample_report.items():\n",
    "            f.write(f\"  {k}: {v}\\n\")\n",
    "print(f\"[OK] Human-readable summary written to: {txt_path}\")\n",
    "\n",
    "# ----------------------- Diagnostics Checklist ------------------------\n",
    "print(\"\\n== Diagnostics Checklist (Script 1) ==\")\n",
    "print(\" - [\", \"OK\" if OPENS else \"!!\", \"] openslide-python import\")\n",
    "print(\" - [\", \"OK\" if PIL_Image else \"!!\", \"] Pillow import\")\n",
    "if TORCH:\n",
    "    cuda_line = f\"CUDA={TORCH.cuda.is_available()}\"\n",
    "    dev_line = \"\"\n",
    "    if TORCH.cuda.is_available():\n",
    "        try:\n",
    "            prop = TORCH.cuda.get_device_properties(0)\n",
    "            dev_line = f\" | GPU={prop.name} VRAM={round(prop.total_memory/(1024**3),2)} GB\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\" - [ OK ] PyTorch\", TORCH.__version__, cuda_line, dev_line)\n",
    "else:\n",
    "    print(\" - [ !! ] PyTorch not available\")\n",
    "print(\" - [ OK ] All outputs confined to:\", WORKSPACE)\n",
    "print(\" - [ INFO ] Compute-Passport at:\", compute_path)\n",
    "print(\" - [ INFO ] Logs at:\", log_path, \"and\", txt_path)\n",
    "print(\"\\nScript 1 complete. Proceed to Script 2 (Manifest & Provenance).\")\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# OP_FM — Script 2: Dataset Manifest & Provenance \n",
    "import os, sys, json, time, math, hashlib, traceback, datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Optional heavy deps (graceful if missing)\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Please install pandas and numpy (pip install pandas numpy)\") from e\n",
    "\n",
    "# Matplotlib only (no seaborn as per your rules)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reuse imports/vars from Script 1\n",
    "assert 'WSI_ROOT' in globals() and 'WORKSPACE' in globals() and 'SUBDIRS' in globals(), \\\n",
    "    \"Please run Script 1 first to define WSI_ROOT/WORKSPACE/SUBDIRS.\"\n",
    "\n",
    "# ---------------------------- Config knobs ----------------------------\n",
    "MANIFEST_OUT = SUBDIRS[\"manifests\"] / \"manifest_tcga.parquet\"\n",
    "MANIFEST_CSV = SUBDIRS[\"manifests\"] / \"manifest_tcga.csv\"\n",
    "FAILED_CSV   = SUBDIRS[\"manifests\"] / \"failed_slides.csv\"\n",
    "HASH_INDEX   = SUBDIRS[\"hashes\"]   / \"hash_index_tcga.csv\"\n",
    "\n",
    "# Fast fingerprint mode: \"size_only\" | \"sha1_quick\" | \"sha1_full\"\n",
    "# - \"sha1_quick\": hash first 8 MiB + last 8 MiB + size (fast & stable for dedup)\n",
    "# - \"sha1_full\":  hash whole file (very slow on 20k WSIs)\n",
    "# - \"size_only\":  just uses file size (weak dedup; fastest)\n",
    "CHECKSUM_MODE = \"sha1_quick\"\n",
    "\n",
    "# Concurrency (opening WSIs and reading small file regions in parallel)\n",
    "MAX_WORKERS = min(12, (os.cpu_count() or 8))\n",
    "\n",
    "# ---------------------------- Utilities ----------------------------\n",
    "def now_iso():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def file_times(p: Path):\n",
    "    st = p.stat()\n",
    "    # Windows returns st_ctime as \"creation\" time\n",
    "    created = datetime.datetime.fromtimestamp(getattr(st, \"st_ctime\", st.st_mtime)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    modified = datetime.datetime.fromtimestamp(st.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return created, modified\n",
    "\n",
    "def quick_fingerprint(path: Path, mode=\"sha1_quick\", chunk=8*1024*1024):\n",
    "    \"\"\"Return (fingerprint, sha1_full_or_None).\"\"\"\n",
    "    size = path.stat().st_size\n",
    "    if mode == \"size_only\":\n",
    "        return f\"SIZE:{size}\", None\n",
    "\n",
    "    if mode == \"sha1_quick\":\n",
    "        h = hashlib.sha1()\n",
    "        with path.open(\"rb\") as f:\n",
    "            # first chunk\n",
    "            h.update(f.read(chunk))\n",
    "            # last chunk\n",
    "            if size > chunk:\n",
    "                f.seek(max(size - chunk, 0))\n",
    "                h.update(f.read(chunk))\n",
    "        h.update(str(size).encode(\"utf-8\"))\n",
    "        return f\"QSHA1:{h.hexdigest()}\", None\n",
    "\n",
    "    if mode == \"sha1_full\":\n",
    "        h = hashlib.sha1()\n",
    "        with path.open(\"rb\") as f:\n",
    "            while True:\n",
    "                b = f.read(1024*1024)\n",
    "                if not b:\n",
    "                    break\n",
    "                h.update(b)\n",
    "        return f\"SHA1:{h.hexdigest()}\", h.hexdigest()\n",
    "\n",
    "    raise ValueError(f\"Unknown CHECKSUM_MODE: {mode}\")\n",
    "\n",
    "def list_wsi_files(root: Path):\n",
    "    exts = (\".svs\", \".tif\", \".tiff\", \".ndpi\", \".mrxs\", \".scn\")\n",
    "    out = []\n",
    "    for ext in exts:\n",
    "        out.extend(root.rglob(f\"*{ext}\"))\n",
    "        out.extend(root.rglob(f\"*{ext.upper()}\"))\n",
    "    # deduplicate\n",
    "    return sorted(set(out))\n",
    "\n",
    "def cancer_code_from_path(p: Path, root: Path):\n",
    "    rel = p.relative_to(root)\n",
    "    # Expect structure: <CANCER_CODE>/<filename>\n",
    "    return rel.parts[0] if len(rel.parts) >= 2 else \"UNKNOWN\"\n",
    "\n",
    "def open_and_probe(path: Path):\n",
    "    \"\"\"Open WSI with openslide to get dimensions + properties.\"\"\"\n",
    "    import openslide  # local import to isolate any import errors\n",
    "    slide = openslide.OpenSlide(str(path))\n",
    "    props = slide.properties\n",
    "    width, height = slide.dimensions\n",
    "    level_count = slide.level_count\n",
    "\n",
    "    # Try to read common metadata keys\n",
    "    mpp_x = props.get('openslide.mpp-x') or props.get('aperio.MPP') or None\n",
    "    mpp_y = props.get('openslide.mpp-y') or props.get('aperio.MPP') or None\n",
    "    vendor = props.get('openslide.vendor') or 'unknown'\n",
    "    obj_pow = props.get('aperio.AppMag') or props.get('openslide.objective-power') or None\n",
    "\n",
    "    slide.close()\n",
    "    return {\n",
    "        \"width\": int(width),\n",
    "        \"height\": int(height),\n",
    "        \"level_count\": int(level_count),\n",
    "        \"mpp_x\": float(mpp_x) if mpp_x not in (None, \"\") else None,\n",
    "        \"mpp_y\": float(mpp_y) if mpp_y not in (None, \"\") else None,\n",
    "        \"vendor\": str(vendor),\n",
    "        \"objective_power\": float(obj_pow) if (obj_pow is not None and str(obj_pow).replace('.','',1).isdigit()) else str(obj_pow) if obj_pow else None,\n",
    "    }\n",
    "\n",
    "# ---------------------------- Scan & collect ----------------------------\n",
    "start = time.time()\n",
    "print(f\"== OP_FM Script 2: Manifest & Provenance ==\\n[{now_iso()}] Scanning WSI root (read-only): {WSI_ROOT}\")\n",
    "\n",
    "slides = list_wsi_files(WSI_ROOT)\n",
    "n_total = len(slides)\n",
    "print(f\"[INFO] Found {n_total} candidate WSI files.\")\n",
    "\n",
    "records = []\n",
    "failures = []\n",
    "\n",
    "def process_one(path: Path):\n",
    "    rec = {\n",
    "        \"path\": str(path),\n",
    "        \"filename\": path.name,\n",
    "        \"slide_id\": path.stem,  # generic; downstream can parse TCGA ids if needed\n",
    "        \"cancer_code\": cancer_code_from_path(path, WSI_ROOT),\n",
    "        \"size_bytes\": path.stat().st_size,\n",
    "    }\n",
    "    # timestamps\n",
    "    created, modified = file_times(path)\n",
    "    rec[\"created_time\"] = created\n",
    "    rec[\"modified_time\"] = modified\n",
    "\n",
    "    # checksum / fingerprint\n",
    "    try:\n",
    "        fp, sha1_full = quick_fingerprint(path, mode=CHECKSUM_MODE)\n",
    "        rec[\"fingerprint\"] = fp\n",
    "        rec[\"sha1_full\"] = sha1_full\n",
    "    except Exception as e:\n",
    "        rec[\"fingerprint\"] = None\n",
    "        rec[\"sha1_full\"] = None\n",
    "\n",
    "    # attempt to open and read properties\n",
    "    try:\n",
    "        meta = open_and_probe(path)\n",
    "        rec.update(meta)\n",
    "        rec[\"error\"] = None\n",
    "    except Exception as e:\n",
    "        rec.update({\n",
    "            \"width\": None, \"height\": None, \"level_count\": None,\n",
    "            \"mpp_x\": None, \"mpp_y\": None, \"vendor\": None, \"objective_power\": None,\n",
    "            \"error\": f\"{e.__class__.__name__}: {e}\"\n",
    "        })\n",
    "    return rec\n",
    "\n",
    "# Parallel pass\n",
    "t0 = time.time()\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {ex.submit(process_one, p): p for p in slides}\n",
    "    done = 0\n",
    "    last_print = t0\n",
    "    for fut in as_completed(futures):\n",
    "        rec = fut.result()\n",
    "        records.append(rec)\n",
    "        if rec.get(\"error\"):\n",
    "            failures.append({\"path\": rec[\"path\"], \"error\": rec[\"error\"]})\n",
    "        done += 1\n",
    "        # light progress print every ~2 seconds\n",
    "        now = time.time()\n",
    "        if now - last_print > 2 or done == n_total:\n",
    "            rate = done / (now - t0 + 1e-9)\n",
    "            print(f\"[SCAN] {done}/{n_total} ({rate:.1f} files/s)\")\n",
    "            last_print = now\n",
    "\n",
    "elapsed_scan = time.time() - start\n",
    "print(f\"[OK] Scanned {n_total} slides in {elapsed_scan/60:.1f} min.\")\n",
    "\n",
    "# ---------------------------- DataFrame & save ----------------------------\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# Ensure consistent types\n",
    "num_cols = [\"size_bytes\", \"width\", \"height\", \"level_count\", \"mpp_x\", \"mpp_y\"]\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Save manifest (Parquet + CSV)\n",
    "SUBDIRS[\"manifests\"].mkdir(parents=True, exist_ok=True)\n",
    "df.to_parquet(MANIFEST_OUT, index=False)\n",
    "df.to_csv(MANIFEST_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] Manifest saved:\\n - {MANIFEST_OUT}\\n - {MANIFEST_CSV}\")\n",
    "\n",
    "# Save failures (if any)\n",
    "if failures:\n",
    "    pd.DataFrame(failures).to_csv(FAILED_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[WARN] {len(failures)} slides failed to open; see {FAILED_CSV}\")\n",
    "\n",
    "# Save a light hash index (path, size, fingerprint) for quick dedup debugging\n",
    "pd.DataFrame(df[[\"path\", \"size_bytes\", \"fingerprint\"]]).to_csv(HASH_INDEX, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] Hash index written: {HASH_INDEX}\")\n",
    "\n",
    "# ---------------------------- Diagnostics (print) ----------------------------\n",
    "print(\"\\n== Diagnostics (Manifest) ==\")\n",
    "total_bytes = df[\"size_bytes\"].sum(skipna=True)\n",
    "print(f\" Total slides: {len(df):,}\")\n",
    "print(f\" Total size  : {total_bytes / (1024**3):.2f} GB\")\n",
    "by_cancer = df[\"cancer_code\"].value_counts(dropna=False)\n",
    "print(\"\\n Slides by cancer_code (top 20):\")\n",
    "print(by_cancer.head(20).to_string())\n",
    "\n",
    "missing_mpp = df[(df[\"mpp_x\"].isna()) | (df[\"mpp_y\"].isna())]\n",
    "print(f\"\\n Missing MPP entries: {len(missing_mpp)}\")\n",
    "\n",
    "# Duplicate detection (by fingerprint if available, else by size+filename)\n",
    "dup_key = \"fingerprint\" if df[\"fingerprint\"].notna().any() else None\n",
    "if dup_key:\n",
    "    dup_groups = df.groupby(dup_key).size().sort_values(ascending=False)\n",
    "    dup_groups = dup_groups[dup_groups > 1]\n",
    "    print(f\"\\n Potential duplicates by {dup_key}: {int(dup_groups.sum()) - len(dup_groups)} extra files in {len(dup_groups)} groups\")\n",
    "else:\n",
    "    size_groups = df.groupby([\"size_bytes\", \"filename\"]).size().sort_values(ascending=False)\n",
    "    size_groups = size_groups[size_groups > 1]\n",
    "    print(f\"\\n Potential duplicates by (size, filename): {int(size_groups.sum()) - len(size_groups)} extra files in {len(size_groups)} groups\")\n",
    "\n",
    "# Top-N largest slides\n",
    "topN = df.sort_values(\"size_bytes\", ascending=False).head(10)[[\"filename\", \"cancer_code\", \"size_bytes\"]].copy()\n",
    "topN[\"size_gb\"] = topN[\"size_bytes\"] / (1024**3)\n",
    "print(\"\\n Top-10 largest WSIs (GB):\")\n",
    "print(topN[[\"filename\", \"cancer_code\", \"size_gb\"]].to_string(index=False, float_format=lambda x: f\"{x:.2f}\"))\n",
    "\n",
    "# ---------------------------- Diagnostics (plots to file) ----------------------------\n",
    "fig_dir = SUBDIRS[\"figures\"]\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Size distribution (GB)\n",
    "plt.figure(figsize=(8,5))\n",
    "sizes_gb = (df[\"size_bytes\"] / (1024**3)).dropna()\n",
    "plt.hist(sizes_gb.values, bins=40)\n",
    "plt.xlabel(\"Slide size (GB)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"WSI Size Distribution (TCGA)\")\n",
    "plt.tight_layout()\n",
    "p1 = fig_dir / \"manifest_size_distribution.png\"\n",
    "plt.savefig(p1)\n",
    "plt.close()\n",
    "print(f\"[FIG] {p1}\")\n",
    "\n",
    "# 2) Width/Height distributions (log10)\n",
    "plt.figure(figsize=(8,5))\n",
    "wh = df[[\"width\", \"height\"]].dropna()\n",
    "vals = np.log10(wh.values.clip(min=1))\n",
    "plt.hist(vals.flatten(), bins=40)\n",
    "plt.xlabel(\"log10(pixels)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"WSI Width/Height Distribution (log10)\")\n",
    "plt.tight_layout()\n",
    "p2 = fig_dir / \"manifest_wh_log_distribution.png\"\n",
    "plt.savefig(p2)\n",
    "plt.close()\n",
    "print(f\"[FIG] {p2}\")\n",
    "\n",
    "# 3) Slides by cancer_code (bar, top 30)\n",
    "plt.figure(figsize=(10,6))\n",
    "top_codes = by_cancer.head(30)\n",
    "plt.bar(top_codes.index.astype(str), top_codes.values)\n",
    "plt.xticks(rotation=80, ha=\"right\")\n",
    "plt.ylabel(\"Slides\")\n",
    "plt.title(\"Slides per cancer_code (Top 30)\")\n",
    "plt.tight_layout()\n",
    "p3 = fig_dir / \"manifest_counts_by_cancer.png\"\n",
    "plt.savefig(p3)\n",
    "plt.close()\n",
    "print(f\"[FIG] {p3}\")\n",
    "\n",
    "# 4) MPP completeness (% with both mpp_x & mpp_y)\n",
    "mpp_complete = df[\"mpp_x\"].notna() & df[\"mpp_y\"].notna()\n",
    "pct_mpp = 100.0 * mpp_complete.mean()\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.bar([\"MPP complete\", \"MPP missing\"], [pct_mpp, 100.0 - pct_mpp])\n",
    "plt.title(\"MPP Availability (%)\")\n",
    "plt.tight_layout()\n",
    "p4 = fig_dir / \"manifest_mpp_availability.png\"\n",
    "plt.savefig(p4)\n",
    "plt.close()\n",
    "print(f\"[FIG] {p4} (MPP complete: {pct_mpp:.1f}%)\")\n",
    "\n",
    "# ---------------------------- Append compute-passport ----------------------------\n",
    "compute_path = SUBDIRS[\"compute\"] / \"compute_passport.json\"\n",
    "try:\n",
    "    with compute_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        cp = json.load(f)\n",
    "except Exception:\n",
    "    cp = {\"stages\": []}\n",
    "\n",
    "stage_entry = {\n",
    "    \"stage\": \"manifest_tcga\",\n",
    "    \"timestamp\": now_iso(),\n",
    "    \"inputs\": {\"wsi_root\": str(WSI_ROOT)},\n",
    "    \"outputs\": {\n",
    "        \"manifest_parquet\": str(MANIFEST_OUT),\n",
    "        \"manifest_csv\": str(MANIFEST_CSV),\n",
    "        \"failed_csv\": str(FAILED_CSV) if failures else None,\n",
    "        \"hash_index_csv\": str(HASH_INDEX),\n",
    "        \"figures\": [str(p1), str(p2), str(p3), str(p4)],\n",
    "    },\n",
    "    \"stats\": {\n",
    "        \"n_files_found\": int(n_total),\n",
    "        \"n_records\": int(len(df)),\n",
    "        \"n_failures\": int(len(failures)),\n",
    "        \"total_gb\": float(total_bytes / (1024**3)),\n",
    "        \"elapsed_minutes\": float(elapsed_scan / 60.0),\n",
    "        \"checksum_mode\": CHECKSUM_MODE,\n",
    "    }\n",
    "}\n",
    "cp.setdefault(\"stages\", []).append(stage_entry)\n",
    "\n",
    "# Write back atomically\n",
    "tmp = compute_path.with_suffix(\".json.tmp\")\n",
    "with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cp, f, ensure_ascii=False, indent=2)\n",
    "tmp.replace(compute_path)\n",
    "print(f\"\\n[OK] Compute-Passport updated: {compute_path}\")\n",
    "\n",
    "print(\"\\nScript 2 complete. Next: Script 3 (QC & Tissue Mask).\")\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# OP_FM — Script 3: QC & Tissue Mask (metrics, exclusions, figures)\n",
    "\n",
    "import os, sys, time, json, math, traceback, datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local import (avoid global hard fail if missing — you already used it in Script 1)\n",
    "try:\n",
    "    import openslide\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"openslide-python is required for Script 3. Install and rerun.\") from e\n",
    "\n",
    "# ----------------------------- Inputs & Outputs -----------------------------\n",
    "assert 'WSI_ROOT' in globals() and 'WORKSPACE' in globals() and 'SUBDIRS' in globals(), \\\n",
    "    \"Please run Script 1 first to define WSI_ROOT/WORKSPACE/SUBDIRS.\"\n",
    "\n",
    "MANIFEST_PARQUET = SUBDIRS[\"manifests\"] / \"manifest_tcga.parquet\"\n",
    "assert MANIFEST_PARQUET.exists(), f\"Manifest not found at {MANIFEST_PARQUET}. Run Script 2 first.\"\n",
    "\n",
    "QC_METRICS_PARQUET = SUBDIRS[\"qc\"] / \"qc_metrics_tcga.parquet\"\n",
    "QC_METRICS_CSV     = SUBDIRS[\"qc\"] / \"qc_metrics_tcga.csv\"\n",
    "QC_EXCLUSIONS_CSV  = SUBDIRS[\"qc\"] / \"exclusions_tcga.csv\"\n",
    "QC_THUMBS_DIR      = SUBDIRS[\"qc\"] / \"thumbs\"\n",
    "\n",
    "FIG_DIR = SUBDIRS[\"figures\"]\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUBDIRS[\"qc\"].mkdir(parents=True, exist_ok=True)\n",
    "QC_THUMBS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------- Config knobs --------------------------------\n",
    "# Set to None to process all slides; set to a small number for a smoke test\n",
    "QC_MAX_SLIDES = None  # e.g., 500\n",
    "\n",
    "# Thumbnail target (max dimension in pixels). Larger = more accurate, slower.\n",
    "THUMB_MAX_SIDE = 1024\n",
    "\n",
    "# QC thresholds (tune if you need stricter/looser gating)\n",
    "MIN_TISSUE_PCT   = 0.10   # exclude if < 10% tissue\n",
    "MAX_WHITE_PCT    = 0.75   # exclude if > 75% white background\n",
    "MIN_BLUR_VAR     = 15.0   # exclude if Laplacian var < 15 (thumbnail-level)\n",
    "MAX_PEN_PCT      = 0.02   # exclude if > 2% pen/ink (blue-ish, high saturation)\n",
    "\n",
    "# HSV gates (0..255 space from PIL)\n",
    "HSV_S_TISSUE_MIN = 20     # tissue tends to have some saturation\n",
    "HSV_V_WHITE_MIN  = 230    # very bright ~white pixels (V high)\n",
    "HSV_S_WHITE_MAX  = 30     # near-white has low saturation\n",
    "# Blue ink (pen) heuristic in HSV:\n",
    "#   - Hue roughly in 180..255 (on PIL 0..255 scale; ~ 255 ~ 360°)\n",
    "#   - Saturation high to avoid white/gray\n",
    "HSV_H_BLUE_MIN   = 170\n",
    "HSV_H_BLUE_MAX   = 255\n",
    "HSV_S_PEN_MIN    = 60\n",
    "\n",
    "# Concurrency for QC (safe to run multiple readers; each opens its own slide)\n",
    "MAX_WORKERS = min(8, (os.cpu_count() or 8))\n",
    "\n",
    "# ----------------------------- Helpers -------------------------------------\n",
    "def now_iso():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def load_thumbnail(slide_path: Path, max_side: int = 1024) -> Image.Image:\n",
    "    \"\"\"Open slide and return a PIL thumbnail with max_side dimension.\"\"\"\n",
    "    slide = openslide.OpenSlide(str(slide_path))\n",
    "    w, h = slide.dimensions\n",
    "    scale = max(w, h) / max_side if max(w, h) > max_side else 1.0\n",
    "    tw, th = int(w / scale), int(h / scale)\n",
    "    thumb = slide.get_thumbnail((tw, th)).convert(\"RGB\")\n",
    "    slide.close()\n",
    "    return thumb\n",
    "\n",
    "def to_hsv_np(img_rgb: Image.Image):\n",
    "    \"\"\"Return HSV uint8 arrays (H,S,V in 0..255) from an RGB PIL image.\"\"\"\n",
    "    hsv = img_rgb.convert(\"HSV\")\n",
    "    a = np.array(hsv, dtype=np.uint8)\n",
    "    H, S, V = a[..., 0], a[..., 1], a[..., 2]\n",
    "    return H, S, V\n",
    "\n",
    "def laplacian_var(gray_u8: np.ndarray, tissue_mask: np.ndarray = None) -> float:\n",
    "    \"\"\"Variance of 3x3 Laplacian (manual conv) over uint8 grayscale. Optionally restricted to tissue.\"\"\"\n",
    "    # 3x3 kernel:\n",
    "    #  0  1  0\n",
    "    #  1 -4  1\n",
    "    #  0  1  0\n",
    "    g = gray_u8.astype(np.float32)\n",
    "    # pad edges\n",
    "    p = np.pad(g, 1, mode=\"reflect\")\n",
    "    c  = -4 * p[1:-1, 1:-1]\n",
    "    n  = 1 * (p[:-2, 1:-1] + p[2:, 1:-1] + p[1:-1, :-2] + p[1:-1, 2:])\n",
    "    lap = c + n\n",
    "    if tissue_mask is not None:\n",
    "        mask = tissue_mask.astype(bool)\n",
    "        if mask.sum() == 0:\n",
    "            return 0.0\n",
    "        vals = lap[mask]\n",
    "    else:\n",
    "        vals = lap.ravel()\n",
    "    # variance\n",
    "    return float(np.var(vals))\n",
    "\n",
    "def qc_on_thumbnail(img: Image.Image):\n",
    "    \"\"\"Compute QC metrics on a thumbnail image.\"\"\"\n",
    "    # RGB -> HSV\n",
    "    H, S, V = to_hsv_np(img)\n",
    "    # grayscale for blur\n",
    "    gray = np.array(img.convert(\"L\"), dtype=np.uint8)\n",
    "\n",
    "    # Tissue mask: S high AND not pure white\n",
    "    tissue_mask = (S >= HSV_S_TISSUE_MIN) & (V < HSV_V_WHITE_MIN)\n",
    "\n",
    "    # White mask: very bright with low saturation\n",
    "    white_mask = (V >= HSV_V_WHITE_MIN) & (S <= HSV_S_WHITE_MAX)\n",
    "\n",
    "    # Pen mask: blue-ish + saturated\n",
    "    pen_mask = (H >= HSV_H_BLUE_MIN) & (H <= HSV_H_BLUE_MAX) & (S >= HSV_S_PEN_MIN)\n",
    "\n",
    "    total = img.size[0] * img.size[1]\n",
    "    tissue_pct = float(tissue_mask.sum() / total)\n",
    "    white_pct  = float(white_mask.sum() / total)\n",
    "    pen_pct    = float(pen_mask.sum() / total)\n",
    "\n",
    "    # Blur (variance of Laplacian) only on tissue\n",
    "    blur_val = laplacian_var(gray, tissue_mask)\n",
    "\n",
    "    # Simple stats inside tissue\n",
    "    if tissue_mask.sum() > 0:\n",
    "        brightness_mean = float(V[tissue_mask].mean())\n",
    "        saturation_mean = float(S[tissue_mask].mean())\n",
    "    else:\n",
    "        brightness_mean = float(V.mean())\n",
    "        saturation_mean = float(S.mean())\n",
    "\n",
    "    return {\n",
    "        \"tissue_pct\": tissue_pct,\n",
    "        \"white_pct\": white_pct,\n",
    "        \"pen_pct\": pen_pct,\n",
    "        \"blur_var\": blur_val,\n",
    "        \"brightness_mean\": brightness_mean,\n",
    "        \"saturation_mean\": saturation_mean,\n",
    "    }, tissue_mask\n",
    "\n",
    "def qc_reason_flags(m, thresholds):\n",
    "    \"\"\"Return list of exclusion reasons (strings). Empty list => keep.\"\"\"\n",
    "    reasons = []\n",
    "    if m[\"tissue_pct\"] < thresholds[\"min_tissue_pct\"]:\n",
    "        reasons.append(f\"low_tissue<{thresholds['min_tissue_pct']:.2f}\")\n",
    "    if m[\"white_pct\"] > thresholds[\"max_white_pct\"]:\n",
    "        reasons.append(f\"white>{thresholds['max_white_pct']:.2f}\")\n",
    "    if m[\"blur_var\"] < thresholds[\"min_blur_var\"]:\n",
    "        reasons.append(f\"blur<{thresholds['min_blur_var']:.1f}\")\n",
    "    if m[\"pen_pct\"] > thresholds[\"max_pen_pct\"]:\n",
    "        reasons.append(f\"pen>{thresholds['max_pen_pct']:.2f}\")\n",
    "    return reasons\n",
    "\n",
    "def save_thumb_and_mask(slide_id: str, img: Image.Image, tissue_mask: np.ndarray):\n",
    "    \"\"\"Save plain thumbnail and a quick tissue overlay for audit.\"\"\"\n",
    "    # Save plain thumb (JPEG)\n",
    "    thumb_path = QC_THUMBS_DIR / f\"{slide_id}_thumb.jpg\"\n",
    "    img.save(str(thumb_path), \"JPEG\", quality=90)\n",
    "\n",
    "    # Save overlay (red mask)\n",
    "    overlay = np.array(img).copy()\n",
    "    red = np.zeros_like(overlay)\n",
    "    red[..., 0] = 255\n",
    "    alpha = 0.35\n",
    "    mask3 = np.stack([tissue_mask]*3, axis=-1)\n",
    "    overlay = (overlay * (~mask3) + (alpha * overlay + (1 - alpha) * red) * mask3).astype(np.uint8)\n",
    "    overlay_img = Image.fromarray(overlay)\n",
    "    overlay_path = QC_THUMBS_DIR / f\"{slide_id}_overlay.jpg\"\n",
    "    overlay_img.save(str(overlay_path), \"JPEG\", quality=90)\n",
    "\n",
    "    return str(thumb_path), str(overlay_path)\n",
    "\n",
    "# ----------------------------- Load manifest --------------------------------\n",
    "print(f\"== OP_FM Script 3: QC & Tissue Mask ==\\n[{now_iso()}] Loading manifest:\", MANIFEST_PARQUET)\n",
    "df_manifest = pd.read_parquet(MANIFEST_PARQUET)\n",
    "df_manifest = df_manifest.copy()\n",
    "\n",
    "if QC_MAX_SLIDES is not None:\n",
    "    df_manifest = df_manifest.head(QC_MAX_SLIDES).copy()\n",
    "print(f\"[INFO] Slides to QC: {len(df_manifest)}\")\n",
    "\n",
    "# ----------------------------- Run QC (multi-thread) ------------------------\n",
    "thresholds = {\n",
    "    \"min_tissue_pct\": MIN_TISSUE_PCT,\n",
    "    \"max_white_pct\": MAX_WHITE_PCT,\n",
    "    \"min_blur_var\": MIN_BLUR_VAR,\n",
    "    \"max_pen_pct\": MAX_PEN_PCT,\n",
    "}\n",
    "\n",
    "results = []\n",
    "failures = []\n",
    "t_start = time.time()\n",
    "\n",
    "def worker(row):\n",
    "    slide_path = Path(row[\"path\"])\n",
    "    slide_id   = str(row[\"slide_id\"])\n",
    "    cancer     = str(row.get(\"cancer_code\", \"UNKNOWN\"))\n",
    "    try:\n",
    "        img = load_thumbnail(slide_path, THUMB_MAX_SIDE)\n",
    "        metrics, tissue_mask = qc_on_thumbnail(img)\n",
    "        reasons = qc_reason_flags(metrics, thresholds)\n",
    "        thumb_p, overlay_p = save_thumb_and_mask(slide_id, img, tissue_mask)\n",
    "        rec = {\n",
    "            \"slide_id\": slide_id,\n",
    "            \"cancer_code\": cancer,\n",
    "            \"path\": str(slide_path),\n",
    "            \"tissue_pct\": metrics[\"tissue_pct\"],\n",
    "            \"white_pct\": metrics[\"white_pct\"],\n",
    "            \"pen_pct\": metrics[\"pen_pct\"],\n",
    "            \"blur_var\": metrics[\"blur_var\"],\n",
    "            \"brightness_mean\": metrics[\"brightness_mean\"],\n",
    "            \"saturation_mean\": metrics[\"saturation_mean\"],\n",
    "            \"excluded\": int(len(reasons) > 0),\n",
    "            \"reasons\": \";\".join(reasons) if reasons else \"\",\n",
    "            \"thumb\": thumb_p,\n",
    "            \"overlay\": overlay_p,\n",
    "        }\n",
    "        return True, rec\n",
    "    except Exception as e:\n",
    "        return False, {\"slide_id\": slide_id, \"path\": str(slide_path), \"error\": f\"{e.__class__.__name__}: {e}\"}\n",
    "\n",
    "done = 0\n",
    "last_print = time.time()\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futs = [ex.submit(worker, row) for _, row in df_manifest.iterrows()]\n",
    "    for fut in as_completed(futs):\n",
    "        ok, payload = fut.result()\n",
    "        if ok:\n",
    "            results.append(payload)\n",
    "        else:\n",
    "            failures.append(payload)\n",
    "        done += 1\n",
    "        now = time.time()\n",
    "        if now - last_print > 2 or done == len(df_manifest):\n",
    "            rate = done / (now - t_start + 1e-9)\n",
    "            print(f\"[QC] {done}/{len(df_manifest)} ({rate:.1f} slides/s)\")\n",
    "            last_print = now\n",
    "\n",
    "elapsed = time.time() - t_start\n",
    "print(f\"[OK] QC completed in {elapsed/60:.1f} min.\")\n",
    "\n",
    "# ----------------------------- Save metrics & exclusions --------------------\n",
    "df_qc = pd.DataFrame.from_records(results)\n",
    "df_qc = df_qc.sort_values(\"slide_id\").reset_index(drop=True)\n",
    "df_qc.to_parquet(QC_METRICS_PARQUET, index=False)\n",
    "df_qc.to_csv(QC_METRICS_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] QC metrics saved:\\n - {QC_METRICS_PARQUET}\\n - {QC_METRICS_CSV}\")\n",
    "\n",
    "if failures:\n",
    "    df_fail = pd.DataFrame(failures)\n",
    "    fail_path = SUBDIRS[\"qc\"] / \"qc_failures.csv\"\n",
    "    df_fail.to_csv(fail_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[WARN] {len(failures)} slides failed during QC; see {fail_path}\")\n",
    "\n",
    "# Exclusions file\n",
    "df_excl = df_qc[df_qc[\"excluded\"] == 1].copy()\n",
    "df_excl.to_csv(QC_EXCLUSIONS_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] Exclusions written: {QC_EXCLUSIONS_CSV} (n={len(df_excl)})\")\n",
    "\n",
    "# ----------------------------- Diagnostics & Figures -----------------------\n",
    "# 1) Summary prints\n",
    "n_total = len(df_qc)\n",
    "n_excl  = len(df_excl)\n",
    "print(\"\\n== QC Summary ==\")\n",
    "print(f\" Total slides QC'd : {n_total:,}\")\n",
    "print(f\" Excluded          : {n_excl:,} ({100.0*n_excl/max(1,n_total):.1f}%)\")\n",
    "\n",
    "by_reason = (df_excl[\"reasons\"].str.split(\";\", expand=True)\n",
    "             .stack().str.strip().value_counts())\n",
    "print(\"\\n Exclusions by reason:\")\n",
    "print(by_reason.to_string())\n",
    "\n",
    "by_cancer_excl = df_excl[\"cancer_code\"].value_counts()\n",
    "print(\"\\n Exclusions by cancer_code (top 20):\")\n",
    "print(by_cancer_excl.head(20).to_string())\n",
    "\n",
    "# 2) Histograms of QC metrics\n",
    "def hist_plot(series, title, xlabel, outname, bins=40):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    vals = series.dropna().values\n",
    "    plt.hist(vals, bins=bins)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    outp = FIG_DIR / outname\n",
    "    plt.savefig(outp)\n",
    "    plt.close()\n",
    "    print(f\"[FIG] {outp}\")\n",
    "    return str(outp)\n",
    "\n",
    "figs = []\n",
    "figs.append(hist_plot(df_qc[\"tissue_pct\"],     \"Tissue % (thumbnail)\", \"tissue_pct\", \"qc_tissue_pct_hist.png\"))\n",
    "figs.append(hist_plot(df_qc[\"white_pct\"],      \"White % (thumbnail)\",  \"white_pct\",  \"qc_white_pct_hist.png\"))\n",
    "figs.append(hist_plot(df_qc[\"pen_pct\"],        \"Pen/ink %\",            \"pen_pct\",    \"qc_pen_pct_hist.png\"))\n",
    "figs.append(hist_plot(df_qc[\"blur_var\"],       \"Blur variance\",        \"blur_var\",   \"qc_blur_var_hist.png\"))\n",
    "figs.append(hist_plot(df_qc[\"brightness_mean\"],\"Brightness (mean)\",    \"V_mean\",     \"qc_brightness_mean_hist.png\"))\n",
    "figs.append(hist_plot(df_qc[\"saturation_mean\"],\"Saturation (mean)\",    \"S_mean\",     \"qc_saturation_mean_hist.png\"))\n",
    "\n",
    "# 3) Exclusion reason bar\n",
    "plt.figure(figsize=(10,6))\n",
    "x = by_reason.index.tolist()\n",
    "y = by_reason.values.tolist()\n",
    "plt.bar(x, y)\n",
    "plt.xticks(rotation=70, ha=\"right\")\n",
    "plt.ylabel(\"Excluded slides\")\n",
    "plt.title(\"Exclusions by reason\")\n",
    "plt.tight_layout()\n",
    "p_exr = FIG_DIR / \"qc_exclusions_by_reason.png\"\n",
    "plt.savefig(p_exr)\n",
    "plt.close()\n",
    "print(f\"[FIG] {p_exr}\")\n",
    "\n",
    "# 4) Exclusion rate by cancer_code (top 30)\n",
    "rates = (df_excl[\"cancer_code\"].value_counts() / df_qc[\"cancer_code\"].value_counts()).fillna(0)\n",
    "rates = rates.sort_values(ascending=False)\n",
    "plt.figure(figsize=(12,6))\n",
    "top_rates = rates.head(30)\n",
    "plt.bar(top_rates.index.astype(str), (100.0*top_rates.values))\n",
    "plt.xticks(rotation=70, ha=\"right\")\n",
    "plt.ylabel(\"Exclusion rate (%)\")\n",
    "plt.title(\"Exclusion rate by cancer_code (Top 30)\")\n",
    "plt.tight_layout()\n",
    "p_exrate = FIG_DIR / \"qc_exclusion_rate_by_cancer.png\"\n",
    "plt.savefig(p_exrate)\n",
    "plt.close()\n",
    "print(f\"[FIG] {p_exrate}\")\n",
    "\n",
    "# ----------------------------- Update compute-passport ---------------------\n",
    "compute_path = SUBDIRS[\"compute\"] / \"compute_passport.json\"\n",
    "try:\n",
    "    with compute_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        cp = json.load(f)\n",
    "except Exception:\n",
    "    cp = {\"stages\": []}\n",
    "\n",
    "stage_entry = {\n",
    "    \"stage\": \"qc_tcga\",\n",
    "    \"timestamp\": now_iso(),\n",
    "    \"inputs\": {\n",
    "        \"manifest_parquet\": str(MANIFEST_PARQUET),\n",
    "        \"thumb_max_side\": THUMB_MAX_SIDE,\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"qc_metrics_parquet\": str(QC_METRICS_PARQUET),\n",
    "        \"qc_metrics_csv\": str(QC_METRICS_CSV),\n",
    "        \"qc_exclusions_csv\": str(QC_EXCLUSIONS_CSV),\n",
    "        \"qc_thumbs_dir\": str(QC_THUMBS_DIR),\n",
    "        \"figures\": figs + [str(p_exr), str(p_exrate)],\n",
    "    },\n",
    "    \"thresholds\": {\n",
    "        \"min_tissue_pct\": MIN_TISSUE_PCT,\n",
    "        \"max_white_pct\": MAX_WHITE_PCT,\n",
    "        \"min_blur_var\": MIN_BLUR_VAR,\n",
    "        \"max_pen_pct\": MAX_PEN_PCT,\n",
    "        \"hsv\": {\n",
    "            \"S_tissue_min\": HSV_S_TISSUE_MIN,\n",
    "            \"V_white_min\": HSV_V_WHITE_MIN,\n",
    "            \"S_white_max\": HSV_S_WHITE_MAX,\n",
    "            \"H_blue_min\": HSV_H_BLUE_MIN,\n",
    "            \"H_blue_max\": HSV_H_BLUE_MAX,\n",
    "            \"S_pen_min\": HSV_S_PEN_MIN,\n",
    "        }\n",
    "    },\n",
    "    \"stats\": {\n",
    "        \"n_qc\": int(n_total),\n",
    "        \"n_excluded\": int(n_excl),\n",
    "        \"elapsed_minutes\": float(elapsed / 60.0),\n",
    "        \"failures\": int(len(failures)),\n",
    "        \"workers\": MAX_WORKERS,\n",
    "    }\n",
    "}\n",
    "cp.setdefault(\"stages\", []).append(stage_entry)\n",
    "\n",
    "tmp = compute_path.with_suffix(\".json.tmp\")\n",
    "with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cp, f, ensure_ascii=False, indent=2)\n",
    "tmp.replace(compute_path)\n",
    "print(f\"\\n[OK] Compute-Passport updated: {compute_path}\")\n",
    "\n",
    "print(\"\\nScript 3 complete. Next: Script 4 (Two-scale tiling & token budget).\")\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# OP_FM — Script 4: Two-scale Tiling & Token Budget \n",
    "\n",
    "import os, sys, math, json, time, random, datetime, traceback\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import openslide\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"openslide-python is required for tiling. Install and rerun.\") from e\n",
    "\n",
    "# ----------------------------- Prereqs -----------------------------\n",
    "assert 'WSI_ROOT' in globals() and 'WORKSPACE' in globals() and 'SUBDIRS' in globals(), \\\n",
    "    \"Please run Script 1 first to define WSI_ROOT/WORKSPACE/SUBDIRS.\"\n",
    "\n",
    "MANIFEST_PARQUET = SUBDIRS[\"manifests\"] / \"manifest_tcga.parquet\"\n",
    "QC_METRICS_PARQUET = SUBDIRS[\"qc\"] / \"qc_metrics_tcga.parquet\"\n",
    "TILES_DIR = SUBDIRS[\"tiles\"] / \"manifests\"\n",
    "TILES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR = SUBDIRS[\"figures\"]\n",
    "\n",
    "# ----------------------------- Config ------------------------------\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# QC policy: 'strict' | 'medium' | 'lenient' | 'none'\n",
    "QC_POLICY = \"medium\"  # default, safer after the aggressive pen hits\n",
    "\n",
    "# Target scales (μm/px)\n",
    "TARGET_SCALES = [0.5, 2.0]\n",
    "\n",
    "# Tile geometry\n",
    "TILE_SIZE = 256     # pixels at the chosen level\n",
    "OVERLAP   = 32      # pixels\n",
    "STRIDE    = TILE_SIZE - OVERLAP\n",
    "\n",
    "# Token budgets (per slide per scale)\n",
    "MAX_TOKENS = {0.5: 1200, 2.0: 400}\n",
    "\n",
    "# Tile acceptance\n",
    "MIN_TILE_TISSUE_COVERAGE = 0.30  # fraction of tile area that must be tissue\n",
    "\n",
    "# Low-res mask rendering for each slide (thumbnail)\n",
    "MASK_MAX_SIDE = 2048         # make a thumbnail up to this max side\n",
    "HSV_S_TISSUE_MIN = 20        # same basics as QC (scaled 0..255)\n",
    "HSV_V_WHITE_MIN  = 230\n",
    "\n",
    "# Sampling method for downselecting to budget: 'uniform' or 'variance_topk'\n",
    "SAMPLING_METHOD = \"uniform\"\n",
    "\n",
    "# Execution controls\n",
    "MAX_WORKERS = min(6, (os.cpu_count() or 8))  # worker = one slide at a time\n",
    "FORCE_REDO = False  # if True, re-generate even if manifest exists\n",
    "\n",
    "# Optional quick heatmaps (for 2 random slides)\n",
    "QUICK_HEATMAPS = True\n",
    "N_HEATMAPS = 2\n",
    "\n",
    "# ----------------------------- Helpers ------------------------------\n",
    "def now_iso():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def choose_level_for_target_mpp(slide, target_mpp, fallback_base_mpp=0.25):\n",
    "    \"\"\"Return (level, approx_mpp) closest to target μm/px.\"\"\"\n",
    "    props = slide.properties\n",
    "    base_mpp = None\n",
    "    # Try openslide props\n",
    "    for k in (\"openslide.mpp-x\", \"aperio.MPP\"):\n",
    "        if k in props:\n",
    "            try:\n",
    "                base_mpp = float(props.get(k))\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if base_mpp is None:\n",
    "        base_mpp = fallback_base_mpp  # fallback\n",
    "\n",
    "    best_level = 0\n",
    "    best_mpp = base_mpp\n",
    "    for lvl in range(slide.level_count):\n",
    "        mpp = base_mpp * slide.level_downsamples[lvl]\n",
    "        if abs(mpp - target_mpp) < abs(best_mpp - target_mpp):\n",
    "            best_mpp = mpp\n",
    "            best_level = lvl\n",
    "    return best_level, float(best_mpp)\n",
    "\n",
    "def make_tissue_mask(slide, max_side=MASK_MAX_SIDE):\n",
    "    \"\"\"Return RGB thumbnail and boolean tissue mask at thumbnail scale.\"\"\"\n",
    "    w, h = slide.dimensions\n",
    "    scale = max(w, h) / max_side if max(w, h) > max_side else 1.0\n",
    "    tw, th = int(w / scale), int(h / scale)\n",
    "    thumb = slide.get_thumbnail((tw, th)).convert(\"RGB\")\n",
    "\n",
    "    hsv = thumb.convert(\"HSV\")\n",
    "    a = np.array(hsv, dtype=np.uint8)\n",
    "    H, S, V = a[..., 0], a[..., 1], a[..., 2]\n",
    "    tissue = (S >= HSV_S_TISSUE_MIN) & (V < HSV_V_WHITE_MIN)\n",
    "    return thumb, tissue\n",
    "\n",
    "def grid_positions(level_w, level_h, tile=TILE_SIZE, stride=STRIDE):\n",
    "    xs = list(range(0, max(level_w - tile, 0) + 1, stride))\n",
    "    ys = list(range(0, max(level_h - tile, 0) + 1, stride))\n",
    "    return xs, ys\n",
    "\n",
    "def coverage_from_mask(mask, level, level_to_mask_scale, x, y, tile=TILE_SIZE):\n",
    "    \"\"\"\n",
    "    Estimate tissue coverage of the tile (x,y,level) using the low-res mask.\n",
    "    level_to_mask_scale = (sx, sy): multiplies level coords to mask coords.\n",
    "    \"\"\"\n",
    "    sx, sy = level_to_mask_scale\n",
    "    mx0, my0 = int(x * sx), int(y * sy)\n",
    "    mx1, my1 = int((x + tile) * sx), int((y + tile) * sy)\n",
    "    mx0, my0 = max(mx0, 0), max(my0, 0)\n",
    "    mx1, my1 = min(mx1, mask.shape[1]-1), min(my1, mask.shape[0]-1)\n",
    "    if mx1 <= mx0 or my1 <= my0:\n",
    "        return 0.0\n",
    "    roi = mask[my0:my1, mx0:mx1]\n",
    "    return float(roi.mean())  # True=1, False=0\n",
    "\n",
    "def sample_tiles_uniform(coords, k, rng):\n",
    "    if len(coords) <= k:\n",
    "        return coords\n",
    "    idx = rng.choice(len(coords), size=k, replace=False)\n",
    "    return [coords[i] for i in idx]\n",
    "\n",
    "# Optional variance-based sampler (needs quick per-tile Laplacian on low-res)\n",
    "def sample_tiles_variance(mask_rgb, coords, k, rng):\n",
    "    if len(coords) <= k:\n",
    "        return coords\n",
    "    # Simple variance proxy on grayscale thumbnail region (downscaled)\n",
    "    gray = np.array(mask_rgb.convert(\"L\"), dtype=np.float32) / 255.0\n",
    "    scores = []\n",
    "    for (x, y) in coords:\n",
    "        # Take a tiny patch around the mapped region center on the thumbnail\n",
    "        # This is a rough heuristic; we keep it light\n",
    "        cx, cy = int(x), int(y)\n",
    "        # Already in level coords — but we need to work in mask space; caller should pass coords in mask space for this method\n",
    "        # To keep Script 4 straightforward, we won’t use variance_topk by default.\n",
    "        scores.append(0.0)\n",
    "    # Fallback to uniform if not implemented\n",
    "    return sample_tiles_uniform(coords, k, rng)\n",
    "\n",
    "def write_parquet(df, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(path, index=False)\n",
    "\n",
    "# ----------------------------- Load inputs ------------------------------\n",
    "print(f\"== OP_FM Script 4: Two-scale Tiling & Token Budget ==\\n[{now_iso()}] Loading:\", MANIFEST_PARQUET)\n",
    "df_manifest = pd.read_parquet(MANIFEST_PARQUET)\n",
    "\n",
    "# Try to load QC metrics; optional\n",
    "df_qc = None\n",
    "if QC_POLICY in (\"strict\", \"medium\") and QC_METRICS_PARQUET.exists():\n",
    "    df_qc = pd.read_parquet(QC_METRICS_PARQUET)\n",
    "    df_qc = df_qc[[\"slide_id\", \"tissue_pct\", \"white_pct\", \"pen_pct\", \"reasons\"]].copy()\n",
    "\n",
    "# ----------------------------- Select slides ------------------------------\n",
    "if QC_POLICY == \"strict\" and df_qc is not None:\n",
    "    keep = df_manifest.merge(df_qc[[\"slide_id\", \"reasons\"]], on=\"slide_id\", how=\"left\")\n",
    "    keep = keep[keep[\"reasons\"].isna() | (keep[\"reasons\"] == \"\")]\n",
    "elif QC_POLICY == \"medium\" and df_qc is not None:\n",
    "    keep = df_manifest.merge(df_qc, on=\"slide_id\", how=\"left\")\n",
    "    # Ignore 'pen'; only exclude if clearly unusable\n",
    "    keep = keep[\n",
    "        (keep[\"tissue_pct\"].fillna(1.0) >= 0.05) &\n",
    "        (keep[\"white_pct\"].fillna(0.0) <= 0.95)\n",
    "    ].copy()\n",
    "else:\n",
    "    keep = df_manifest.copy()\n",
    "\n",
    "keep = keep.reset_index(drop=True)\n",
    "print(f\"[INFO] Slides selected under QC policy '{QC_POLICY}': {len(keep):,} out of {len(df_manifest):,}\")\n",
    "\n",
    "# ----------------------------- Per-slide worker ---------------------------\n",
    "def process_slide(row):\n",
    "    slide_path = Path(row[\"path\"])\n",
    "    slide_id   = str(row[\"slide_id\"])\n",
    "    cancer     = str(row.get(\"cancer_code\", \"UNKNOWN\"))\n",
    "\n",
    "    outputs = []\n",
    "    errors = []\n",
    "    try:\n",
    "        slide = openslide.OpenSlide(str(slide_path))\n",
    "    except Exception as e:\n",
    "        return slide_id, cancer, None, [f\"OpenSlideError: {e}\"]\n",
    "\n",
    "    # Build once per slide: thumbnail mask\n",
    "    try:\n",
    "        thumb_rgb, mask = make_tissue_mask(slide, MASK_MAX_SIDE)\n",
    "    except Exception as e:\n",
    "        slide.close()\n",
    "        return slide_id, cancer, None, [f\"MaskBuildError: {e}\"]\n",
    "\n",
    "    # Dimensions at each level\n",
    "    level_dims = [slide.level_dimensions[i] for i in range(slide.level_count)]\n",
    "    base_w, base_h = level_dims[0]\n",
    "\n",
    "    for target in TARGET_SCALES:\n",
    "        # output path\n",
    "        out_path = TILES_DIR / f\"{slide_id}_scale{str(target).replace('.','p')}.parquet\"\n",
    "        if out_path.exists() and not FORCE_REDO:\n",
    "            outputs.append({\"scale\": target, \"manifest\": str(out_path), \"n_tiles\": None, \"skipped\": True})\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            level, approx_mpp = choose_level_for_target_mpp(slide, target)\n",
    "            level_w, level_h = level_dims[level]\n",
    "            # mapping from level coords to mask coords\n",
    "            # mask is a thumbnail of base level; compute scale factors\n",
    "            # mask.shape = (th, tw), thumb corresponds to base (w,h) scaled\n",
    "            tw, th = thumb_rgb.size\n",
    "            sx = tw / base_w\n",
    "            sy = th / base_h\n",
    "            # level to base downsample\n",
    "            ds = slide.level_downsamples[level]\n",
    "            # final: level->mask multiply by (ds * s(mask/base))\n",
    "            level_to_mask_scale = (sx * ds, sy * ds)\n",
    "\n",
    "            xs, ys = grid_positions(level_w, level_h, TILE_SIZE, STRIDE)\n",
    "            # gather candidate coordinates with coverage check\n",
    "            cand = []\n",
    "            for y in ys:\n",
    "                for x in xs:\n",
    "                    cov = coverage_from_mask(mask, level, level_to_mask_scale, x, y, TILE_SIZE)\n",
    "                    if cov >= MIN_TILE_TISSUE_COVERAGE:\n",
    "                        cand.append((x, y))\n",
    "            n_cand = len(cand)\n",
    "\n",
    "            # Downselect to budget\n",
    "            budget = MAX_TOKENS.get(target, 0)\n",
    "            rng = np.random.default_rng(SEED + hash(slide_id) % (2**16) + int(target*100))\n",
    "            if SAMPLING_METHOD == \"uniform\":\n",
    "                chosen = sample_tiles_uniform(cand, budget, rng)\n",
    "            else:\n",
    "                chosen = sample_tiles_uniform(cand, budget, rng)  # keep uniform default\n",
    "\n",
    "            # Build dataframe\n",
    "            # Note: store mm-scale positional approximations if needed later (optional)\n",
    "            data = []\n",
    "            for idx, (x, y) in enumerate(chosen):\n",
    "                data.append({\n",
    "                    \"slide_id\": slide_id,\n",
    "                    \"cancer_code\": cancer,\n",
    "                    \"scale_um_per_px\": float(target),\n",
    "                    \"level\": int(level),\n",
    "                    \"x\": int(x),\n",
    "                    \"y\": int(y),\n",
    "                    \"tile_size\": TILE_SIZE,\n",
    "                    \"overlap\": OVERLAP,\n",
    "                    \"approx_mpp\": approx_mpp,\n",
    "                    \"tile_idx\": int(idx),\n",
    "                    \"seed\": int(SEED),\n",
    "                })\n",
    "            df_tiles = pd.DataFrame.from_records(data)\n",
    "\n",
    "            # Write manifest\n",
    "            write_parquet(df_tiles, out_path)\n",
    "            outputs.append({\"scale\": target, \"manifest\": str(out_path), \"n_tiles\": len(df_tiles), \"skipped\": False})\n",
    "\n",
    "        except Exception as e:\n",
    "            errors.append(f\"TilingError(scale={target}): {e}\")\n",
    "\n",
    "    slide.close()\n",
    "    return slide_id, cancer, outputs, errors\n",
    "\n",
    "# ----------------------------- Run (multi-slide) ---------------------------\n",
    "t0 = time.time()\n",
    "done = 0\n",
    "errors_all = []\n",
    "per_slide_counts = []\n",
    "\n",
    "print(f\"[{now_iso()}] Starting tiling on {len(keep)} slides with {MAX_WORKERS} workers...\")\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futs = [ex.submit(process_slide, row) for _, row in keep.iterrows()]\n",
    "    for fut in as_completed(futs):\n",
    "        slide_id, cancer, outputs, errs = fut.result()\n",
    "        done += 1\n",
    "        if errs:\n",
    "            for e in errs:\n",
    "                errors_all.append({\"slide_id\": slide_id, \"error\": e})\n",
    "        if outputs:\n",
    "            for rec in outputs:\n",
    "                if rec is None: \n",
    "                    continue\n",
    "                if not rec.get(\"skipped\", False):\n",
    "                    per_slide_counts.append({\n",
    "                        \"slide_id\": slide_id,\n",
    "                        \"cancer_code\": cancer,\n",
    "                        \"scale_um_per_px\": rec[\"scale\"],\n",
    "                        \"n_tiles\": rec[\"n_tiles\"],\n",
    "                        \"manifest\": rec[\"manifest\"],\n",
    "                    })\n",
    "        if done % 25 == 0 or done == len(keep):\n",
    "            rate = done / (time.time() - t0 + 1e-9)\n",
    "            print(f\"[TILING] {done}/{len(keep)} ({rate:.2f} slides/s)\")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"[OK] Tiling finished in {elapsed/60:.1f} min.\")\n",
    "\n",
    "# ----------------------------- Summaries & Figures -------------------------\n",
    "df_sum = pd.DataFrame.from_records(per_slide_counts)\n",
    "sum_path = SUBDIRS[\"tiles\"] / \"tiling_summary_tcga.parquet\"\n",
    "df_sum.to_parquet(sum_path, index=False)\n",
    "print(f\"[OK] Tiling summary: {sum_path}\")\n",
    "\n",
    "if errors_all:\n",
    "    df_err = pd.DataFrame.from_records(errors_all)\n",
    "    err_path = SUBDIRS[\"tiles\"] / \"tiling_errors_tcga.csv\"\n",
    "    df_err.to_csv(err_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[WARN] {len(df_err)} tiling errors logged at {err_path}\")\n",
    "\n",
    "# Figures: tokens/slide per scale\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for scale in TARGET_SCALES:\n",
    "    df_sc = df_sum[df_sum[\"scale_um_per_px\"] == scale]\n",
    "    if len(df_sc) == 0:\n",
    "        continue\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(df_sc[\"n_tiles\"].dropna().values, bins=40)\n",
    "    plt.xlabel(f\"Tokens per slide @ {scale} μm/px\")\n",
    "    plt.ylabel(\"Slides\")\n",
    "    plt.title(f\"Token Distribution @ {scale} μm/px\")\n",
    "    plt.tight_layout()\n",
    "    outp = FIG_DIR / f\"tiling_tokens_dist_scale{str(scale).replace('.','p')}.png\"\n",
    "    plt.savefig(outp)\n",
    "    plt.close()\n",
    "    print(f\"[FIG] {outp}\")\n",
    "\n",
    "# Optional quick heatmaps for a couple of slides\n",
    "if QUICK_HEATMAPS and len(df_sum) > 0:\n",
    "    sample_ids = df_sum[\"slide_id\"].drop_duplicates().sample(min(N_HEATMAPS, df_sum[\"slide_id\"].nunique()), random_state=SEED).tolist()\n",
    "    for sid in sample_ids:\n",
    "        try:\n",
    "            row0 = keep[keep[\"slide_id\"] == sid].iloc[0]\n",
    "            slide = openslide.OpenSlide(str(row0[\"path\"]))\n",
    "            thumb_rgb, mask = make_tissue_mask(slide, MASK_MAX_SIDE)\n",
    "            tw, th = thumb_rgb.size\n",
    "            # overlay sampled tile centers for 0.5 μm/px only (if present)\n",
    "            df_s = df_sum[(df_sum[\"slide_id\"] == sid) & (df_sum[\"scale_um_per_px\"] == 0.5)]\n",
    "            if len(df_s):\n",
    "                rec = df_s.iloc[0]\n",
    "                # Recompute level/mapping to draw tile centers\n",
    "                level, approx_mpp = choose_level_for_target_mpp(slide, 0.5)\n",
    "                base_w, base_h = slide.level_dimensions[0]\n",
    "                ds = slide.level_downsamples[level]\n",
    "                sx = tw / base_w\n",
    "                sy = th / base_h\n",
    "                level_to_mask_scale = (sx * ds, sy * ds)\n",
    "\n",
    "                # Load that slide's manifest\n",
    "                man_path = Path(rec[\"manifest\"])\n",
    "                df_tiles = pd.read_parquet(man_path)\n",
    "                # Draw centers\n",
    "                overlay = np.array(thumb_rgb).copy()\n",
    "                for _, t in df_tiles.iterrows():\n",
    "                    mx = int((t[\"x\"] + TILE_SIZE//2) * level_to_mask_scale[0])\n",
    "                    my = int((t[\"y\"] + TILE_SIZE//2) * level_to_mask_scale[1])\n",
    "                    if 0 <= mx < overlay.shape[1] and 0 <= my < overlay.shape[0]:\n",
    "                        # small dot\n",
    "                        y0, y1 = max(my-1,0), min(my+2, overlay.shape[0])\n",
    "                        x0, x1 = max(mx-1,0), min(mx+2, overlay.shape[1])\n",
    "                        overlay[y0:y1, x0:x1, :] = [255, 0, 0]\n",
    "                outp = FIG_DIR / f\"tiling_heatmap_{sid}.png\"\n",
    "                Image.fromarray(overlay).save(outp)\n",
    "                print(f\"[FIG] {outp}\")\n",
    "            slide.close()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Heatmap for {sid} failed: {e}\")\n",
    "\n",
    "# ----------------------------- Update compute-passport ---------------------\n",
    "compute_path = SUBDIRS[\"compute\"] / \"compute_passport.json\"\n",
    "try:\n",
    "    with compute_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        cp = json.load(f)\n",
    "except Exception:\n",
    "    cp = {\"stages\": []}\n",
    "\n",
    "stage_entry = {\n",
    "    \"stage\": \"tiling_tcga\",\n",
    "    \"timestamp\": now_iso(),\n",
    "    \"inputs\": {\n",
    "        \"manifest_parquet\": str(MANIFEST_PARQUET),\n",
    "        \"qc_metrics_parquet\": str(QC_METRICS_PARQUET) if QC_METRICS_PARQUET.exists() else None,\n",
    "        \"qc_policy\": QC_POLICY,\n",
    "        \"target_scales_um_per_px\": TARGET_SCALES,\n",
    "        \"tile_size\": TILE_SIZE,\n",
    "        \"overlap\": OVERLAP,\n",
    "        \"min_tile_tissue_coverage\": MIN_TILE_TISSUE_COVERAGE,\n",
    "        \"mask_max_side\": MASK_MAX_SIDE,\n",
    "        \"sampling_method\": SAMPLING_METHOD,\n",
    "        \"seed\": SEED,\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"tiling_summary_parquet\": str(sum_path),\n",
    "        \"manifests_dir\": str(TILES_DIR),\n",
    "        \"figures_dir\": str(FIG_DIR),\n",
    "    },\n",
    "    \"stats\": {\n",
    "        \"n_slides_considered\": int(len(df_manifest)),\n",
    "        \"n_slides_selected\": int(len(keep)),\n",
    "        \"n_slide_scale_entries\": int(len(df_sum)),\n",
    "        \"elapsed_minutes\": float(elapsed / 60.0),\n",
    "        \"errors\": int(len(errors_all)),\n",
    "    }\n",
    "}\n",
    "cp.setdefault(\"stages\", []).append(stage_entry)\n",
    "\n",
    "tmp = compute_path.with_suffix(\".json.tmp\")\n",
    "with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cp, f, ensure_ascii=False, indent=2)\n",
    "tmp.replace(compute_path)\n",
    "print(f\"\\n[OK] Compute-Passport updated: {compute_path}\")\n",
    "\n",
    "print(\"\\nScript 4 complete. Next: Script 5 (Frozen-backbone feature extraction to 768-D).\")\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Script 5 — OpenSlide extractor\n",
    "\n",
    "import os, sys, json, time, math, random, shutil, subprocess, platform, gc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from time import perf_counter\n",
    "\n",
    "# ---------- Paths (strict) ----------\n",
    "WORKSPACE = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\").resolve()\n",
    "WSI_ROOT  = Path(r\"D:\\个人文件夹\\Sanwal\\DL_V2\\Histo slides 20k\").resolve()\n",
    "SUBDIRS = {\n",
    "    \"features\": WORKSPACE / \"features\",\n",
    "    \"tiles\":    WORKSPACE / \"tiles\",\n",
    "    \"logs\":     WORKSPACE / \"logs\",\n",
    "    \"figures\":  WORKSPACE / \"figures\",\n",
    "}\n",
    "for p in SUBDIRS.values(): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TSUM = SUBDIRS[\"tiles\"] / \"tiling_summary_tcga.parquet\"\n",
    "assert TSUM.exists(), f\"Missing tiling summary: {TSUM}\"\n",
    "\n",
    "# ---------- Quiet-install deps (no admin) ----------\n",
    "def ensure(pkg): \n",
    "    try:\n",
    "        __import__(pkg.split('[')[0].replace('-','_').split('==')[0])\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"-q\",pkg])\n",
    "\n",
    "ensure(\"openslide_python\")\n",
    "ensure(\"openslide_bin\")           # provides DLLs on Windows\n",
    "ensure(\"torch>=2.1\")\n",
    "ensure(\"torchvision\")\n",
    "ensure(\"pandas\")\n",
    "ensure(\"pyarrow\")\n",
    "ensure(\"Pillow\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as tvm\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import openslide\n",
    "\n",
    "# ---------- Config ----------\n",
    "DEVICE  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "AMP_DTYPE = torch.float16 if DEVICE==\"cuda\" else torch.bfloat16\n",
    "TILE_SIZE = 256       # manifest tile size (Script 4 default)\n",
    "MODEL_IN  = 224\n",
    "SELFTEST_SECONDS = 60\n",
    "TARGET_TILES_PER_SEC = 50.0       # <-- your target gate\n",
    "RANDOM_SEED = 13\n",
    "SAVE_DTYPE = np.float16\n",
    "\n",
    "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED); torch.manual_seed(RANDOM_SEED)\n",
    "if hasattr(torch.backends,\"cudnn\"):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "if hasattr(torch,\"set_float32_matmul_precision\"):\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# ---------- Transforms ----------\n",
    "IMAGENET_MEAN=[0.485,0.456,0.406]; IMAGENET_STD=[0.229,0.224,0.225]\n",
    "_to_tensor = T.ToTensor()\n",
    "_resize    = T.Resize((MODEL_IN, MODEL_IN), interpolation=T.InterpolationMode.BILINEAR)\n",
    "_normalize = T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "def to_model_tensor(img: Image.Image) -> torch.Tensor:\n",
    "    if img.size != (MODEL_IN, MODEL_IN):\n",
    "        img = _resize(img)\n",
    "    t = _to_tensor(img); t = _normalize(t)\n",
    "    return t\n",
    "\n",
    "# ---------- Model (ConvNeXt-Tiny → 768D) ----------\n",
    "class ConvNeXtTinyFeats(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        w = tvm.ConvNeXt_Tiny_Weights.DEFAULT\n",
    "        m = tvm.convnext_tiny(weights=w)\n",
    "        self.features = m.features\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        for p in self.parameters(): p.requires_grad=False\n",
    "        self.eval()\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):              # [N,3,224,224]\n",
    "        x = self.features(x)           # [N,768,H,W]\n",
    "        x = self.gap(x).flatten(1)     # [N,768]\n",
    "        return x\n",
    "\n",
    "def build_model():\n",
    "    m = ConvNeXtTinyFeats().to(DEVICE)\n",
    "    if DEVICE==\"cuda\":\n",
    "        m = m.to(memory_format=torch.channels_last)\n",
    "        # short warmup\n",
    "        d = torch.randn(256,3,MODEL_IN,MODEL_IN, device=DEVICE).to(memory_format=torch.channels_last)\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=AMP_DTYPE, enabled=True):\n",
    "            _ = m(d)\n",
    "        torch.cuda.synchronize()\n",
    "    return m\n",
    "\n",
    "MODEL = build_model()\n",
    "\n",
    "# ---------- Tiling summary & manifest helpers ----------\n",
    "df_sum = pd.read_parquet(TSUM)   # columns include: slide_id, manifest, scale_um_per_px, n_tiles, ...\n",
    "assert \"manifest\" in df_sum.columns and \"slide_id\" in df_sum.columns\n",
    "\n",
    "SLIDE_INDEX_PATH = SUBDIRS[\"logs\"] / \"slide_path_index.json\"\n",
    "def index_slide_paths(root: Path) -> dict:\n",
    "    print(\"[INDEX] Building slide path map (once) ...\")\n",
    "    mp={}\n",
    "    for ext in (\"*.svs\",\"*.ndpi\",\"*.tif\",\"*.mrxs\",\"*.scn\"):\n",
    "        for p in root.rglob(ext):\n",
    "            mp[p.stem] = str(p)\n",
    "    return mp\n",
    "if SLIDE_INDEX_PATH.exists():\n",
    "    slide_map = json.loads(SLIDE_INDEX_PATH.read_text(encoding=\"utf-8\"))\n",
    "else:\n",
    "    slide_map = index_slide_paths(WSI_ROOT)\n",
    "    SLIDE_INDEX_PATH.write_text(json.dumps(slide_map, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def slide_path_from_id(slide_id:str, manifest_df:pd.DataFrame|None=None):\n",
    "    # Prefer manifest-sourced path if present\n",
    "    if manifest_df is not None:\n",
    "        for cand in (\"path\",\"source_path\",\"slide_path\",\"wsi_path\"):\n",
    "            if cand in manifest_df.columns:\n",
    "                p = manifest_df[cand].iloc[0]\n",
    "                if isinstance(p,str) and Path(p).exists():\n",
    "                    return p\n",
    "    # Fallback to index\n",
    "    if slide_id in slide_map: return slide_map[slide_id]\n",
    "    base = slide_id.split(\".\")[0]\n",
    "    return slide_map.get(base, None)\n",
    "\n",
    "def load_manifest(man_path:Path):\n",
    "    m = pd.read_parquet(man_path)\n",
    "    # column normalization\n",
    "    lower = {c.lower():c for c in m.columns}\n",
    "    def pick(*names):\n",
    "        for n in names:\n",
    "            if n in m.columns: return n\n",
    "            if n.lower() in lower: return lower[n.lower()]\n",
    "        raise KeyError(f\"Missing columns {names} in {man_path.name}\")\n",
    "    xcol   = pick(\"x\",\"px_x\",\"x_level\")\n",
    "    ycol   = pick(\"y\",\"px_y\",\"y_level\")\n",
    "    lvlcol = pick(\"level\",\"lvl\")\n",
    "    # tile size if present\n",
    "    tsize = TILE_SIZE\n",
    "    for n in (\"tile_size\",\"tile_px\",\"size\"):\n",
    "        if n in m.columns:\n",
    "            try: tsize = int(m[n].iloc[0])\n",
    "            except: pass\n",
    "            break\n",
    "    return m, xcol, ycol, lvlcol, tsize\n",
    "\n",
    "# ---------- OpenSlide reader (level coords → level-0 coords) ----------\n",
    "class SlideReader:\n",
    "    def __init__(self, path:str):\n",
    "        self.path = path\n",
    "        self.osr  = openslide.OpenSlide(path)\n",
    "        self.down = list(self.osr.level_downsamples)  # float\n",
    "    def read_tile(self, level:int, x_level:int, y_level:int, size:int):\n",
    "        # convert level coords to level-0 pixels\n",
    "        ds = self.down[level]\n",
    "        bx = int(round(x_level * ds))\n",
    "        by = int(round(y_level * ds))\n",
    "        img = self.osr.read_region((bx,by), level, (size,size)).convert(\"RGB\")\n",
    "        return img\n",
    "    def close(self):\n",
    "        try: self.osr.close()\n",
    "        except: pass\n",
    "\n",
    "# ---------- Batching & forward ----------\n",
    "def iter_batches_from_manifest(reader:SlideReader, man_df, xcol, ycol, lvlcol, tile_px, max_batch=4096):\n",
    "    # Serial, contiguous batches (HDD-friendly), no multiprocessing\n",
    "    buf=[]\n",
    "    for r in man_df[[xcol,ycol,lvlcol]].itertuples(index=False, name=None):\n",
    "        x,y,lvl = map(int, r)\n",
    "        img = reader.read_tile(lvl, x, y, tile_px)\n",
    "        t = to_model_tensor(img)                    # [3,H,W]\n",
    "        buf.append(t)\n",
    "        if len(buf) >= max_batch:\n",
    "            batch = torch.stack(buf,0).to(memory_format=torch.channels_last)\n",
    "            yield batch\n",
    "            buf.clear()\n",
    "    if buf:\n",
    "        batch = torch.stack(buf,0).to(memory_format=torch.channels_last)\n",
    "        yield batch\n",
    "\n",
    "def forward_batches(model, batches_iter):\n",
    "    outs=[]\n",
    "    for cpu_batch in batches_iter:\n",
    "        with torch.no_grad():\n",
    "            chunk = cpu_batch.to(DEVICE, non_blocking=True)\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=AMP_DTYPE, enabled=(DEVICE==\"cuda\")):\n",
    "                out = model(chunk)                 # [N,768]\n",
    "            outs.append(out.detach().cpu())\n",
    "        del cpu_batch\n",
    "    feats = torch.cat(outs,0).contiguous().numpy()\n",
    "    return feats\n",
    "\n",
    "# ---------- Output paths ----------\n",
    "OUT05 = SUBDIRS[\"features\"] / \"scale0p5\"; OUT20 = SUBDIRS[\"features\"] / \"scale2p0\"\n",
    "OUT05.mkdir(parents=True, exist_ok=True); OUT20.mkdir(parents=True, exist_ok=True)\n",
    "def out_paths(slide_id:str, scale:float, ext=\"npy\"):\n",
    "    d = OUT05 if math.isclose(scale,0.5,abs_tol=1e-6) else OUT20\n",
    "    return d / f\"{slide_id}.{ext}\", d / f\"{slide_id}_meta.parquet\"\n",
    "\n",
    "# ---------- Env print ----------\n",
    "env = {\n",
    "    \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"device\": DEVICE,\n",
    "    \"torch\": torch.__version__,\n",
    "    \"amp_dtype\": str(AMP_DTYPE),\n",
    "    \"openslide_vendor\": None\n",
    "}\n",
    "try:\n",
    "    # peek one slide to get vendor\n",
    "    ptest = next(iter(slide_map.values()), None)\n",
    "    if ptest:\n",
    "        osr = openslide.OpenSlide(ptest); env[\"openslide_vendor\"] = osr.properties.get(\"openslide.vendor\",\"?\"); osr.close()\n",
    "except Exception: pass\n",
    "(SUBDIRS[\"logs\"] / \"script5_env.json\").write_text(json.dumps(env, indent=2), encoding=\"utf-8\")\n",
    "print(\"[ENV]\\n\" + json.dumps(env, indent=2))\n",
    "\n",
    "# ---------- Build pending groups (both scales per slide) ----------\n",
    "done_map={}\n",
    "for sc in (0.5, 2.0):\n",
    "    sub = df_sum[np.isclose(df_sum[\"scale_um_per_px\"], sc)]\n",
    "    for sid in sub[\"slide_id\"].unique():\n",
    "        npy, meta = out_paths(sid, sc)\n",
    "        done_map[(sid, sc)] = npy.exists() and meta.exists()\n",
    "\n",
    "groups=[]\n",
    "for sid, g in df_sum.groupby(\"slide_id\", sort=False):\n",
    "    entries=[]\n",
    "    for _, row in g.sort_values(\"n_tiles\",ascending=False).iterrows():\n",
    "        sc = float(row[\"scale_um_per_px\"])\n",
    "        if not done_map.get((sid, sc), False):\n",
    "            entries.append({\"scale\": sc, \"manifest\": Path(row[\"manifest\"])})\n",
    "    if entries:\n",
    "        groups.append({\"slide_id\": sid, \"entries\": entries})\n",
    "print(f\"[INFO] Slides pending (≥1 scale): {len(groups)}\")\n",
    "\n",
    "# ---------- Self-test (60 s) ----------\n",
    "def selftest(seconds=SELFTEST_SECONDS, target=TARGET_TILES_PER_SEC):\n",
    "    # pick smallest total tiles to minimize seek overhead during test\n",
    "    cand=[]\n",
    "    for sid, g in df_sum.groupby(\"slide_id\"):\n",
    "        n=int(g[\"n_tiles\"].sum()); man=Path(g.sort_values(\"n_tiles\").iloc[-1][\"manifest\"])\n",
    "        cand.append((n, sid, man))\n",
    "    cand.sort(key=lambda x:x[0])\n",
    "    pick = cand[:min(12, len(cand))]\n",
    "\n",
    "    # open readers once\n",
    "    readers={}\n",
    "    for _, sid, manp in pick:\n",
    "        m, xcol, ycol, lvlcol, tpx = load_manifest(manp)\n",
    "        fn = slide_path_from_id(sid, m)\n",
    "        if not fn or not Path(fn).exists(): continue\n",
    "        readers[sid] = (SlideReader(fn), m[[xcol,ycol,lvlcol]].copy(), xcol, ycol, lvlcol, tpx)\n",
    "\n",
    "    tiles_done=0; t0=perf_counter(); stop=t0+seconds\n",
    "    while perf_counter()<stop and readers:\n",
    "        for sid,(sr, m, xcol,ycol,lvlcol,tpx) in list(readers.items()):\n",
    "            # take ~512 tiles per sid per turn\n",
    "            take = m.iloc[:512]\n",
    "            if take.empty:\n",
    "                del readers[sid]; sr.close(); continue\n",
    "            batches = iter_batches_from_manifest(sr, take, xcol,ycol,lvlcol, tpx, max_batch=2048)\n",
    "            with torch.no_grad():\n",
    "                for cpu_batch in batches:\n",
    "                    chunk = cpu_batch.to(DEVICE, non_blocking=True)\n",
    "                    with torch.amp.autocast(device_type=\"cuda\", dtype=AMP_DTYPE, enabled=(DEVICE==\"cuda\")):\n",
    "                        _ = MODEL(chunk)\n",
    "                    tiles_done += chunk.size(0)\n",
    "                    del cpu_batch, chunk\n",
    "                    if perf_counter()>=stop: break\n",
    "            m = m.iloc[len(take):]\n",
    "            readers[sid]=(sr, m, xcol,ycol,lvlcol,tpx)\n",
    "            if perf_counter()>=stop: break\n",
    "\n",
    "    dt = perf_counter()-t0\n",
    "    rate = tiles_done / max(dt,1e-6)\n",
    "    print(f\"[SELFTEST] tiles={tiles_done}  time={dt:.1f}s  tiles/s={rate:.1f}\")\n",
    "    print((\"[PASS] \" if rate>=target else \"[FAIL] \")+f\"{rate:.1f} tiles/s (target ≥ {target:.0f})\")\n",
    "    (SUBDIRS[\"logs\"]/ \"script5_selftest.json\").write_text(json.dumps({\n",
    "        \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"tiles\": tiles_done, \"seconds\": round(dt,2), \"tiles_per_s\": round(rate,2),\n",
    "        \"target\": target, \"pass\": rate>=target\n",
    "    }, indent=2), encoding=\"utf-8\")\n",
    "    # close readers\n",
    "    for sid,(sr, *_rest) in readers.items(): sr.close()\n",
    "    return rate\n",
    "\n",
    "rate = selftest()\n",
    "if rate < TARGET_TILES_PER_SEC:\n",
    "    print(\"[ABORT] Below target. This cell stops here (no full run).\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# ---------- Full run (only if self-test passed) ----------\n",
    "PROG = SUBDIRS[\"logs\"] / \"script5_progress.jsonl\"\n",
    "def log_progress(**kw):\n",
    "    kw[\"ts\"]=datetime.now().isoformat(timespec=\"seconds\")\n",
    "    with open(PROG,\"a\",encoding=\"utf-8\") as f: f.write(json.dumps(kw,ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "for i, grp in enumerate(groups, 1):\n",
    "    sid = grp[\"slide_id\"]\n",
    "    # open once per slide\n",
    "    # load one manifest to discover path (prefer scale 0.5 if exists)\n",
    "    man_pref = min(grp[\"entries\"], key=lambda e: abs(e[\"scale\"]-0.5))\n",
    "    m_probe, xcol, ycol, lvlcol, tpx = load_manifest(man_pref[\"manifest\"])\n",
    "    fn = slide_path_from_id(sid, m_probe)\n",
    "    if not fn or not Path(fn).exists():\n",
    "        print(f\"[WARN] slide path not found: {sid} — skipped\")\n",
    "        continue\n",
    "    reader = SlideReader(fn)\n",
    "\n",
    "    for e in grp[\"entries\"]:\n",
    "        sc = float(e[\"scale\"])\n",
    "        npy_path, meta_path = out_paths(sid, sc)\n",
    "        if npy_path.exists() and meta_path.exists(): \n",
    "            continue\n",
    "\n",
    "        man_df, xcol, ycol, lvlcol, tpx = load_manifest(e[\"manifest\"])\n",
    "        if man_df.empty:\n",
    "            print(f\"[WARN] empty manifest: {e['manifest']} — skip\")\n",
    "            continue\n",
    "\n",
    "        # forward\n",
    "        t0 = perf_counter()\n",
    "        batches = iter_batches_from_manifest(reader, man_df, xcol,ycol,lvlcol, tpx, max_batch=4096)\n",
    "        feats = forward_batches(MODEL, batches)         # [N,768]\n",
    "        if DEVICE==\"cuda\": torch.cuda.synchronize()\n",
    "        dt = perf_counter()-t0\n",
    "\n",
    "        # save\n",
    "        np.save(npy_path, feats.astype(SAVE_DTYPE))\n",
    "        md = man_df.copy()\n",
    "        md[\"slide_id\"]=sid; md[\"scale_um_per_px\"]=sc\n",
    "        md.to_parquet(meta_path, index=False)\n",
    "\n",
    "        N = int(feats.shape[0])\n",
    "        tiles_per_s = N / max(dt,1e-6)\n",
    "        vram = (torch.cuda.max_memory_allocated()/(1024**3)) if DEVICE==\"cuda\" else 0.0\n",
    "        print(f\"[OK] {i}/{len(groups)} | {sid} @{sc:.1f} µm/px → ({N},768) | {tiles_per_s:.1f} tiles/s | VRAM~{vram:.2f} GB\")\n",
    "        log_progress(slide_id=sid, scale=sc, tiles=N, seconds=round(dt,2), tps=round(tiles_per_s,2), vram_gb=round(vram,2))\n",
    "\n",
    "        del feats; gc.collect()\n",
    "        if DEVICE==\"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    reader.close()\n",
    "\n",
    "print(\"[DONE] All pending entries processed.\")\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Script 6 — Two-Scale Feature-Space Pretraining \n",
    "\n",
    "import os, sys, json, math, random, gc, subprocess, platform\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# --------------------------- Workspace ---------------------------\n",
    "WORKSPACE = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\").resolve()\n",
    "FEATURES05 = WORKSPACE / \"features\" / \"scale0p5\"\n",
    "FEATURES20 = WORKSPACE / \"features\" / \"scale2p0\"\n",
    "LOGS       = WORKSPACE / \"logs\"\n",
    "WEIGHTS    = WORKSPACE / \"weights\"\n",
    "FIGS       = WORKSPACE / \"figures\"\n",
    "EMBED      = WORKSPACE / \"embeddings\" / \"student_final\"\n",
    "for p in [LOGS, WEIGHTS, FIGS, EMBED]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    assert str(p).startswith(str(WORKSPACE)), f\"Output path escapes WORKSPACE: {p}\"\n",
    "\n",
    "# --------------------------- Robust deps (no hard failures for optional libs) ---------------------------\n",
    "def _pip(*pkgs):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] pip install failed for {pkgs}: {e}\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    _pip(\"numpy>=1.24\",\"pandas>=2.0\"); import numpy as np, pandas as pd\n",
    "\n",
    "try:\n",
    "    import torch, torch.nn as nn, torch.nn.functional as F\n",
    "except Exception:\n",
    "    _pip(\"torch>=2.1\"); import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from safetensors.torch import save_file as save_safetensors, load_file as load_safetensors\n",
    "except Exception:\n",
    "    _pip(\"safetensors>=0.4.0\"); from safetensors.torch import save_file as save_safetensors, load_file as load_safetensors\n",
    "\n",
    "# Matplotlib is optional; plotting will be skipped if unavailable\n",
    "try:\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    HAS_MPL = True\n",
    "except Exception:\n",
    "    HAS_MPL = False\n",
    "\n",
    "# --------------------------- Config ---------------------------\n",
    "CONFIG = {\n",
    "    \"seed\": 13,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"dtype_amp\": \"float16\",                 # \"float16\" on CUDA; \"float32\" on CPU\n",
    "    \"token_budget_0p5\": 1200,               # tokens from 0.5 μm per slide\n",
    "    \"token_budget_2p0\":  400,               # tokens from 2.0 μm per slide\n",
    "    \"mask_frac\": 0.25,                      # fraction of tokens masked for MFR\n",
    "    \"lambda_mfr\": 0.5,                      # weight for MFR loss\n",
    "    \"d_model\": 768,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 6,\n",
    "    \"ff_mult\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "    \"batch_slides\": 3,                      # fits 24 GB with defaults\n",
    "    \"grad_accum\": 2,                        # effective batch = batch_slides * grad_accum\n",
    "    \"epochs\": 4,\n",
    "    \"steps_per_epoch_cap\": None,            # None = full pass; or int to cap\n",
    "    \"lr\": 1.5e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"ema_tau\": 0.996,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"save_every_steps\": 1000,\n",
    "    \"log_every_steps\": 50,\n",
    "    \"resume_if_available\": True,            # resume from weights/latest.txt if present\n",
    "    \"export_embeddings_after_train\": True,  # export per-slide g-embeddings after training\n",
    "    \"export_use_budget\": True               # True: budgets; False: all tokens (slower)\n",
    "}\n",
    "\n",
    "# --------------------------- Reproducibility ---------------------------\n",
    "SEED = CONFIG[\"seed\"]\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if hasattr(torch.backends,\"cudnn\"):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "DEVICE = CONFIG[\"device\"]\n",
    "AMP_DTYPE = (torch.float16 if (DEVICE==\"cuda\" and CONFIG[\"dtype_amp\"]==\"float16\") else\n",
    "             torch.bfloat16 if (DEVICE==\"cuda\" and CONFIG[\"dtype_amp\"]==\"bfloat16\") else\n",
    "             torch.float32)\n",
    "\n",
    "# --------------------------- Slide inventory (require both scales) ---------------------------\n",
    "def _collect(dir_path: Path) -> Dict[str, Path]:\n",
    "    mp = {}\n",
    "    for p in dir_path.glob(\"*.npy\"):\n",
    "        mp[p.stem] = p\n",
    "    return mp\n",
    "\n",
    "mp05 = _collect(FEATURES05)\n",
    "mp20 = _collect(FEATURES20)\n",
    "common_ids = sorted(set(mp05.keys()) & set(mp20.keys()))\n",
    "assert len(common_ids)>0, \"No slides found that have both 0.5 and 2.0 μm features. Check Script 5 outputs.\"\n",
    "\n",
    "@dataclass\n",
    "class SlideRec:\n",
    "    slide_id: str\n",
    "    npy05: Path\n",
    "    meta05: Path\n",
    "    npy20: Path\n",
    "    meta20: Path\n",
    "\n",
    "def meta_path(npy_path: Path) -> Path:\n",
    "    return npy_path.with_name(npy_path.stem + \"_meta.parquet\")\n",
    "\n",
    "slides: List[SlideRec] = []\n",
    "for sid in common_ids:\n",
    "    p05 = mp05[sid]; p20 = mp20[sid]\n",
    "    m05 = meta_path(p05); m20 = meta_path(p20)\n",
    "    if m05.exists() and m20.exists():\n",
    "        slides.append(SlideRec(sid, p05, m05, p20, m20))\n",
    "assert len(slides)>0, \"Found slides but *_meta.parquet files are missing. Re-run Script 5 or verify meta files.\"\n",
    "\n",
    "print(json.dumps({\n",
    "    \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"torch\": torch.__version__,\n",
    "    \"device\": DEVICE,\n",
    "    \"amp_dtype\": str(AMP_DTYPE).split(\".\")[-1],\n",
    "    \"slides_2scale\": len(slides)\n",
    "}, indent=2))\n",
    "\n",
    "# --------------------------- Meta loading (robust to column names) ---------------------------\n",
    "_META_CACHE: Dict[Path, pd.DataFrame] = {}\n",
    "def load_meta(p: Path) -> pd.DataFrame:\n",
    "    if p in _META_CACHE: return _META_CACHE[p]\n",
    "    df = pd.read_parquet(p)  # Script 5 produced pyarrow-style parquet\n",
    "    # normalize columns\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    def pick(*names):\n",
    "        for n in names:\n",
    "            if n in df.columns: return n\n",
    "            if n.lower() in cols_lower: return cols_lower[n.lower()]\n",
    "        raise KeyError(f\"Missing one of {names} in {p.name}\")\n",
    "    xcol   = pick(\"x\")\n",
    "    ycol   = pick(\"y\")\n",
    "    lvlcol = pick(\"level\",\"lvl\")\n",
    "    sccol  = pick(\"scale_um_per_px\")\n",
    "    tsize = 256\n",
    "    for n in (\"tile_size\",\"tile_px\",\"size\"):\n",
    "        if n in df.columns:\n",
    "            try: tsize = int(df[n].iloc[0])\n",
    "            except: pass\n",
    "            break\n",
    "    out = df[[xcol,ycol,lvlcol,sccol]].copy()\n",
    "    out.columns = [\"x\",\"y\",\"level\",\"scale_um_per_px\"]\n",
    "    out[\"tile_px\"] = tsize\n",
    "    _META_CACHE[p] = out\n",
    "    return out\n",
    "\n",
    "def compute_mm_xy(df: pd.DataFrame) -> np.ndarray:\n",
    "    um_per_px = df[\"scale_um_per_px\"].astype(float).to_numpy()\n",
    "    mm_per_px = um_per_px / 1000.0\n",
    "    cx = (df[\"x\"].to_numpy() + df[\"tile_px\"].to_numpy()/2.0) * mm_per_px\n",
    "    cy = (df[\"y\"].to_numpy() + df[\"tile_px\"].to_numpy()/2.0) * mm_per_px\n",
    "    return np.stack([cx, cy], axis=1).astype(np.float32)\n",
    "\n",
    "# --------------------------- MIL model ---------------------------\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(3, d_model//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model//2, d_model)\n",
    "        )\n",
    "    def forward(self, mmxy: torch.Tensor, scale_um: torch.Tensor):\n",
    "        x = torch.cat([mmxy, scale_um], dim=-1)  # [B,T,3]\n",
    "        return self.proj(x)\n",
    "\n",
    "class MILTransformer(nn.Module):\n",
    "    def __init__(self, d_model=768, n_heads=8, n_layers=6, ff_mult=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.cls = nn.Parameter(torch.zeros(1,1,d_model))\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=int(ff_mult*d_model),\n",
    "            dropout=dropout, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.ln  = nn.LayerNorm(d_model)\n",
    "        self.pos = PositionalEncoder(d_model)\n",
    "        self.proj_global = nn.Sequential(nn.Linear(d_model, d_model), nn.GELU(), nn.Linear(d_model, d_model))\n",
    "        self.proj_token  = nn.Sequential(nn.Linear(d_model, d_model), nn.GELU(), nn.Linear(d_model, d_model))\n",
    "        self.pred_global = nn.Sequential(nn.Linear(d_model, d_model), nn.GELU(), nn.Linear(d_model, d_model))\n",
    "        self.pred_token  = nn.Sequential(nn.Linear(d_model, d_model), nn.GELU(), nn.Linear(d_model, d_model))\n",
    "\n",
    "    def forward(self, feats: torch.Tensor, mmxy: torch.Tensor, scale_um: torch.Tensor, pad_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        feats   : [B,T,768]\n",
    "        mmxy    : [B,T,2]\n",
    "        scale_um: [B,T,1]\n",
    "        pad_mask: [B,T] (True for PADs)\n",
    "        \"\"\"\n",
    "        B,T,_ = feats.shape\n",
    "        pos = self.pos(mmxy, scale_um)\n",
    "        x = feats + pos\n",
    "        cls = self.cls.expand(B,1,-1)\n",
    "        x = torch.cat([cls, x], dim=1)  # [B,1+T,D]\n",
    "        pad = torch.zeros(B,1, dtype=torch.bool, device=pad_mask.device)\n",
    "        key_padding = torch.cat([pad, pad_mask], dim=1)\n",
    "        x = self.enc(x, src_key_padding_mask=key_padding)\n",
    "        x = self.ln(x)\n",
    "        g = x[:,0,:]\n",
    "        t = x[:,1:,:]\n",
    "        g_proj = self.proj_global(g)\n",
    "        t_proj = self.proj_token(t)\n",
    "        g_pred = self.pred_global(g_proj)\n",
    "        t_pred = self.pred_token(t_proj)\n",
    "        return g_proj, t_proj, g_pred, t_pred\n",
    "\n",
    "# --------------------------- Losses & EMA ---------------------------\n",
    "def cosine_loss(p: torch.Tensor, z: torch.Tensor):\n",
    "    p = F.normalize(p, dim=-1)\n",
    "    z = F.normalize(z.detach(), dim=-1)\n",
    "    return (1.0 - (p * z).sum(dim=-1)).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(teacher: nn.Module, student: nn.Module, tau: float):\n",
    "    for pt, ps in zip(teacher.parameters(), student.parameters()):\n",
    "        pt.data.mul_(tau).add_(ps.data, alpha=(1.0 - tau))\n",
    "\n",
    "# --------------------------- Build models/opt ---------------------------\n",
    "student = MILTransformer(\n",
    "    d_model=CONFIG[\"d_model\"], n_heads=CONFIG[\"n_heads\"],\n",
    "    n_layers=CONFIG[\"n_layers\"], ff_mult=CONFIG[\"ff_mult\"], dropout=CONFIG[\"dropout\"]\n",
    ").to(DEVICE)\n",
    "\n",
    "teacher = MILTransformer(\n",
    "    d_model=CONFIG[\"d_model\"], n_heads=CONFIG[\"n_heads\"],\n",
    "    n_layers=CONFIG[\"n_layers\"], ff_mult=CONFIG[\"ff_mult\"], dropout=CONFIG[\"dropout\"]\n",
    ").to(DEVICE)\n",
    "teacher.load_state_dict(student.state_dict())\n",
    "for p in teacher.parameters(): p.requires_grad = False\n",
    "\n",
    "opt = torch.optim.AdamW([p for p in student.parameters() if p.requires_grad],\n",
    "                        lr=CONFIG[\"lr\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "# --------------------------- Token sampling & batching ---------------------------\n",
    "def _sample(n: int, k: int) -> np.ndarray:\n",
    "    if n <= k: return np.arange(n, dtype=np.int64)\n",
    "    return np.random.choice(n, size=k, replace=False).astype(np.int64)\n",
    "\n",
    "def load_tokens_for_slide(rec: SlideRec, budget05: int, budget20: int):\n",
    "    \"\"\"Return (feats[T,768], mmxy[T,2], scl[T,1]) with T = budget05 + budget20.\"\"\"\n",
    "    # 0.5 μm\n",
    "    f05 = np.load(rec.npy05, mmap_mode='r')                     # [N05,768]\n",
    "    assert f05.shape[1] == CONFIG[\"d_model\"], f\"Feature dim {f05.shape[1]}≠{CONFIG['d_model']} for {rec.slide_id} @0.5\"\n",
    "    m05 = load_meta(rec.meta05)\n",
    "    idx05 = _sample(f05.shape[0], budget05)\n",
    "    mm05  = compute_mm_xy(m05.iloc[idx05])                      # [budget05,2]\n",
    "    sc05  = m05[\"scale_um_per_px\"].iloc[idx05].to_numpy(np.float32).reshape(-1,1)\n",
    "\n",
    "    # 2.0 μm\n",
    "    f20 = np.load(rec.npy20, mmap_mode='r')                     # [N20,768]\n",
    "    assert f20.shape[1] == CONFIG[\"d_model\"], f\"Feature dim {f20.shape[1]}≠{CONFIG['d_model']} for {rec.slide_id} @2.0\"\n",
    "    m20 = load_meta(rec.meta20)\n",
    "    idx20 = _sample(f20.shape[0], budget20)\n",
    "    mm20  = compute_mm_xy(m20.iloc[idx20])                      # [budget20,2]\n",
    "    sc20  = m20[\"scale_um_per_px\"].iloc[idx20].to_numpy(np.float32).reshape(-1,1)\n",
    "\n",
    "    feats = np.concatenate([f05[idx05], f20[idx20]], axis=0).astype(np.float32)  # [T,768]\n",
    "    mmxy  = np.concatenate([mm05, mm20], axis=0).astype(np.float32)              # [T,2]\n",
    "    scl   = np.concatenate([sc05, sc20], axis=0).astype(np.float32)              # [T,1]\n",
    "    return feats, mmxy, scl\n",
    "\n",
    "def make_batch(batch_recs: List[SlideRec], budget05: int, budget20: int, mask_frac: float):\n",
    "    feats_list=[]; mmxy_list=[]; sc_list=[]; mask_tiles=[]\n",
    "    for rec in batch_recs:\n",
    "        f, mm, sc = load_tokens_for_slide(rec, budget05, budget20)\n",
    "        Tn = f.shape[0]\n",
    "        feats_list.append(torch.from_numpy(f))\n",
    "        mmxy_list.append(torch.from_numpy(mm))\n",
    "        sc_list.append(torch.from_numpy(sc))\n",
    "        mcount = max(1, int(round(mask_frac*Tn)))\n",
    "        mask_idx = np.random.choice(Tn, size=mcount, replace=False).astype(np.int64)\n",
    "        mask_tiles.append(torch.from_numpy(mask_idx))\n",
    "\n",
    "    T = max(t.shape[0] for t in feats_list)\n",
    "    B = len(batch_recs); D = feats_list[0].shape[1]\n",
    "    feats = torch.zeros(B, T, D, dtype=torch.float32)\n",
    "    mmxy  = torch.zeros(B, T, 2, dtype=torch.float32)\n",
    "    scl   = torch.zeros(B, T, 1, dtype=torch.float32)\n",
    "    pad   = torch.ones(B, T, dtype=torch.bool)\n",
    "    for i in range(B):\n",
    "        n = feats_list[i].shape[0]\n",
    "        feats[i,:n] = feats_list[i]\n",
    "        mmxy[i,:n]  = mmxy_list[i]\n",
    "        scl[i,:n]   = sc_list[i]\n",
    "        pad[i,:n]   = False\n",
    "\n",
    "    mfr_index = []\n",
    "    for b, idx in enumerate(mask_tiles):\n",
    "        mfr_index.append(torch.stack([torch.full_like(idx, b), idx], dim=1))\n",
    "    mfr_index = torch.cat(mfr_index, dim=0)  # [M,2]\n",
    "\n",
    "    return {\n",
    "        \"feats\": feats.to(DEVICE, non_blocking=True),\n",
    "        \"mmxy\":  mmxy.to(DEVICE, non_blocking=True),\n",
    "        \"scl\":   scl.to(DEVICE, non_blocking=True),\n",
    "        \"pad\":   pad.to(DEVICE, non_blocking=True),\n",
    "        \"mfr_index\": mfr_index.to(DEVICE, non_blocking=True)\n",
    "    }\n",
    "\n",
    "# --------------------------- Cosine scheduler w/ warmup ---------------------------\n",
    "class CosineWarmup:\n",
    "    def __init__(self, optimizer, warmup, max_steps, base_lr):\n",
    "        self.opt=optimizer; self.warmup=warmup; self.max=max_steps; self.base=base_lr; self.t=0\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        if self.t <= self.warmup:\n",
    "            lr = self.base * self.t / max(1,self.warmup)\n",
    "        else:\n",
    "            p = (self.t - self.warmup) / max(1, self.max - self.warmup)\n",
    "            lr = self.base * 0.5*(1+math.cos(math.pi*p))\n",
    "        for g in self.opt.param_groups: g[\"lr\"]=lr\n",
    "        return lr\n",
    "\n",
    "# --------------------------- Logging & checkpoints ---------------------------\n",
    "LOG_CSV = LOGS / \"script6_train_log.csv\"\n",
    "if not LOG_CSV.exists():\n",
    "    LOG_CSV.write_text(\"ts,epoch,step,lr,loss,loss_byol,loss_mfr,tokens_per_s,vram_gb\\n\", encoding=\"utf-8\")\n",
    "LOG_JL  = LOGS / \"script6_train_log.jsonl\"\n",
    "\n",
    "def log_row(d: dict):\n",
    "    d2 = d.copy(); d2[\"ts\"]=datetime.now().isoformat(timespec=\"seconds\")\n",
    "    with open(LOG_JL,\"a\",encoding=\"utf-8\") as f: f.write(json.dumps(d2,ensure_ascii=False)+\"\\n\")\n",
    "    with open(LOG_CSV,\"a\",encoding=\"utf-8\") as f:\n",
    "        f.write(f'{d2[\"ts\"]},{d2.get(\"epoch\",0)},{d2.get(\"step\",0)},'\n",
    "                f'{d2.get(\"lr\",0):.6f},{d2.get(\"loss\",0):.6f},{d2.get(\"loss_byol\",0):.6f},'\n",
    "                f'{d2.get(\"loss_mfr\",0):.6f},{d2.get(\"tps\",0):.2f},{d2.get(\"vram_gb\",0):.2f}\\n')\n",
    "\n",
    "def save_ckpt(tag: str):\n",
    "    fn = WEIGHTS / f\"script6_student_{tag}.safetensors\"\n",
    "    state = {k: v.detach().cpu() for k,v in student.state_dict().items()}\n",
    "    save_safetensors(state, str(fn))\n",
    "    (WEIGHTS / \"latest.txt\").write_text(fn.name, encoding=\"utf-8\")\n",
    "    print(f\"[SAVE] {fn.name}\")\n",
    "\n",
    "def try_resume():\n",
    "    if not CONFIG[\"resume_if_available\"]: return False\n",
    "    txt = WEIGHTS / \"latest.txt\"\n",
    "    if not txt.exists(): return False\n",
    "    ck = WEIGHTS / txt.read_text(encoding=\"utf-8\").strip()\n",
    "    if not ck.exists(): return False\n",
    "    print(f\"[RESUME] Loading {ck.name}\")\n",
    "    sd = load_safetensors(str(ck))\n",
    "    student.load_state_dict(sd, strict=True)\n",
    "    teacher.load_state_dict(sd, strict=False)  # teacher weights will sync by EMA\n",
    "    return True\n",
    "\n",
    "# --------------------------- Training loop ---------------------------\n",
    "total_steps = CONFIG[\"epochs\"] * (len(slides)//CONFIG[\"batch_slides\"] + 1)\n",
    "if CONFIG[\"steps_per_epoch_cap\"]:\n",
    "    total_steps = CONFIG[\"epochs\"] * CONFIG[\"steps_per_epoch_cap\"]\n",
    "sched = CosineWarmup(opt, warmup=CONFIG[\"warmup_steps\"], max_steps=total_steps, base_lr=CONFIG[\"lr\"])\n",
    "\n",
    "resumed = try_resume()\n",
    "print(f\"[TRAIN] slides={len(slides)} | batch_slides={CONFIG['batch_slides']} | grad_accum={CONFIG['grad_accum']} | epochs={CONFIG['epochs']} | resume={resumed}\")\n",
    "\n",
    "global_step=0\n",
    "for epoch in range(1, CONFIG[\"epochs\"]+1):\n",
    "    random.shuffle(slides)\n",
    "    steps_this_epoch = 0\n",
    "    max_steps_epoch = (CONFIG[\"steps_per_epoch_cap\"] or (len(slides)//CONFIG[\"batch_slides\"] + 1))\n",
    "\n",
    "    i = 0\n",
    "    while steps_this_epoch < max_steps_epoch and i < len(slides):\n",
    "        batch_recs = slides[i : i+CONFIG[\"batch_slides\"]]\n",
    "        i += CONFIG[\"batch_slides\"]\n",
    "\n",
    "        try:\n",
    "            b = make_batch(batch_recs, CONFIG[\"token_budget_0p5\"], CONFIG[\"token_budget_2p0\"], CONFIG[\"mask_frac\"])\n",
    "        except AssertionError as ae:\n",
    "            print(f\"[SKIP] {batch_recs[0].slide_id} assert: {ae}\"); continue\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP] Batch error: {e}\"); continue\n",
    "\n",
    "        feats, mmxy, scl, pad, mfr_index = b[\"feats\"], b[\"mmxy\"], b[\"scl\"], b[\"pad\"], b[\"mfr_index\"]\n",
    "        tokens_total = int((~pad).sum().item())\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        t0 = perf_counter()\n",
    "\n",
    "        # teacher forward\n",
    "        with torch.no_grad():\n",
    "            g_t, t_t, _, _ = teacher(feats, mmxy, scl, pad)\n",
    "\n",
    "        # student forward + losses\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=AMP_DTYPE, enabled=(DEVICE==\"cuda\" and AMP_DTYPE!=torch.float32)):\n",
    "            g_s, t_s, g_sp, t_sp = student(feats, mmxy, scl, pad)\n",
    "            loss_byol = cosine_loss(g_sp, g_t)\n",
    "            bi = mfr_index\n",
    "            t_s_mask = t_sp[bi[:,0], bi[:,1], :]\n",
    "            t_t_mask = t_t[bi[:,0], bi[:,1], :]\n",
    "            loss_mfr = cosine_loss(t_s_mask, t_t_mask)\n",
    "            loss = loss_byol + CONFIG[\"lambda_mfr\"] * loss_mfr\n",
    "\n",
    "        scaler.scale(loss / CONFIG[\"grad_accum\"]).backward()\n",
    "\n",
    "        if ((steps_this_epoch+1) % CONFIG[\"grad_accum\"] == 0):\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            ema_update(teacher, student, tau=CONFIG[\"ema_tau\"])\n",
    "            lr = sched.step()\n",
    "        else:\n",
    "            lr = sched.opt.param_groups[0][\"lr\"]\n",
    "\n",
    "        if DEVICE==\"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "            vram = torch.cuda.max_memory_allocated()/(1024**3)\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        else:\n",
    "            vram = 0.0\n",
    "\n",
    "        dt = perf_counter()-t0\n",
    "        tps = tokens_total/max(dt,1e-6)\n",
    "\n",
    "        global_step += 1\n",
    "        steps_this_epoch += 1\n",
    "\n",
    "        if global_step % CONFIG[\"log_every_steps\"] == 0:\n",
    "            print(f\"[E{epoch} S{global_step}] loss={loss.item():.4f} (byol {loss_byol.item():.4f} | mfr {loss_mfr.item():.4f}) \"\n",
    "                  f\"| tokens={tokens_total} | {tps:.1f} tok/s | lr={lr:.2e} | VRAM~{vram:.2f} GB\")\n",
    "            log_row({\"epoch\":epoch, \"step\":global_step, \"lr\":lr,\n",
    "                     \"loss\":float(loss.item()), \"loss_byol\":float(loss_byol.item()),\n",
    "                     \"loss_mfr\":float(loss_mfr.item()), \"tps\":float(tps), \"vram_gb\":float(vram)})\n",
    "\n",
    "        if global_step % CONFIG[\"save_every_steps\"] == 0:\n",
    "            save_ckpt(f\"e{epoch}_s{global_step}\")\n",
    "\n",
    "        # light periodic cleanup\n",
    "        if (global_step % 200) == 0:\n",
    "            del feats, mmxy, scl, pad, mfr_index, g_t, t_t, g_s, t_s, g_sp, t_sp\n",
    "            gc.collect()\n",
    "            if DEVICE==\"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    save_ckpt(f\"e{epoch}\")\n",
    "\n",
    "print(\"[TRAIN] Finished Script 6 pretraining.\")\n",
    "\n",
    "# --------------------------- Optional: quick curve (skips if matplotlib missing) ---------------------------\n",
    "try:\n",
    "    df_plot = pd.read_csv(LOG_CSV)\n",
    "    if HAS_MPL and not df_plot.empty:\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(df_plot[\"step\"], df_plot[\"loss\"], label=\"loss\")\n",
    "        if \"loss_byol\" in df_plot: plt.plot(df_plot[\"step\"], df_plot[\"loss_byol\"], label=\"BYOL\")\n",
    "        if \"loss_mfr\" in df_plot:  plt.plot(df_plot[\"step\"], df_plot[\"loss_mfr\"],  label=\"MFR\")\n",
    "        plt.xlabel(\"step\"); plt.ylabel(\"loss\"); plt.grid(True, alpha=0.3); plt.legend()\n",
    "        outp = FIGS / \"script6_training_curves.png\"\n",
    "        plt.tight_layout(); plt.savefig(outp, dpi=150); plt.close()\n",
    "        print(f\"[FIG] {outp}\")\n",
    "    else:\n",
    "        print(\"[SKIP] Plotting not available or log empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Plotting skipped: {e}\")\n",
    "\n",
    "# --------------------------- Optional: export slide embeddings ---------------------------\n",
    "def export_embeddings(ckpt_name: Optional[str]=None, use_budget=True):\n",
    "    if ckpt_name is None:\n",
    "        txt = (WEIGHTS / \"latest.txt\")\n",
    "        assert txt.exists(), \"Missing weights/latest.txt\"\n",
    "        ckpt_name = txt.read_text(encoding=\"utf-8\").strip()\n",
    "    ckpt_path = WEIGHTS / ckpt_name\n",
    "    print(f\"[EXPORT] Loading {ckpt_path.name}\")\n",
    "    sd = load_safetensors(str(ckpt_path))\n",
    "    student.load_state_dict(sd, strict=True)\n",
    "    student.eval()\n",
    "\n",
    "    count=0; t0=perf_counter()\n",
    "    for rec in slides:\n",
    "        outn = EMBED / f\"{rec.slide_id}.npy\"\n",
    "        if outn.exists(): continue\n",
    "        if use_budget:\n",
    "            f, mm, sc = load_tokens_for_slide(rec, CONFIG[\"token_budget_0p5\"], CONFIG[\"token_budget_2p0\"])\n",
    "        else:\n",
    "            f05 = np.load(rec.npy05, mmap_mode='r'); m05 = load_meta(rec.meta05)\n",
    "            f20 = np.load(rec.npy20, mmap_mode='r'); m20 = load_meta(rec.meta20)\n",
    "            f = np.concatenate([f05, f20], axis=0).astype(np.float32)\n",
    "            mm = np.concatenate([compute_mm_xy(m05), compute_mm_xy(m20)], axis=0).astype(np.float32)\n",
    "            sc = np.concatenate([\n",
    "                m05[\"scale_um_per_px\"].to_numpy(dtype=np.float32).reshape(-1,1),\n",
    "                m20[\"scale_um_per_px\"].to_numpy(dtype=np.float32).reshape(-1,1)\n",
    "            ], axis=0).astype(np.float32)\n",
    "        feats = torch.from_numpy(f).unsqueeze(0).to(DEVICE)\n",
    "        mmxy  = torch.from_numpy(mm).unsqueeze(0).to(DEVICE)\n",
    "        scl   = torch.from_numpy(sc).unsqueeze(0).to(DEVICE)\n",
    "        pad   = torch.zeros(1, feats.size(1), dtype=torch.bool, device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            g_proj, _, _, _ = student(feats, mmxy, scl, pad)\n",
    "        emb = g_proj.squeeze(0).detach().cpu().numpy().astype(np.float32)\n",
    "        np.save(outn, emb)\n",
    "        count += 1\n",
    "        if count % 200 == 0:\n",
    "            print(f\"[EMB] {count}/{len(slides)} saved...\")\n",
    "    dt = perf_counter()-t0\n",
    "    print(f\"[EMB] Done: {count} slides in {dt/60:.1f} min\")\n",
    "\n",
    "if CONFIG[\"export_embeddings_after_train\"]:\n",
    "    export_embeddings(ckpt_name=None, use_budget=CONFIG[\"export_use_budget\"])\n",
    "\n",
    "print(\"[DONE] Script 6 complete.\")\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Script 6B — Finalize & Save Encoder Checkpoint \n",
    "\n",
    "import os, sys, json, time, random, shutil, subprocess, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------- Paths -----------------------\n",
    "WORKSPACE   = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\")\n",
    "FEAT05_DIR  = WORKSPACE / \"features\" / \"scale0p5\"\n",
    "FEAT20_DIR  = WORKSPACE / \"features\" / \"scale2p0\"\n",
    "MODELS_DIR  = WORKSPACE / \"models\"\n",
    "LOGS_DIR    = WORKSPACE / \"logs\"\n",
    "for p in (MODELS_DIR, LOGS_DIR): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STUDENT_OUT = MODELS_DIR / \"openslidefm_student.pt\"\n",
    "TEACHER_OUT = MODELS_DIR / \"openslidefm_teacher_ema.pt\"\n",
    "MANIFEST    = MODELS_DIR / \"openslidefm_checkpoint_manifest.json\"\n",
    "TRAIN_LOG   = LOGS_DIR / \"script6c_finalize_log.csv\"\n",
    "\n",
    "# ----------------------- Deps ------------------------\n",
    "def _ensure(pkgs):\n",
    "    miss=[]\n",
    "    for name, spec in pkgs:\n",
    "        try: __import__(name)\n",
    "        except Exception: miss.append(spec)\n",
    "    if miss:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *miss])\n",
    "\n",
    "_ensure([(\"numpy\",\"numpy>=1.24\"), (\"torch\",\"torch>=2.1\")])\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "AMP_DTYPE = torch.float16 if DEVICE==\"cuda\" else torch.bfloat16\n",
    "\n",
    "# ------------------- Config -------------------\n",
    "CFG = {\n",
    "    \"token_dim\": 768,\n",
    "    \"budget_0p5\": 1200,       # target tokens @ 0.5 µm/px\n",
    "    \"budget_2p0\": 400,        # target tokens @ 2.0 µm/px\n",
    "    \"mask_frac\": 0.25,\n",
    "    \"d_model\": 768,\n",
    "    \"nhead\": 8,\n",
    "    \"nlayers\": 6,\n",
    "    \"dropout\": 0.1,\n",
    "    \"proj_dim\": 256,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 0.05,\n",
    "    \"total_steps\": 400,       # short top-up to materialize weights\n",
    "    \"ema_decay\": 0.996,\n",
    "    \"batch_slides\": 3,\n",
    "    \"print_every\": 20,\n",
    "    \"seed\": 1337,\n",
    "}\n",
    "\n",
    "def set_seed(s):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "set_seed(CFG[\"seed\"])\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# ------------------- Utilities -------------------\n",
    "def list_slide_ids():\n",
    "    s05 = {p.stem for p in FEAT05_DIR.glob(\"*.npy\")}\n",
    "    s20 = {p.stem for p in FEAT20_DIR.glob(\"*.npy\")}\n",
    "    inter = sorted(s05 & s20)\n",
    "    return inter\n",
    "\n",
    "def _sample_idx(n_avail: int, k: int) -> np.ndarray:\n",
    "    if n_avail <= 0:\n",
    "        return np.zeros((0,), dtype=np.int64)\n",
    "    replace = n_avail < k\n",
    "    return np.random.choice(n_avail, size=k, replace=replace).astype(np.int64)\n",
    "\n",
    "def load_tokens_fixed(slide_id: str, k05: int, k20: int) -> np.ndarray:\n",
    "    \"\"\"Always returns shape [(k05+k20), 768]. Uses replacement if needed.\"\"\"\n",
    "    f05 = np.load(FEAT05_DIR / f\"{slide_id}.npy\", mmap_mode=\"r\")  # [N05,768] float32\n",
    "    f20 = np.load(FEAT20_DIR / f\"{slide_id}.npy\", mmap_mode=\"r\")  # [N20,768]\n",
    "    i05 = _sample_idx(int(f05.shape[0]), k05)\n",
    "    i20 = _sample_idx(int(f20.shape[0]), k20)\n",
    "    x05 = f05[i05]\n",
    "    x20 = f20[i20]\n",
    "    # Guard against any unexpected dtype/shape issues\n",
    "    x05 = x05.astype(np.float32, copy=False).reshape(k05, CFG[\"token_dim\"])\n",
    "    x20 = x20.astype(np.float32, copy=False).reshape(k20, CFG[\"token_dim\"])\n",
    "    x   = np.concatenate([x05, x20], axis=0)  # [(k05+k20), 768]\n",
    "    return x\n",
    "\n",
    "def feature_view(x: np.ndarray, drop_p=0.1, noise_std=0.02) -> np.ndarray:\n",
    "    \"\"\"Simple feature-space augmentation (keeps shape).\"\"\"\n",
    "    if drop_p > 0:\n",
    "        m = (np.random.rand(*x.shape) > drop_p).astype(np.float32)\n",
    "        x = x * m\n",
    "    if noise_std > 0:\n",
    "        x = x + np.random.normal(0.0, noise_std, size=x.shape).astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def write_log_row(step:int, loss:float, l_byol:float, l_mfr:float, tps:int, vram_gb:float):\n",
    "    header = [\"ts\",\"step\",\"loss\",\"loss_byol\",\"loss_mfr\",\"tps\",\"vram_gb\"]\n",
    "    if not TRAIN_LOG.exists():\n",
    "        TRAIN_LOG.write_text(\",\".join(header) + \"\\n\", encoding=\"utf-8\")\n",
    "    row = {\n",
    "        \"ts\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"step\": step,\n",
    "        \"loss\": round(float(loss),6),\n",
    "        \"loss_byol\": round(float(l_byol),6),\n",
    "        \"loss_mfr\": round(float(l_mfr),6),\n",
    "        \"tps\": int(tps),\n",
    "        \"vram_gb\": round(float(vram_gb),2),\n",
    "    }\n",
    "    with open(TRAIN_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\",\".join(str(row[h]) for h in header) + \"\\n\")\n",
    "\n",
    "# ------------------- Model -------------------\n",
    "class TransformerMIL(nn.Module):\n",
    "    def __init__(self, d_model=768, nhead=8, nlayers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.cls = nn.Parameter(torch.zeros(1,1,d_model))\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=4*d_model,\n",
    "            dropout=dropout, batch_first=True, norm_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n",
    "        self.ln  = nn.LayerNorm(d_model)\n",
    "    def forward(self, tokens: torch.Tensor):  # [B,T,768]\n",
    "        B, T, D = tokens.shape\n",
    "        cls = self.cls.expand(B, -1, -1)         # [B,1,D]\n",
    "        x = torch.cat([cls, tokens], dim=1)      # [B,1+T,D]\n",
    "        x = self.enc(x)                          # [B,1+T,D]\n",
    "        x = self.ln(x)\n",
    "        cls_emb = x[:,0]                         # [B,D]\n",
    "        tok_emb = x[:,1:]                        # [B,T,D]\n",
    "        return cls_emb, tok_emb\n",
    "\n",
    "class BYOLHead(nn.Module):\n",
    "    def __init__(self, d_model=768, proj_dim=256):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model), nn.GELU(),\n",
    "            nn.Linear(d_model, proj_dim)\n",
    "        )\n",
    "        self.pred = nn.Sequential(\n",
    "            nn.Linear(proj_dim, proj_dim), nn.GELU(),\n",
    "            nn.Linear(proj_dim, proj_dim)\n",
    "        )\n",
    "    def forward(self, h):  # [B,D]\n",
    "        z = F.normalize(self.proj(h), dim=-1)\n",
    "        p = F.normalize(self.pred(z), dim=-1)\n",
    "        return z, p\n",
    "\n",
    "class EncoderWrapper(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.backbone = TransformerMIL(cfg[\"d_model\"], cfg[\"nhead\"], cfg[\"nlayers\"], cfg[\"dropout\"])\n",
    "        self.head     = BYOLHead(cfg[\"d_model\"], cfg[\"proj_dim\"])\n",
    "    def forward(self, tokens):  # [B,T,768]\n",
    "        cls_emb, tok_emb = self.backbone(tokens)\n",
    "        z, p = self.head(cls_emb)\n",
    "        return cls_emb, tok_emb, z, p\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(teacher: nn.Module, student: nn.Module, decay: float):\n",
    "    for t, s in zip(teacher.parameters(), student.parameters()):\n",
    "        t.data.mul_(decay).add_(s.data, alpha=1.0 - decay)\n",
    "\n",
    "def byol_loss(p_s, z_t):\n",
    "    return 2.0 - 2.0 * (p_s * z_t.detach()).sum(dim=-1).mean()\n",
    "\n",
    "def mfr_loss(tok_s, tok_t, mask):\n",
    "    # mask: [B,T] bool — random subset; we always have full tokens (fixed shape), so no padding mask needed.\n",
    "    if mask is None or mask.sum() == 0:\n",
    "        return torch.tensor(0.0, device=tok_s.device)\n",
    "    diff = tok_s[mask] - tok_t.detach()[mask]\n",
    "    return (diff*diff).mean()\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "if STUDENT_OUT.exists() and TEACHER_OUT.exists():\n",
    "    print(f\"[OK] Checkpoints already exist:\\n - {STUDENT_OUT}\\n - {TEACHER_OUT}\")\n",
    "else:\n",
    "    slide_ids = list_slide_ids()\n",
    "    assert len(slide_ids) >= 100, f\"Too few 2-scale slides: {len(slide_ids)}\"\n",
    "    print(f\"[INFO] Slides with both scales: {len(slide_ids)}\")\n",
    "    print(f\"[INFO] Device={DEVICE}, AMP={AMP_DTYPE}\")\n",
    "\n",
    "    model_s = EncoderWrapper(CFG).to(DEVICE)\n",
    "    model_t = EncoderWrapper(CFG).to(DEVICE)\n",
    "    model_t.load_state_dict(model_s.state_dict())\n",
    "\n",
    "    opt    = torch.optim.AdamW(model_s.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "    T = CFG[\"budget_0p5\"] + CFG[\"budget_2p0\"]\n",
    "    B = CFG[\"batch_slides\"]\n",
    "    tokens_per_batch = B * T\n",
    "\n",
    "    step = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    while step < CFG[\"total_steps\"]:\n",
    "\n",
    "        # ---- batch: fixed-shape tokens for all slides ----\n",
    "        batch_ids = random.sample(slide_ids, B)\n",
    "        xs, xs2, xt = [], [], []\n",
    "        for sid in batch_ids:\n",
    "            base = load_tokens_fixed(sid, CFG[\"budget_0p5\"], CFG[\"budget_2p0\"])  # [T,768], fixed shape\n",
    "            xs.append(feature_view(base, drop_p=0.1, noise_std=0.02))\n",
    "            xs2.append(feature_view(base, drop_p=0.1, noise_std=0.02))\n",
    "            xt.append(base)\n",
    "\n",
    "        x1 = torch.from_numpy(np.stack(xs,  axis=0)).to(DEVICE, non_blocking=True)  # [B,T,768]\n",
    "        x2 = torch.from_numpy(np.stack(xs2, axis=0)).to(DEVICE, non_blocking=True)\n",
    "        xt = torch.from_numpy(np.stack(xt,  axis=0)).to(DEVICE, non_blocking=True)\n",
    "\n",
    "        # random mask for MFR (same shape for all)\n",
    "        mask = (torch.rand((B, T), device=DEVICE) < CFG[\"mask_frac\"])\n",
    "\n",
    "        with torch.amp.autocast(device_type=(\"cuda\" if DEVICE==\"cuda\" else \"cpu\"), dtype=AMP_DTYPE, enabled=True):\n",
    "            cls1, tok1, z1, p1 = model_s(x1)\n",
    "            cls2, tok2, z2, p2 = model_s(x2)\n",
    "            with torch.no_grad():\n",
    "                cls_t, tok_t, zt, _ = model_t(xt)\n",
    "\n",
    "            L_byol = 0.5 * byol_loss(p1, zt) + 0.5 * byol_loss(p2, zt)\n",
    "            L_mfr  = 0.5 * mfr_loss(tok1, tok_t, mask) + 0.5 * mfr_loss(tok2, tok_t, mask)\n",
    "            loss   = L_byol + 0.5 * L_mfr\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        ema_update(model_t, model_s, CFG[\"ema_decay\"])\n",
    "\n",
    "        step += 1\n",
    "        if step == 1 or step % CFG[\"print_every\"] == 0 or step == CFG[\"total_steps\"]:\n",
    "            dt  = max(1e-6, time.time() - t0)\n",
    "            tps = int((step * tokens_per_batch) / dt)\n",
    "            vram = torch.cuda.max_memory_allocated() / (1024**3) if DEVICE==\"cuda\" else 0.0\n",
    "            print(f\"[S{step:05d}] loss={loss.item():.4f} (byol {L_byol.item():.4f} | mfr {L_mfr.item():.4f}) | \"\n",
    "                  f\"tps={tps:,} | VRAM~{vram:.2f} GB\")\n",
    "            write_log_row(step, loss.item(), L_byol.item(), L_mfr.item(), tps, vram)\n",
    "\n",
    "    # ---- Save final checkpoints ----\n",
    "    torch.save(model_s.state_dict(), STUDENT_OUT)\n",
    "    torch.save(model_t.state_dict(), TEACHER_OUT)\n",
    "\n",
    "    meta = {\n",
    "        \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"device\": DEVICE,\n",
    "        \"dtype\": str(AMP_DTYPE).split(\".\")[-1],\n",
    "        \"slides_2scale\": len(slide_ids),\n",
    "        \"config\": CFG,\n",
    "        \"student_path\": str(STUDENT_OUT),\n",
    "        \"teacher_path\": str(TEACHER_OUT),\n",
    "    }\n",
    "    MANIFEST.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "    print(\"\\n[OK] Saved:\")\n",
    "    print(\" -\", STUDENT_OUT)\n",
    "    print(\" -\", TEACHER_OUT)\n",
    "    print(\" -\", MANIFEST)\n",
    "\n",
    "print(f\"\\n[CHECK] checkpoints_present = {STUDENT_OUT.exists() and TEACHER_OUT.exists()}\")\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# COMPLETE TCGA EVALUATION PIPELINE\n",
    "# Trains and evaluates cancer classification on TCGA dataset to establish baseline performance for comparison with external validation (CAM16/17/PANDA).\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, \n",
    "    classification_report, confusion_matrix,\n",
    "    balanced_accuracy_score\n",
    ")\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    # Paths\n",
    "    OPENSLIDE = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\")\n",
    "    DL_V2 = Path(r\"D:\\个人文件夹\\Sanwal\\DL_V2\")\n",
    "    \n",
    "    # Data\n",
    "    EMBEDDINGS = DL_V2 / \"artifacts\" / \"embeddings\" / \"patient_means_clean_run_20250908_020405_emb_openclip_vitb16_turbo.parquet\"\n",
    "    LABELS = DL_V2 / \"artifacts\" / \"labels\" / \"labels.csv\"\n",
    "    MANIFEST = OPENSLIDE / \"manifests\" / \"manifest_tcga.csv\"\n",
    "    \n",
    "    # Output\n",
    "    OUTPUT = OPENSLIDE / \"results\" / \"tcga_baseline_evaluation\"\n",
    "    \n",
    "    # Model\n",
    "    HIDDEN_DIM = 256\n",
    "    DROPOUT = 0.3\n",
    "    LEARNING_RATE = 1e-3\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 64\n",
    "    PATIENCE = 10\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITIES\n",
    "# ============================================================================\n",
    "def print_header(text):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" {text}\")\n",
    "    print('='*80)\n",
    "\n",
    "def print_subheader(text):\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\" {text}\")\n",
    "    print('-'*80)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING\n",
    "# ============================================================================\n",
    "def load_data():\n",
    "    \"\"\"Load embeddings, labels, and manifest\"\"\"\n",
    "    print_header(\"1. LOADING DATA\")\n",
    "    \n",
    "    # Load embeddings\n",
    "    print(\"\\n📦 Loading embeddings...\")\n",
    "    df_emb = pd.read_parquet(CFG.EMBEDDINGS)\n",
    "    print(f\"  ✓ Embeddings: {df_emb.shape}\")\n",
    "    print(f\"    Patients: {len(df_emb)}\")\n",
    "    print(f\"    Features: {df_emb.shape[1]}\")\n",
    "    \n",
    "    # Load labels\n",
    "    print(\"\\n📋 Loading labels...\")\n",
    "    df_labels = pd.read_csv(CFG.LABELS)\n",
    "    print(f\"  ✓ Labels: {df_labels.shape}\")\n",
    "    \n",
    "    # Check split distribution\n",
    "    if 'split' in df_labels.columns:\n",
    "        split_dist = df_labels['split'].value_counts()\n",
    "        print(f\"\\n  Split distribution:\")\n",
    "        for split, count in split_dist.items():\n",
    "            print(f\"    {split}: {count}\")\n",
    "    \n",
    "    # Load manifest for cancer codes\n",
    "    print(\"\\n🗂️  Loading manifest...\")\n",
    "    df_manifest = pd.read_csv(CFG.MANIFEST)\n",
    "    print(f\"  ✓ Manifest: {df_manifest.shape}\")\n",
    "    print(f\"    Total slides: {len(df_manifest)}\")\n",
    "    print(f\"    Cancer types: {df_manifest['cancer_code'].nunique()}\")\n",
    "    \n",
    "    return df_emb, df_labels, df_manifest\n",
    "\n",
    "def prepare_dataset(df_emb, df_labels, df_manifest):\n",
    "    \"\"\"Prepare train/test datasets with labels\"\"\"\n",
    "    print_header(\"2. PREPARING DATASET\")\n",
    "    \n",
    "    # Extract patient IDs from embeddings index\n",
    "    print(\"\\n🔗 Mapping patients to cancer types...\")\n",
    "    \n",
    "    # Get patient-to-cancer mapping from manifest\n",
    "    # Extract patient ID from slide_id (e.g., TCGA-02-0001-01A-01-TS1 -> TCGA-02-0001)\n",
    "    df_manifest['patient_id'] = df_manifest['slide_id'].str.extract(r'(TCGA-[A-Z0-9]{2}-[A-Z0-9]{4})', expand=False)\n",
    "    \n",
    "    # Get unique patient-cancer mapping\n",
    "    patient_cancer_map = df_manifest.groupby('patient_id')['cancer_code'].first().to_dict()\n",
    "    \n",
    "    # Map embeddings to cancer types\n",
    "    df_emb['patient_id'] = df_emb.index\n",
    "    df_emb['cancer_type'] = df_emb['patient_id'].map(patient_cancer_map)\n",
    "    \n",
    "    # Remove patients without cancer labels\n",
    "    df_emb_labeled = df_emb[df_emb['cancer_type'].notna()].copy()\n",
    "    print(f\"  ✓ Patients with labels: {len(df_emb_labeled)}\")\n",
    "    print(f\"    Removed {len(df_emb) - len(df_emb_labeled)} patients without labels\")\n",
    "    \n",
    "    # Add split information from labels.csv if available\n",
    "    if 'split' in df_labels.columns:\n",
    "        # Create patient-split mapping\n",
    "        df_labels['patient_id'] = df_labels['patient']\n",
    "        patient_split_map = df_labels.set_index('patient_id')['split'].to_dict()\n",
    "        df_emb_labeled['split'] = df_emb_labeled['patient_id'].map(patient_split_map)\n",
    "        \n",
    "        # Use patients with defined splits\n",
    "        df_emb_labeled = df_emb_labeled[df_emb_labeled['split'].notna()].copy()\n",
    "        print(f\"  ✓ Patients with train/test split: {len(df_emb_labeled)}\")\n",
    "    else:\n",
    "        # Create random split if none exists\n",
    "        print(\"  ⚠️  No split found, creating 80/10/10 split...\")\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        patients = df_emb_labeled['patient_id'].values\n",
    "        train_val, test = train_test_split(patients, test_size=0.1, random_state=42)\n",
    "        train, val = train_test_split(train_val, test_size=0.111, random_state=42)  # 0.111 * 0.9 ≈ 0.1\n",
    "        \n",
    "        split_map = {}\n",
    "        for p in train: split_map[p] = 'train'\n",
    "        for p in val: split_map[p] = 'val'\n",
    "        for p in test: split_map[p] = 'test'\n",
    "        df_emb_labeled['split'] = df_emb_labeled['patient_id'].map(split_map)\n",
    "    \n",
    "    # Show cancer type distribution\n",
    "    print(f\"\\n📊 Cancer type distribution:\")\n",
    "    cancer_counts = df_emb_labeled['cancer_type'].value_counts()\n",
    "    print(f\"  Total cancer types: {len(cancer_counts)}\")\n",
    "    print(f\"  Top 10:\")\n",
    "    for cancer, count in cancer_counts.head(10).items():\n",
    "        print(f\"    {cancer}: {count}\")\n",
    "    \n",
    "    # Show split distribution\n",
    "    print(f\"\\n📊 Split distribution:\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        count = (df_emb_labeled['split'] == split).sum()\n",
    "        print(f\"  {split}: {count}\")\n",
    "    \n",
    "    # Prepare feature matrix and labels\n",
    "    feature_cols = [c for c in df_emb_labeled.columns if c.startswith('f')]\n",
    "    X = df_emb_labeled[feature_cols].values.astype(np.float32)\n",
    "    \n",
    "    # Encode cancer types\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df_emb_labeled['cancer_type'].values)\n",
    "    \n",
    "    print(f\"\\n✓ Feature matrix: {X.shape}\")\n",
    "    print(f\"✓ Number of classes: {len(le.classes_)}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_mask = df_emb_labeled['split'] == 'train'\n",
    "    val_mask = df_emb_labeled['split'] == 'val'\n",
    "    test_mask = df_emb_labeled['split'] == 'test'\n",
    "    \n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "    \n",
    "    print(f\"\\n✓ Train: {X_train.shape}, {len(np.unique(y_train))} classes\")\n",
    "    print(f\"✓ Val:   {X_val.shape}, {len(np.unique(y_val))} classes\")\n",
    "    print(f\"✓ Test:  {X_test.shape}, {len(np.unique(y_test))} classes\")\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), le, df_emb_labeled\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL\n",
    "# ============================================================================\n",
    "class CancerClassifier(nn.Module):\n",
    "    \"\"\"Simple MLP for cancer classification\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "def train_model(train_data, val_data, num_classes):\n",
    "    \"\"\"Train cancer classifier\"\"\"\n",
    "    print_header(\"3. TRAINING MODEL\")\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    X_val, y_val = val_data\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.from_numpy(X_train),\n",
    "        torch.from_numpy(y_train).long()\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.from_numpy(X_val),\n",
    "        torch.from_numpy(y_val).long()\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = CancerClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=CFG.HIDDEN_DIM,\n",
    "        num_classes=num_classes,\n",
    "        dropout=CFG.DROPOUT\n",
    "    ).to(CFG.DEVICE)\n",
    "    \n",
    "    print(f\"\\n🧠 Model architecture:\")\n",
    "    print(f\"  Input dim: {input_dim}\")\n",
    "    print(f\"  Hidden dim: {CFG.HIDDEN_DIM}\")\n",
    "    print(f\"  Output classes: {num_classes}\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CFG.LEARNING_RATE,\n",
    "        weight_decay=CFG.WEIGHT_DECAY\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\n🚀 Starting training...\")\n",
    "    print(f\"  Epochs: {CFG.EPOCHS}\")\n",
    "    print(f\"  Batch size: {CFG.BATCH_SIZE}\")\n",
    "    print(f\"  Learning rate: {CFG.LEARNING_RATE}\")\n",
    "    print(f\"  Device: {CFG.DEVICE}\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(1, CFG.EPOCHS + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(CFG.DEVICE)\n",
    "            y_batch = y_batch.to(CFG.DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(CFG.DEVICE)\n",
    "                y_batch = y_batch.to(CFG.DEVICE)\n",
    "                \n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_true.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = accuracy_score(val_true, val_preds)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"  Epoch {epoch:3d}/{CFG.EPOCHS} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Ensure output directory exists\n",
    "            CFG.OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Save best model (workaround for Unicode path issue)\n",
    "            # Save to temp file, then copy using pure Python binary I/O\n",
    "            with tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='.pth') as tmp:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_acc': val_acc\n",
    "                }, tmp)\n",
    "                tmp_path = tmp.name\n",
    "            \n",
    "            # Copy using pure Python binary I/O (handles Unicode)\n",
    "            final_path = CFG.OUTPUT / \"best_model.pth\"\n",
    "            try:\n",
    "                with open(tmp_path, 'rb') as src:\n",
    "                    with open(final_path, 'wb') as dst:\n",
    "                        dst.write(src.read())\n",
    "            finally:\n",
    "                os.unlink(tmp_path)  # Delete temp file\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= CFG.PATIENCE:\n",
    "                print(f\"\\n⏸️  Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n✓ Training complete!\")\n",
    "    print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"  Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Load best model (workaround for Unicode path)\n",
    "    model_path = str(CFG.OUTPUT / \"best_model.pth\")\n",
    "    with open(model_path, 'rb') as f:\n",
    "        checkpoint = torch.load(f, map_location=CFG.DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION\n",
    "# ============================================================================\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, test_data, label_encoder):\n",
    "    \"\"\"Evaluate on test set\"\"\"\n",
    "    print_header(\"4. EVALUATING ON TEST SET\")\n",
    "    \n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    # Create dataloader\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.from_numpy(X_test),\n",
    "        torch.from_numpy(y_test).long()\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_true = []\n",
    "    \n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(CFG.DEVICE)\n",
    "        outputs = model(X_batch)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_true.extend(y_batch.numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.vstack(all_probs)\n",
    "    all_true = np.array(all_true)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"\\n📊 Test Set Performance:\")\n",
    "    \n",
    "    # Overall accuracy\n",
    "    acc = accuracy_score(all_true, all_preds)\n",
    "    print(f\"\\n  Overall Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Balanced accuracy\n",
    "    bal_acc = balanced_accuracy_score(all_true, all_preds)\n",
    "    print(f\"  Balanced Accuracy: {bal_acc:.4f}\")\n",
    "    \n",
    "    # Macro F1\n",
    "    macro_f1 = f1_score(all_true, all_preds, average='macro')\n",
    "    print(f\"  Macro F1: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Weighted F1\n",
    "    weighted_f1 = f1_score(all_true, all_preds, average='weighted')\n",
    "    print(f\"  Weighted F1: {weighted_f1:.4f}\")\n",
    "    \n",
    "    # Multi-class AUC (one-vs-rest)\n",
    "    try:\n",
    "        auc_ovr = roc_auc_score(all_true, all_probs, multi_class='ovr', average='macro')\n",
    "        print(f\"  Macro AUC (OvR): {auc_ovr:.4f}\")\n",
    "    except:\n",
    "        auc_ovr = None\n",
    "        print(f\"  Macro AUC (OvR): N/A\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(f\"\\n📋 Classification Report:\")\n",
    "    class_names = label_encoder.classes_\n",
    "    report = classification_report(\n",
    "        all_true, all_preds,\n",
    "        target_names=class_names,\n",
    "        digits=3\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    # Save detailed metrics\n",
    "    results = {\n",
    "        'overall': {\n",
    "            'accuracy': float(acc),\n",
    "            'balanced_accuracy': float(bal_acc),\n",
    "            'macro_f1': float(macro_f1),\n",
    "            'weighted_f1': float(weighted_f1),\n",
    "            'macro_auc_ovr': float(auc_ovr) if auc_ovr is not None else None,\n",
    "            'num_samples': int(len(all_true)),\n",
    "            'num_classes': int(len(class_names))\n",
    "        },\n",
    "        'per_class': classification_report(\n",
    "            all_true, all_preds,\n",
    "            target_names=class_names,\n",
    "            output_dict=True\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_true, all_preds)\n",
    "    \n",
    "    return results, cm, all_preds, all_probs, all_true\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "def plot_results(history, cm, label_encoder):\n",
    "    \"\"\"Create visualization plots\"\"\"\n",
    "    print_header(\"5. CREATING VISUALIZATIONS\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    # Training curves\n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation accuracy\n",
    "    ax2 = plt.subplot(1, 3, 2)\n",
    "    ax2.plot(epochs, history['val_acc'], 'g-', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax2.set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confusion matrix (top 15 classes by support)\n",
    "    ax3 = plt.subplot(1, 3, 3)\n",
    "    \n",
    "    # Get top classes\n",
    "    class_support = cm.sum(axis=1)\n",
    "    top_indices = np.argsort(class_support)[-15:][::-1]\n",
    "    cm_top = cm[np.ix_(top_indices, top_indices)]\n",
    "    class_names_top = label_encoder.classes_[top_indices]\n",
    "    \n",
    "    sns.heatmap(cm_top, annot=False, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names_top, yticklabels=class_names_top,\n",
    "                ax=ax3, cbar_kws={'label': 'Count'})\n",
    "    ax3.set_title('Confusion Matrix (Top 15 Classes)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Predicted', fontsize=12)\n",
    "    ax3.set_ylabel('True', fontsize=12)\n",
    "    plt.setp(ax3.get_xticklabels(), rotation=45, ha='right', fontsize=8)\n",
    "    plt.setp(ax3.get_yticklabels(), rotation=0, fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = str(CFG.OUTPUT / 'training_results.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  ✓ Saved: training_results.png\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "def save_results(results, cm, label_encoder, df_labeled, y_pred, y_true):\n",
    "    \"\"\"Save all results to disk\"\"\"\n",
    "    print_header(\"6. SAVING RESULTS\")\n",
    "    \n",
    "    # Save metrics JSON\n",
    "    with open(str(CFG.OUTPUT / 'test_metrics.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"  ✓ Saved: test_metrics.json\")\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=label_encoder.classes_,\n",
    "        columns=label_encoder.classes_\n",
    "    )\n",
    "    cm_df.to_csv(str(CFG.OUTPUT / 'confusion_matrix.csv'))\n",
    "    print(f\"  ✓ Saved: confusion_matrix.csv\")\n",
    "    \n",
    "    # Save per-class metrics\n",
    "    per_class_df = pd.DataFrame(results['per_class']).T\n",
    "    per_class_df.to_csv(str(CFG.OUTPUT / 'per_class_metrics.csv'))\n",
    "    print(f\"  ✓ Saved: per_class_metrics.csv\")\n",
    "    \n",
    "    # Save predictions\n",
    "    test_mask = df_labeled['split'] == 'test'\n",
    "    test_patients = df_labeled[test_mask]['patient_id'].values\n",
    "    \n",
    "    pred_df = pd.DataFrame({\n",
    "        'patient_id': test_patients,\n",
    "        'true_label': label_encoder.inverse_transform(y_true),\n",
    "        'pred_label': label_encoder.inverse_transform(y_pred),\n",
    "        'correct': y_true == y_pred\n",
    "    })\n",
    "    pred_df.to_csv(str(CFG.OUTPUT / 'test_predictions.csv'), index=False)\n",
    "    print(f\"  ✓ Saved: test_predictions.csv\")\n",
    "    \n",
    "    # Create summary\n",
    "    summary = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'dataset': 'TCGA',\n",
    "        'num_patients': len(df_labeled),\n",
    "        'num_classes': len(label_encoder.classes_),\n",
    "        'train_size': int((df_labeled['split'] == 'train').sum()),\n",
    "        'val_size': int((df_labeled['split'] == 'val').sum()),\n",
    "        'test_size': int((df_labeled['split'] == 'test').sum()),\n",
    "        'model': {\n",
    "            'type': 'MLP',\n",
    "            'hidden_dim': CFG.HIDDEN_DIM,\n",
    "            'dropout': CFG.DROPOUT,\n",
    "            'learning_rate': CFG.LEARNING_RATE,\n",
    "            'weight_decay': CFG.WEIGHT_DECAY\n",
    "        },\n",
    "        'results': results['overall']\n",
    "    }\n",
    "    \n",
    "    with open(str(CFG.OUTPUT / 'summary.json'), 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"  ✓ Saved: summary.json\")\n",
    "    \n",
    "    print(f\"\\n✓ All results saved to: {CFG.OUTPUT}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\" TCGA BASELINE EVALUATION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Output directory: {CFG.OUTPUT}\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    CFG.OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Load data\n",
    "    df_emb, df_labels, df_manifest = load_data()\n",
    "    \n",
    "    # 2. Prepare dataset\n",
    "    train_data, val_data, test_data, label_encoder, df_labeled = prepare_dataset(\n",
    "        df_emb, df_labels, df_manifest\n",
    "    )\n",
    "    \n",
    "    # 3. Train model\n",
    "    model, history = train_model(train_data, val_data, num_classes=len(label_encoder.classes_))\n",
    "    \n",
    "    # 4. Evaluate\n",
    "    results, cm, y_pred, y_probs, y_true = evaluate_model(model, test_data, label_encoder)\n",
    "    \n",
    "    # 5. Visualize\n",
    "    plot_results(history, cm, label_encoder)\n",
    "    \n",
    "    # 6. Save\n",
    "    save_results(results, cm, label_encoder, df_labeled, y_pred, y_true)\n",
    "    \n",
    "    # Final summary\n",
    "    print_header(\"SUMMARY\")\n",
    "    print(f\"\\n✅ TCGA Baseline Evaluation Complete!\")\n",
    "    print(f\"\\n📊 Key Metrics:\")\n",
    "    print(f\"  Test Accuracy: {results['overall']['accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {results['overall']['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  Macro F1: {results['overall']['macro_f1']:.4f}\")\n",
    "    if results['overall']['macro_auc_ovr'] is not None:\n",
    "        print(f\"  Macro AUC (OvR): {results['overall']['macro_auc_ovr']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📁 Results saved to: {CFG.OUTPUT}\")\n",
    "    print(f\"\\n💡 Next Steps:\")\n",
    "    print(f\"  1. Compare TCGA test metrics with CAM16/17/PANDA\")\n",
    "    print(f\"  2. Calculate performance drop: (CAM16 - TCGA test)\")\n",
    "    print(f\"  3. Include in publication tables\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Script 6C — Post-Pretraining Diagnostics\n",
    "\n",
    "import os, sys, json, math, hashlib, shutil, subprocess, warnings, tempfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# -------- Paths --------\n",
    "WORKSPACE   = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\")\n",
    "LOGS_DIR    = WORKSPACE / \"logs\"\n",
    "MODELS_DIR  = WORKSPACE / \"models\"\n",
    "FEAT05_DIR  = WORKSPACE / \"features\" / \"scale0p5\"\n",
    "FEAT20_DIR  = WORKSPACE / \"features\" / \"scale2p0\"\n",
    "DIAG_DIR    = WORKSPACE / \"diagnostics\"\n",
    "DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_CSV = LOGS_DIR / \"script6_train_log.csv\"\n",
    "\n",
    "# -------- Deps (install quietly if missing) --------\n",
    "def _ensure(pkgs):\n",
    "    miss=[]\n",
    "    for name, spec in pkgs:\n",
    "        try: __import__(name)\n",
    "        except Exception: miss.append(spec)\n",
    "    if miss:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *miss])\n",
    "\n",
    "_ensure([(\"pandas\",\"pandas>=2.0\"), (\"numpy\",\"numpy>=1.24\")])\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    HAS_TORCH = True\n",
    "except Exception:\n",
    "    HAS_TORCH = False\n",
    "\n",
    "# -------- Helpers --------\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists(): return pd.DataFrame()\n",
    "    # copy to temp to avoid Windows file-lock while training writes\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp:\n",
    "        tmp_path = Path(tmp.name)\n",
    "    try:\n",
    "        shutil.copy2(path, tmp_path)\n",
    "        df = pd.read_csv(tmp_path)\n",
    "    except Exception:\n",
    "        df = pd.DataFrame()\n",
    "    finally:\n",
    "        try: tmp_path.unlink(missing_ok=True)\n",
    "        except: pass\n",
    "    return df\n",
    "\n",
    "def list_ckpts(models_dir: Path):\n",
    "    exts = (\".pt\",\".pth\",\".safetensors\")\n",
    "    return sorted([p for p in models_dir.glob(\"*\") if p.suffix.lower() in exts],\n",
    "                  key=lambda x: x.stat().st_mtime)\n",
    "\n",
    "def sha256_12(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path,\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024*1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()[:12]\n",
    "\n",
    "def try_torch_load(path: Path):\n",
    "    if not HAS_TORCH: return False, {\"error\":\"torch not available\"}\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "        meta = {\"type\": type(obj).__name__}\n",
    "        if isinstance(obj, dict): meta[\"top_keys\"] = list(obj.keys())[:8]\n",
    "        return True, meta\n",
    "    except Exception as e:\n",
    "        return False, {\"error\": str(e)[:180]}\n",
    "\n",
    "def count_2scale_slides():\n",
    "    s05 = {p.stem for p in FEAT05_DIR.glob(\"*.npy\")}\n",
    "    s20 = {p.stem for p in FEAT20_DIR.glob(\"*.npy\")}\n",
    "    return len(s05 & s20), len(s05), len(s20)\n",
    "\n",
    "def rolling_median(x: pd.Series, frac=0.1):\n",
    "    if len(x) == 0: return np.nan\n",
    "    k = max(3, int(len(x)*frac))\n",
    "    if k % 2 == 0: k += 1\n",
    "    return x.rolling(k, center=True, min_periods=max(3,k//3)).median()\n",
    "\n",
    "# -------- Load logs (robust to missing cols) --------\n",
    "df = safe_read_csv(LOG_CSV)\n",
    "diag = {\"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"workspace\": str(WORKSPACE),\n",
    "        \"log_csv_exists\": LOG_CSV.exists(),\n",
    "        \"log_rows\": int(len(df))}\n",
    "\n",
    "for c in [\"epoch\",\"step\",\"loss\",\"loss_byol\",\"loss_mfr\",\"tps\",\"vram_gb\",\"ts\"]:\n",
    "    if c not in df.columns:\n",
    "        if c == \"ts\":\n",
    "            df[c] = datetime.now().isoformat(timespec=\"seconds\")\n",
    "        else:\n",
    "            df[c] = np.nan\n",
    "\n",
    "# Coerce numeric\n",
    "for c in [\"epoch\",\"step\",\"loss\",\"loss_byol\",\"loss_mfr\",\"tps\",\"vram_gb\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# -------- Derive training stats --------\n",
    "steps_logged = int(df[\"step\"].max()) if len(df) else 0\n",
    "diag[\"steps_logged\"] = steps_logged\n",
    "\n",
    "# Loss trend (smoothed medians over first/last ~10%)\n",
    "if len(df) > 3 and df[\"loss\"].notna().any():\n",
    "    n = len(df)\n",
    "    head = df[\"loss\"].dropna().iloc[:max(3, n//10)]\n",
    "    tail = df[\"loss\"].dropna().iloc[-max(3, n//10):]\n",
    "    start_med = float(np.median(head)) if len(head) else np.nan\n",
    "    end_med   = float(np.median(tail)) if len(tail) else np.nan\n",
    "    rel_impr  = float((start_med - end_med) / start_med) if (start_med and start_med==start_med) else np.nan\n",
    "else:\n",
    "    start_med = end_med = rel_impr = np.nan\n",
    "\n",
    "diag[\"loss_start_median\"] = start_med\n",
    "diag[\"loss_end_median\"]   = end_med\n",
    "diag[\"loss_rel_improvement\"] = rel_impr\n",
    "\n",
    "# TPS (tokens/sec) robust median over recent rows (filter tiny/NaN)\n",
    "RECENT_ROWS = 200\n",
    "MIN_VALID_TPS = 100\n",
    "tps_recent = None\n",
    "if len(df):\n",
    "    tail = df.tail(RECENT_ROWS).copy()\n",
    "    good = tail[\"tps\"].where((tail[\"tps\"] > MIN_VALID_TPS) & np.isfinite(tail[\"tps\"]))\n",
    "    if good.notna().any():\n",
    "        tps_recent = float(np.nanmedian(good))\n",
    "    elif len(df) >= 2:\n",
    "        # 2-point fallback from last two rows\n",
    "        r1, r0 = df.iloc[-1], df.iloc[-2]\n",
    "        try:\n",
    "            t1 = datetime.fromisoformat(str(r1[\"ts\"]))\n",
    "            t0 = datetime.fromisoformat(str(r0[\"ts\"]))\n",
    "            dt = (t1 - t0).total_seconds()\n",
    "        except Exception:\n",
    "            dt = None\n",
    "        dstep = (r1[\"step\"] - r0[\"step\"]) if (np.isfinite(r1[\"step\"]) and np.isfinite(r0[\"step\"])) else 0\n",
    "        # Use your Script-6 batch sizing (3 slides × (1200+400) tokens)\n",
    "        TOKENS_PER_BATCH = 3 * (1200 + 400)\n",
    "        if dt and dt > 0 and dstep > 0:\n",
    "            tps_recent = float((dstep * TOKENS_PER_BATCH) / dt)\n",
    "\n",
    "diag[\"tps_recent_median\"] = tps_recent if tps_recent is not None else None\n",
    "diag[\"vram_last_gb\"] = float(df[\"vram_gb\"].dropna().iloc[-1]) if df[\"vram_gb\"].notna().any() else None\n",
    "\n",
    "# Staleness\n",
    "last_ts = None\n",
    "if len(df):\n",
    "    try: last_ts = datetime.fromisoformat(str(df[\"ts\"].iloc[-1]))\n",
    "    except Exception: last_ts = None\n",
    "diag[\"last_log_update\"] = last_ts.isoformat(timespec=\"seconds\") if last_ts else None\n",
    "diag[\"log_stale_over_5min\"] = bool((datetime.now() - last_ts) > timedelta(minutes=5)) if last_ts else None\n",
    "\n",
    "# -------- Feature coverage (2-scale) --------\n",
    "n_both, n05, n20 = count_2scale_slides()\n",
    "diag[\"features_2scale_intersection\"] = int(n_both)\n",
    "diag[\"features_0p5_count\"] = int(n05)\n",
    "diag[\"features_2p0_count\"] = int(n20)\n",
    "\n",
    "# -------- Checkpoints --------\n",
    "ckpts = list_ckpts(MODELS_DIR)\n",
    "diag[\"checkpoint_count\"] = int(len(ckpts))\n",
    "ckpt_info = []\n",
    "for p in ckpts[-6:]:\n",
    "    ok, meta = try_torch_load(p)\n",
    "    ckpt_info.append({\n",
    "        \"file\": str(p),\n",
    "        \"size_mb\": round(p.stat().st_size/(1024**2),2),\n",
    "        \"sha256_12\": sha256_12(p),\n",
    "        \"load_ok\": bool(ok),\n",
    "        \"meta\": meta\n",
    "    })\n",
    "diag[\"checkpoints_recent\"] = ckpt_info\n",
    "\n",
    "# Suggested selection (latest by mtime)\n",
    "diag[\"suggest_checkpoint\"] = (str(ckpts[-1]) if len(ckpts) else None)\n",
    "\n",
    "# -------- Gates (PASS/WARN/FAIL) --------\n",
    "gates = []\n",
    "\n",
    "# G1: 2-scale coverage\n",
    "if n_both >= 18000:\n",
    "    gates.append((\"G1_2scale_coverage\", \"PASS\", f\"{n_both} slides with both scales\"))\n",
    "elif n_both >= 15000:\n",
    "    gates.append((\"G1_2scale_coverage\", \"WARN\", f\"{n_both} < expected; verify features export\"))\n",
    "else:\n",
    "    gates.append((\"G1_2scale_coverage\", \"FAIL\", f\"{n_both} very low; investigate features export\"))\n",
    "\n",
    "# G2: loss improvement\n",
    "if rel_impr == rel_impr:  # not NaN\n",
    "    if rel_impr >= 0.60:\n",
    "        gates.append((\"G2_loss_improvement\", \"PASS\", f\"relative drop {rel_impr:.2f}\"))\n",
    "    elif rel_impr >= 0.30:\n",
    "        gates.append((\"G2_loss_improvement\", \"WARN\", f\"modest drop {rel_impr:.2f}\"))\n",
    "    else:\n",
    "        gates.append((\"G2_loss_improvement\", \"FAIL\", f\"weak drop {rel_impr:.2f}\"))\n",
    "else:\n",
    "    gates.append((\"G2_loss_improvement\", \"WARN\", \"loss trend unavailable\"))\n",
    "\n",
    "# G3: throughput (tokens/sec)\n",
    "if tps_recent is None:\n",
    "    gates.append((\"G3_throughput\", \"WARN\", \"no recent TPS in logs\"))\n",
    "elif tps_recent >= 20000:\n",
    "    gates.append((\"G3_throughput\", \"PASS\", f\"{tps_recent:.0f} tok/s\"))\n",
    "elif tps_recent >= 5000:\n",
    "    gates.append((\"G3_throughput\", \"WARN\", f\"{tps_recent:.0f} tok/s\"))\n",
    "else:\n",
    "    gates.append((\"G3_throughput\", \"FAIL\", f\"{tps_recent:.0f} tok/s\"))\n",
    "\n",
    "# G4: checkpoints presence & loadability\n",
    "if len(ckpts) == 0:\n",
    "    gates.append((\"G4_checkpoints\", \"FAIL\", \"no model files in /models\"))\n",
    "elif any(not c[\"load_ok\"] for c in ckpt_info):\n",
    "    bad = sum(1 for c in ckpt_info if not c[\"load_ok\"])\n",
    "    gates.append((\"G4_checkpoints\", \"WARN\", f\"{bad} recent checkpoint(s) failed to load\"))\n",
    "else:\n",
    "    gates.append((\"G4_checkpoints\", \"PASS\", f\"{len(ckpts)} file(s), latest loads OK\"))\n",
    "\n",
    "diag[\"gates\"] = [{\"name\": n, \"status\": s, \"detail\": d} for (n,s,d) in gates]\n",
    "\n",
    "# -------- Save reports --------\n",
    "OUT_JSON = DIAG_DIR / \"script6b_posttrain_diagnostics.json\"\n",
    "OUT_TXT  = DIAG_DIR / \"script6b_posttrain_diagnostics.txt\"\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(diag, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "lines = []\n",
    "lines.append(f\"== Script 6B — Post-Pretraining Diagnostics (no matplotlib) ==\")\n",
    "lines.append(f\"time={diag['time']}\")\n",
    "lines.append(f\"workspace={diag['workspace']}\")\n",
    "lines.append(f\"log_csv_exists={diag['log_csv_exists']} rows={diag['log_rows']} steps_logged={diag['steps_logged']}\")\n",
    "lines.append(f\"loss_start_median={diag['loss_start_median']}\")\n",
    "lines.append(f\"loss_end_median={diag['loss_end_median']}\")\n",
    "lines.append(f\"loss_rel_improvement={diag['loss_rel_improvement']}\")\n",
    "lines.append(f\"tps_recent_median={diag['tps_recent_median']}\")\n",
    "lines.append(f\"vram_last_gb={diag['vram_last_gb']}\")\n",
    "lines.append(f\"last_log_update={diag['last_log_update']}  stale_over_5min={diag['log_stale_over_5min']}\")\n",
    "lines.append(f\"features_2scale_intersection={diag['features_2scale_intersection']}  (0.5={diag['features_0p5_count']}, 2.0={diag['features_2p0_count']})\")\n",
    "lines.append(f\"checkpoint_count={diag['checkpoint_count']}  suggest_checkpoint={diag['suggest_checkpoint']}\")\n",
    "for c in ckpt_info:\n",
    "    lines.append(f\"  - {c['file']}  size={c['size_mb']} MB  sha256[:12]={c['sha256_12']}  load_ok={c['load_ok']}  meta={c['meta']}\")\n",
    "\n",
    "lines.append(\"\\nGATES:\")\n",
    "for (n,s,d) in gates:\n",
    "    tag = {\"PASS\":\"[ OK ]\", \"WARN\":\"[WARN]\", \"FAIL\":\"[FAIL]\"}[s]\n",
    "    lines.append(f\" {tag} {n}: {d}\")\n",
    "\n",
    "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"\\n\".join(lines))\n",
    "print(f\"\\n[OK] Saved: {OUT_JSON}\")\n",
    "print(f\"[OK] Saved: {OUT_TXT}\")\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Script 7 CAMELYON16/17y Feature Extraction \n",
    "\n",
    "import os, sys, re, json, time, math, random, shutil, gc, hashlib, subprocess, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------- Paths -----------------------\n",
    "WORKSPACE = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\")\n",
    "RAW_CAM16 = WORKSPACE / r\"Raw Data\" / \"CAMELYON16\"\n",
    "RAW_CAM17 = WORKSPACE / r\"Raw Data\" / \"CAMELYON17\"\n",
    "\n",
    "MANIFESTS = WORKSPACE / \"manifests\"\n",
    "QC_DIR    = WORKSPACE / \"qc\"\n",
    "FEAT05    = WORKSPACE / \"features\" / \"scale0p5\"\n",
    "FEAT20    = WORKSPACE / \"features\" / \"scale2p0\"\n",
    "LOG_DIR   = WORKSPACE / \"logs\"\n",
    "COMP_DIR  = WORKSPACE / \"compute\"\n",
    "for p in [MANIFESTS, QC_DIR, FEAT05, FEAT20, LOG_DIR, COMP_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------- Deps -----------------------\n",
    "def _ensure(pkgs):\n",
    "    miss=[]\n",
    "    for name, spec in pkgs:\n",
    "        try: __import__(name)\n",
    "        except Exception: miss.append(spec)\n",
    "    if miss:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *miss])\n",
    "\n",
    "_ensure([\n",
    "    (\"pandas\",\"pandas>=2.0\"),\n",
    "    (\"numpy\",\"numpy>=1.24\"),\n",
    "    (\"openslide\",\"openslide-python>=1.2\"),\n",
    "    (\"PIL\",\"Pillow>=10.0\"),\n",
    "    (\"torch\",\"torch>=2.1\"),\n",
    "    (\"torchvision\",\"torchvision>=0.16\"),\n",
    "    (\"pyarrow\",\"pyarrow>=14\"),\n",
    "])\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "import openslide\n",
    "from PIL import Image\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import torchvision.models as tvm\n",
    "from torchvision.transforms.functional import to_tensor as _to_tensor\n",
    "\n",
    "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "AMP_DTYPE = torch.float16 if DEVICE==\"cuda\" else torch.bfloat16\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# ----------------------- Config (same as TCGA) -----------------------\n",
    "CFG = {\n",
    "  \"scales_um_per_px\": [0.5, 2.0],\n",
    "  \"tile_px\": 256,\n",
    "  \"tile_overlap\": 32,\n",
    "  \"token_budget\": {0.5: 1200, 2.0: 400},\n",
    "  \"input_size\": 224,\n",
    "  \"batch_size\": 2048,\n",
    "  \"num_workers\": 0,\n",
    "  \"seed\": 1337,\n",
    "  \"print_every_slides\": 50,\n",
    "  # WSI-only filters\n",
    "  \"bad_name_substrings\": [\"_tissue\", \"mask\", \"prob\", \"heatmap\", \"anno\", \"overlay\", \"xml\", \"thumb\", \"down\", \"level\"],\n",
    "  \"min_side_px\": 5000  # require at least one side >= 5000 px to consider WSI-like\n",
    "}\n",
    "\n",
    "random.seed(CFG[\"seed\"]); np.random.seed(CFG[\"seed\"]); torch.manual_seed(CFG[\"seed\"]); torch.cuda.manual_seed_all(CFG[\"seed\"])\n",
    "\n",
    "# ----------------------- Helpers -----------------------\n",
    "def list_tifs(root: Path):\n",
    "    exts = {\".svs\",\".tif\",\".tiff\",\".ndpi\",\".mrxs\",\".scn\",\".svslide\",\".bif\",\".vms\",\".vmu\"}\n",
    "    out = []\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in exts:\n",
    "            out.append(p)\n",
    "    return sorted(out)\n",
    "\n",
    "def looks_like_aux(name_stem: str) -> bool:\n",
    "    low = name_stem.lower()\n",
    "    return any(b in low for b in CFG[\"bad_name_substrings\"])\n",
    "\n",
    "def mpp_xy(slide: openslide.OpenSlide):\n",
    "    x = slide.properties.get(\"openslide.mpp-x\")\n",
    "    y = slide.properties.get(\"openslide.mpp-y\")\n",
    "    try:\n",
    "        return (float(x), float(y))\n",
    "    except Exception:\n",
    "        # heuristic fallback; CAMELYON often ~0.25 µm/px at base\n",
    "        return (0.25, 0.25)\n",
    "\n",
    "def level_for_um(slide: openslide.OpenSlide, target_um):\n",
    "    base_x, _ = mpp_xy(slide)\n",
    "    best = 0; best_diff = 1e9\n",
    "    for lvl in range(slide.level_count):\n",
    "        mpp = base_x * slide.level_downsamples[lvl]\n",
    "        diff = abs(mpp - target_um)\n",
    "        if diff < best_diff:\n",
    "            best_diff, best = diff, lvl\n",
    "    return best\n",
    "\n",
    "def pil_to_tensor(img_rgb: Image.Image, size=224):\n",
    "    if img_rgb.size != (size, size):\n",
    "        img_rgb = img_rgb.resize((size,size), Image.BILINEAR)\n",
    "    t = _to_tensor(img_rgb)\n",
    "    mean = torch.tensor([0.485,0.456,0.406]).view(3,1,1)\n",
    "    std  = torch.tensor([0.229,0.224,0.225]).view(3,1,1)\n",
    "    return (t - mean) / std\n",
    "\n",
    "def grid_tiles(w, h, size, overlap):\n",
    "    stride = size - overlap\n",
    "    xs = list(range(0, max(1, w-size+1), stride))\n",
    "    ys = list(range(0, max(1, h-size+1), stride))\n",
    "    if len(xs)==0: xs=[0]\n",
    "    if len(ys)==0: ys=[0]\n",
    "    return [(x,y) for y in ys for x in xs]\n",
    "\n",
    "def choose_tiles(slide: openslide.OpenSlide, lvl: int, size: int, overlap: int, budget: int):\n",
    "    w, h = slide.level_dimensions[lvl]\n",
    "    coords = grid_tiles(w, h, size, overlap)\n",
    "    if len(coords) == 0:\n",
    "        coords = [(0,0)]\n",
    "    # sample uniformly up to budget (with replacement if needed)\n",
    "    if len(coords) >= budget:\n",
    "        idx = np.random.choice(len(coords), size=budget, replace=False)\n",
    "    else:\n",
    "        idx = np.random.choice(len(coords), size=budget, replace=True)\n",
    "    return [coords[i] for i in idx]\n",
    "\n",
    "# ----------------------- Model (frozen backbone → 768) -----------------------\n",
    "class ResNet50Proj768(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        m = tvm.resnet50(weights=tvm.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = nn.Sequential(*(list(m.children())[:-1]))  # -> [B,2048,1,1]\n",
    "        self.proj = nn.Linear(2048, 768)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "def build_model():\n",
    "    model = ResNet50Proj768().to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# ----------------------- WSI-only manifest build -----------------------\n",
    "def build_manifest_cam(tag: str, root: Path) -> pd.DataFrame:\n",
    "    files = list_tifs(root)\n",
    "    rows, skipped = [], []\n",
    "    for p in files:\n",
    "        stem = p.stem\n",
    "        # 1) name filter\n",
    "        if looks_like_aux(stem):\n",
    "            skipped.append({\"path\":str(p),\"reason\":\"aux_name\"})\n",
    "            continue\n",
    "        # 2) probe + size filter\n",
    "        try:\n",
    "            with openslide.OpenSlide(str(p)) as s:\n",
    "                w, h = s.dimensions\n",
    "                if max(w,h) < CFG[\"min_side_px\"]:\n",
    "                    skipped.append({\"path\":str(p),\"reason\":f\"too_small_{w}x{h}\"})\n",
    "                    continue\n",
    "                vendor = s.properties.get(\"openslide.vendor\",\"unknown\")\n",
    "                mppx, mppy = mpp_xy(s)\n",
    "        except Exception as e:\n",
    "            skipped.append({\"path\":str(p),\"reason\":f\"open_fail:{type(e).__name__}\"})\n",
    "            continue\n",
    "        rows.append({\n",
    "           \"dataset\": tag,\n",
    "           \"slide_id\": stem,\n",
    "           \"path\": str(p),\n",
    "           \"w\": int(w), \"h\": int(h),\n",
    "           \"mpp_x\": float(mppx), \"mpp_y\": float(mppy),\n",
    "           \"vendor\": vendor\n",
    "        })\n",
    "    df = pd.DataFrame(rows).sort_values(\"slide_id\").reset_index(drop=True)\n",
    "    sk = pd.DataFrame(skipped)\n",
    "    df.to_csv(MANIFESTS / f\"manifest_{tag.lower()}.csv\", index=False)\n",
    "    df.to_parquet(MANIFESTS / f\"manifest_{tag.lower()}.parquet\", index=False)\n",
    "    if len(sk)>0:\n",
    "        sk.to_csv(MANIFESTS / f\"manifest_{tag.lower()}_skipped.csv\", index=False)\n",
    "    print(f\"[OK] Manifest {tag}: {len(df)} slides (skipped {len(sk)}) → {MANIFESTS/f'manifest_{tag.lower()}.csv'}\")\n",
    "    return df\n",
    "\n",
    "# ----------------------- Light QC -----------------------\n",
    "def light_qc(df: pd.DataFrame, tag: str) -> pd.DataFrame:\n",
    "    kept, excl = [], []\n",
    "    for _,row in df.iterrows():\n",
    "        try:\n",
    "            with openslide.OpenSlide(str(row[\"path\"])) as s:\n",
    "                lvl = s.get_best_level_for_downsample(64)\n",
    "                img = s.read_region((0,0), lvl, s.level_dimensions[lvl]).convert(\"RGB\")\n",
    "                a = np.asarray(img)\n",
    "                gray = (0.299*a[...,0] + 0.587*a[...,1] + 0.114*a[...,2]).astype(np.float32)\n",
    "                tissue_frac = float((gray < 240).mean())\n",
    "                white_frac  = float((a.mean(axis=2) > 240).mean())\n",
    "                ok = (tissue_frac >= 0.05) and (white_frac <= 0.99)\n",
    "        except Exception as e:\n",
    "            ok = False\n",
    "        (kept if ok else excl).append(row)\n",
    "    kept_df = pd.DataFrame(kept).reset_index(drop=True)\n",
    "    excl_df = pd.DataFrame(excl).reset_index(drop=True)\n",
    "    kept_df.to_csv(QC_DIR / f\"qc_pass_{tag.lower()}.csv\", index=False)\n",
    "    excl_df.to_csv(QC_DIR / f\"qc_fail_{tag.lower()}.csv\", index=False)\n",
    "    print(f\"[QC] {tag}: kept={len(kept_df)}  excluded={len(excl_df)}\")\n",
    "    return kept_df\n",
    "\n",
    "# ----------------------- Extraction -----------------------\n",
    "@torch.no_grad()\n",
    "def extract_for_slide(model, slide_path: Path, budgets: dict, tile_sz=256, overlap=32, input_sz=224):\n",
    "    out = {}\n",
    "    with openslide.OpenSlide(str(slide_path)) as s:\n",
    "        for scale in CFG[\"scales_um_per_px\"]:\n",
    "            lvl = level_for_um(s, scale)\n",
    "            coords = choose_tiles(s, lvl, tile_sz, overlap, budgets[scale])\n",
    "            batch=[]\n",
    "            for (x,y) in coords:\n",
    "                try:\n",
    "                    img = s.read_region((x,y), lvl, (tile_sz, tile_sz)).convert(\"RGB\")\n",
    "                except Exception as e:\n",
    "                    # if a coordinate is bad due to pyramid quirk, fallback to (0,0)\n",
    "                    img = s.read_region((0,0), lvl, (tile_sz, tile_sz)).convert(\"RGB\")\n",
    "                batch.append(pil_to_tensor(img, size=input_sz))\n",
    "            X = torch.stack(batch, dim=0).to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "            outs=[]\n",
    "            bs = CFG[\"batch_size\"]\n",
    "            for i in range(0, X.shape[0], bs):\n",
    "                chunk = X[i:i+bs]\n",
    "                with torch.amp.autocast(device_type=(\"cuda\" if DEVICE==\"cuda\" else \"cpu\"), dtype=AMP_DTYPE, enabled=True):\n",
    "                    z = model(chunk)\n",
    "                outs.append(z.detach().cpu())\n",
    "            out[scale] = torch.cat(outs, dim=0).numpy().astype(np.float32)\n",
    "            del X, outs; gc.collect()\n",
    "            if DEVICE==\"cuda\": torch.cuda.empty_cache()\n",
    "    return out\n",
    "\n",
    "def save_feats(slide_id: str, arr05: np.ndarray, arr20: np.ndarray):\n",
    "    np.save(FEAT05 / f\"{slide_id}.npy\", arr05)\n",
    "    np.save(FEAT20 / f\"{slide_id}.npy\", arr20)\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    # Passport update\n",
    "    COMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    passport = {\n",
    "        \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"device\": DEVICE,\n",
    "        \"torch\": torch.__version__,\n",
    "        \"gpu\": (torch.cuda.get_device_name(0) if DEVICE==\"cuda\" else \"cpu\"),\n",
    "        \"workspace\": str(WORKSPACE),\n",
    "        \"filters\": {\"bad_name_substrings\": CFG[\"bad_name_substrings\"], \"min_side_px\": CFG[\"min_side_px\"]}\n",
    "    }\n",
    "    (COMP_DIR / \"compute_passport.json\").write_text(json.dumps(passport, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # 1) Build WSI-only manifests (overwrites previous)\n",
    "    df16 = build_manifest_cam(\"CAMELYON16\", RAW_CAM16)\n",
    "    df17 = build_manifest_cam(\"CAMELYON17\", RAW_CAM17)\n",
    "\n",
    "    # 2) Light QC\n",
    "    df16 = light_qc(df16, \"CAMELYON16\")\n",
    "    df17 = light_qc(df17, \"CAMELYON17\")\n",
    "\n",
    "    # 3) Assemble TODO set (skip already done at both scales)\n",
    "    todo = pd.concat([df16, df17], ignore_index=True)\n",
    "    keep=[]\n",
    "    for _,row in todo.iterrows():\n",
    "        sid = row[\"slide_id\"]\n",
    "        if (FEAT05 / f\"{sid}.npy\").exists() and (FEAT20 / f\"{sid}.npy\").exists():\n",
    "            continue\n",
    "        keep.append(row)\n",
    "    todo = pd.DataFrame(keep) if keep else pd.DataFrame(columns=todo.columns)\n",
    "    total = len(todo)\n",
    "    print(f\"[RUN] Pending slides (2 scales missing): {total}\")\n",
    "    if total == 0:\n",
    "        print(\"[DONE] Nothing to do.\")\n",
    "        return\n",
    "\n",
    "    # 4) Model\n",
    "    model = build_model()\n",
    "\n",
    "    # 5) Loop\n",
    "    t0=time.time(); last=t0; done=0\n",
    "    for _,row in todo.iterrows():\n",
    "        sid = row[\"slide_id\"]; sp = Path(row[\"path\"])\n",
    "        try:\n",
    "            d = extract_for_slide(model, sp, CFG[\"token_budget\"],\n",
    "                                  tile_sz=CFG[\"tile_px\"], overlap=CFG[\"tile_overlap\"], input_sz=CFG[\"input_size\"])\n",
    "            save_feats(sid, d[0.5], d[2.0])\n",
    "            done += 1\n",
    "        except Exception as e:\n",
    "            # log the skip\n",
    "            with open(MANIFESTS / \"camelyon_errors.log\", \"a\", encoding=\"utf-8\") as fh:\n",
    "                fh.write(f\"{sid}\\t{sp}\\t{type(e).__name__}:{e}\\n\")\n",
    "        # progress\n",
    "        now=time.time()\n",
    "        if (done % CFG[\"print_every_slides\"]==0) or (done==total) or (now-last>60):\n",
    "            dt = now - t0\n",
    "            eps = (done*2) / max(1e-6, dt)  # entries/sec (two scales per slide)\n",
    "            print(f\"[{done:5d}/{total}] slide={sid}  elapsed={dt/60:.1f} min  entries/s={eps:.2f}  VRAM~{(torch.cuda.max_memory_allocated()/(1024**3) if DEVICE=='cuda' else 0):.2f} GB\")\n",
    "            last=now\n",
    "\n",
    "    # 6) Summary\n",
    "    s05 = len(list(FEAT05.glob(\"*.npy\"))); s20 = len(list(FEAT20.glob(\"*.npy\")))\n",
    "    common = len(set([p.stem for p in FEAT05.glob(\"*.npy\")]) & set([p.stem for p in FEAT20.glob(\"*.npy\")]))\n",
    "    diag = {\n",
    "        \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"manifest_counts\": {\"CAMELYON16_qc\": int(len(df16)), \"CAMELYON17_qc\": int(len(df17))},\n",
    "        \"features_written_scale0p5\": s05,\n",
    "        \"features_written_scale2p0\": s20,\n",
    "        \"features_2scale_intersection\": common\n",
    "    }\n",
    "    (WORKSPACE / \"diagnostics\").mkdir(exist_ok=True)\n",
    "    (WORKSPACE / \"diagnostics\" / \"script7_camelyon_summary.json\").write_text(json.dumps(diag, indent=2), encoding=\"utf-8\")\n",
    "    (WORKSPACE / \"diagnostics\" / \"script7_camelyon_summary.txt\").write_text(\n",
    "        \"\\n\".join([f\"{k}: {v}\" for k,v in diag.items()]), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\n[OK] Script 7 complete.\")\n",
    "    print(f\" - @0.5µm: {s05} files\")\n",
    "    print(f\" - @2.0µm: {s20} files\")\n",
    "    print(f\" - 2-scale intersection: {common}\")\n",
    "    print(f\" - Summary: {WORKSPACE / 'diagnostics' / 'script7_camelyon_summary.txt'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Script8 — Slide Embeddings Export \n",
    "\n",
    "import os, sys, json, time, math, shutil, gc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import subprocess, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --------------- Workspace ---------------\n",
    "WORKSPACE = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\")\n",
    "FEAT05 = WORKSPACE / \"features\" / \"scale0p5\"\n",
    "FEAT20 = WORKSPACE / \"features\" / \"scale2p0\"\n",
    "EMB_DIR = WORKSPACE / \"embeddings\"\n",
    "MANIFESTS = WORKSPACE / \"manifests\"\n",
    "DIAG = WORKSPACE / \"diagnostics\"\n",
    "for p in [EMB_DIR, DIAG]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------- Deps ---------------\n",
    "def _ensure(pkgs):\n",
    "    miss=[]\n",
    "    for name, spec in pkgs:\n",
    "        try: __import__(name)\n",
    "        except Exception: miss.append(spec)\n",
    "    if miss:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *miss])\n",
    "\n",
    "_ensure([\n",
    "    (\"numpy\",\"numpy>=1.24\"),\n",
    "    (\"pandas\",\"pandas>=2.0\"),\n",
    "])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------- Load manifests (dataset tagging) ---------------\n",
    "def load_slide_sets():\n",
    "    sets = {}\n",
    "    def _load_csv(name):\n",
    "        p = MANIFESTS / f\"manifest_{name}.csv\"\n",
    "        if p.exists():\n",
    "            df = pd.read_csv(p)\n",
    "            # Support both columns we've used: ('slide_id','path') or ('filename', etc.)\n",
    "            sid_col = \"slide_id\" if \"slide_id\" in df.columns else (\"filename\" if \"filename\" in df.columns else None)\n",
    "            if sid_col is None:\n",
    "                return set()\n",
    "            sids = set((df[\"slide_id\"] if \"slide_id\" in df.columns else pd.Series([Path(x).stem for x in df[\"filename\"]])))\n",
    "            return sids\n",
    "        return set()\n",
    "\n",
    "    sets[\"tcga\"] = _load_csv(\"tcga\")  # from Script 2\n",
    "    sets[\"camelyon16\"] = _load_csv(\"camelyon16\")\n",
    "    sets[\"camelyon17\"] = _load_csv(\"camelyon17\")\n",
    "    return sets\n",
    "\n",
    "SLIDESETS = load_slide_sets()\n",
    "\n",
    "# --------------- Discover features present at both scales ---------------\n",
    "def available_two_scale_ids():\n",
    "    s05 = set([p.stem for p in FEAT05.glob(\"*.npy\")])\n",
    "    s20 = set([p.stem for p in FEAT20.glob(\"*.npy\")])\n",
    "    both = sorted(list(s05 & s20))\n",
    "    return both\n",
    "\n",
    "TWO_SCALE_IDS = available_two_scale_ids()\n",
    "\n",
    "# --------------- Decide dataset for each slide_id ---------------\n",
    "def dataset_of(slide_id: str) -> str:\n",
    "    # Priority: camelyon16 / camelyon17 / tcga / other\n",
    "    if slide_id in SLIDESETS.get(\"camelyon16\", set()): return \"CAMELYON16\"\n",
    "    if slide_id in SLIDESETS.get(\"camelyon17\", set()): return \"CAMELYON17\"\n",
    "    if slide_id in SLIDESETS.get(\"tcga\", set()):       return \"TCGA\"\n",
    "    return \"OTHER\"\n",
    "\n",
    "# --------------- Export logic ---------------\n",
    "def embed_one(slide_id: str) -> dict:\n",
    "    f05 = FEAT05 / f\"{slide_id}.npy\"\n",
    "    f20 = FEAT20 / f\"{slide_id}.npy\"\n",
    "    if not (f05.exists() and f20.exists()):\n",
    "        return {\"slide_id\": slide_id, \"ok\": False, \"reason\": \"missing_feature_file\"}\n",
    "\n",
    "    try:\n",
    "        a = np.load(f05, mmap_mode=\"r\")  # [T1,768]\n",
    "        b = np.load(f20, mmap_mode=\"r\")  # [T2,768]\n",
    "        if a.ndim != 2 or b.ndim != 2 or a.shape[1] != 768 or b.shape[1] != 768:\n",
    "            return {\"slide_id\": slide_id, \"ok\": False, \"reason\": f\"bad_shape a{tuple(a.shape)} b{tuple(b.shape)}\"}\n",
    "        v05 = a.mean(axis=0).astype(np.float32)\n",
    "        v20 = b.mean(axis=0).astype(np.float32)\n",
    "        emb = ((v05 + v20) * 0.5).astype(np.float32)  # [768]\n",
    "        ds = dataset_of(slide_id)\n",
    "        out_dir = EMB_DIR / ds\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_path = out_dir / f\"{slide_id}.npy\"\n",
    "        np.save(out_path, emb)\n",
    "\n",
    "        return {\n",
    "            \"slide_id\": slide_id,\n",
    "            \"dataset\": ds,\n",
    "            \"ok\": True,\n",
    "            \"path_emb\": str(out_path),\n",
    "            \"t05\": int(a.shape[0]),\n",
    "            \"t20\": int(b.shape[0]),\n",
    "            \"norm\": float(np.linalg.norm(emb)),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"slide_id\": slide_id, \"ok\": False, \"reason\": f\"{type(e).__name__}:{e}\"}\n",
    "\n",
    "# --------------- Driver ---------------\n",
    "CONFIG = {\n",
    "    \"only_datasets\": [\"CAMELYON16\",\"CAMELYON17\"],  # << do these first; set to None to do ALL (incl. TCGA)\n",
    "    \"workers\": 16,     # Threaded I/O; safe in notebook on Windows\n",
    "    \"print_every\": 200 # progress print interval (slides)\n",
    "}\n",
    "\n",
    "def main():\n",
    "    print(\"== Script 8 — Slide Embeddings Export ==\")\n",
    "    print(json.dumps({\n",
    "        \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"workspace\": str(WORKSPACE),\n",
    "        \"features_0p5\": str(FEAT05),\n",
    "        \"features_2p0\": str(FEAT20),\n",
    "        \"emb_out\": str(EMB_DIR),\n",
    "        \"two_scale_ids\": len(TWO_SCALE_IDS),\n",
    "        \"sets\": {k: len(v) for k,v in SLIDESETS.items()}\n",
    "    }, indent=2))\n",
    "\n",
    "    # Filter IDs by dataset if requested\n",
    "    target_ids = []\n",
    "    for sid in TWO_SCALE_IDS:\n",
    "        ds = dataset_of(sid)\n",
    "        if CONFIG[\"only_datasets\"] is None or ds in CONFIG[\"only_datasets\"]:\n",
    "            target_ids.append(sid)\n",
    "\n",
    "    print(f\"[PLAN] Slides with 2-scale features in target sets: {len(target_ids)}\")\n",
    "    if len(target_ids) == 0:\n",
    "        print(\"[EXIT] Nothing to export for chosen sets.\")\n",
    "        return\n",
    "\n",
    "    # Export with threads\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    t0=time.time(); done=0; ok=0; bad=0\n",
    "    rows=[]\n",
    "    print_every = max(1, CONFIG[\"print_every\"])\n",
    "    with ThreadPoolExecutor(max_workers=CONFIG[\"workers\"]) as ex:\n",
    "        futs = {ex.submit(embed_one, sid): sid for sid in target_ids}\n",
    "        for fut in as_completed(futs):\n",
    "            r = fut.result()\n",
    "            rows.append(r)\n",
    "            done += 1\n",
    "            ok += int(r.get(\"ok\", False))\n",
    "            bad += int(not r.get(\"ok\", False))\n",
    "            if (done % print_every)==0:\n",
    "                dt = time.time() - t0\n",
    "                sps = done / max(1e-6, dt)\n",
    "                print(f\"[{done:6d}/{len(target_ids)}] ok={ok} bad={bad}  {sps:.2f} slides/s\")\n",
    "\n",
    "    # Save per-dataset indices\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"dataset\"] = df[\"dataset\"].fillna(\"UNKNOWN\")\n",
    "    for ds, g in df[df[\"ok\"]==True].groupby(\"dataset\"):\n",
    "        out_csv = EMB_DIR / f\"{ds.lower()}_index.csv\"\n",
    "        g[[\"slide_id\",\"path_emb\",\"t05\",\"t20\",\"norm\"]].to_csv(out_csv, index=False)\n",
    "        print(f\"[OK] Index for {ds}: {len(g)} → {out_csv}\")\n",
    "\n",
    "    # Diagnostics\n",
    "    diag = {\n",
    "        \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"total_attempted\": int(len(target_ids)),\n",
    "        \"ok\": int((df[\"ok\"]==True).sum()) if len(df) else 0,\n",
    "        \"bad\": int((df[\"ok\"]==False).sum()) if len(df) else 0,\n",
    "        \"by_dataset\": df.groupby(\"dataset\")[\"ok\"].sum().to_dict() if len(df) else {},\n",
    "        \"examples_bad\": df[df[\"ok\"]==False].head(10).to_dict(orient=\"records\"),\n",
    "    }\n",
    "    (DIAG / \"script8_embeddings_summary.json\").write_text(json.dumps(diag, indent=2), encoding=\"utf-8\")\n",
    "    (DIAG / \"script8_embeddings_summary.txt\").write_text(\n",
    "        \"\\n\".join([f\"{k}: {v}\" for k,v in diag.items()]), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\n== Summary ==\")\n",
    "    print(json.dumps(diag, indent=2))\n",
    "    print(\"\\n[DONE] Script 8 complete.\")\n",
    "    print(f\"Embeddings dir: {EMB_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Script 9 — CAMELYON17 pN (κ) ablation (LOCO) \n",
    "import os, sys, re, json, math, time, subprocess, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "WS = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\")\n",
    "RAW = WS / r\"Raw Data\" / \"CAMELYON17\"\n",
    "EMB_INDEX = WS / \"embeddings\" / \"camelyon17_index.csv\"\n",
    "MANIFEST  = WS / \"manifests\" / \"manifest_camelyon17.csv\"\n",
    "OUTDIR    = WS / \"results\" / \"cam17_pn_eval\" / \"ablations\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _ensure(pkgs):\n",
    "    miss=[]\n",
    "    for name, spec in pkgs:\n",
    "        try: __import__(name)\n",
    "        except Exception: miss.append(spec)\n",
    "    if miss:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *miss])\n",
    "_ensure([(\"numpy\",\"numpy>=1.24\"),(\"pandas\",\"pandas>=2.0\"),(\"sklearn\",\"scikit-learn>=1.3\")])\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "CFG = {\n",
    "  \"Cs\": [0.1, 0.3, 1.0, 3.0, 10.0],\n",
    "  \"random_state\": 17,\n",
    "  \"max_iter\": 4000,\n",
    "  \"n_jobs\": 4,\n",
    "  \"fallback_k\": 5,\n",
    "  \"boots\": 2000\n",
    "}\n",
    "\n",
    "def _now(): return datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "def guess_label_csv(root: Path) -> Path|None:\n",
    "    cands=[]\n",
    "    for p in root.rglob(\"*.csv\"):\n",
    "        try: hdr = pd.read_csv(p, nrows=5)\n",
    "        except: continue\n",
    "        cols = set(hdr.columns.str.lower())\n",
    "        if {\"patient\",\"pn\"} <= cols: cands.append(p); continue\n",
    "        if {\"case\",\"pn\"} <= cols: cands.append(p); continue\n",
    "        if {\"patient\",\"stage\"} <= cols: cands.append(p); continue\n",
    "    return min(cands, key=lambda x: len(x.name)) if cands else None\n",
    "\n",
    "def _pn_to_int(v):\n",
    "    if pd.isna(v): return None\n",
    "    s=str(v).lower()\n",
    "    m=re.search(r\"pn\\s*([0-3])\", s)\n",
    "    if m: return int(m.group(1))\n",
    "    if s.isdigit() and int(s) in (0,1,2,3): return int(s)\n",
    "    return None\n",
    "\n",
    "def _center_from_any(x):\n",
    "    if pd.isna(x): return None\n",
    "    s=str(x)\n",
    "    m=re.search(r\"center[_\\-]?(\\d+)\", s, flags=re.I)\n",
    "    if m: return int(m.group(1))\n",
    "    if s.isdigit(): \n",
    "        n=int(s); \n",
    "        return n if 0<=n<=9 else None\n",
    "    return None\n",
    "\n",
    "def load_labels():\n",
    "    cand = guess_label_csv(RAW)\n",
    "    if cand is None:\n",
    "        raise FileNotFoundError(\"Place a CAMELYON17 labels CSV (patient, pN[, center]) under Raw Data/CAMELYON17/\")\n",
    "    df = pd.read_csv(cand)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    if \"case\" in df.columns and \"patient\" not in df.columns:\n",
    "        df[\"patient\"] = df[\"case\"]\n",
    "    if \"pn\" not in df.columns and \"stage\" in df.columns:\n",
    "        df[\"pn\"] = df[\"stage\"]\n",
    "    assert \"patient\" in df.columns and \"pn\" in df.columns\n",
    "    df[\"patient\"] = df[\"patient\"].astype(str)\n",
    "    df[\"patient\"] = df[\"patient\"].str.extract(r\"(patient[_\\-]?\\d+)\", expand=False).fillna(df[\"patient\"])\n",
    "    df[\"pn_int\"] = df[\"pn\"].apply(_pn_to_int)\n",
    "    df = df.dropna(subset=[\"pn_int\"]).copy()\n",
    "    # center\n",
    "    if \"center\" not in df.columns and \"centerid\" in df.columns:\n",
    "        df[\"center\"] = df[\"centerid\"]\n",
    "    if \"center\" in df.columns:\n",
    "        df[\"center\"] = df[\"center\"].apply(_center_from_any)\n",
    "    else:\n",
    "        df[\"center\"] = None\n",
    "    # try manifest to fill missing\n",
    "    if MANIFEST.exists() and df[\"center\"].isna().mean() > 0.1:\n",
    "        man = pd.read_csv(MANIFEST)\n",
    "        pcol = None\n",
    "        for c in [\"path\",\"filepath\",\"fullpath\",\"filename\"]:\n",
    "            if c in man.columns.str.lower().tolist():\n",
    "                pcol = man.columns[[cc.lower()==c for cc in man.columns]].tolist()[0]\n",
    "                break\n",
    "        if pcol is not None:\n",
    "            def _pt(s):\n",
    "                s0=Path(str(s)).stem.lower()\n",
    "                m=re.search(r\"(patient[_\\-]?\\d+)\", s0)\n",
    "                return m.group(1) if m else s0.split(\"_node\")[0]\n",
    "            mp = (man.assign(_patient=man[pcol].astype(str).apply(_pt),\n",
    "                             _center=man[pcol].astype(str).apply(_center_from_any))\n",
    "                     .dropna(subset=[\"_patient\",\"_center\"])\n",
    "                     .groupby(\"_patient\")[\"_center\"].agg(lambda s:int(pd.Series(s).mode().iloc[0]))\n",
    "                     .reset_index().rename(columns={\"_patient\":\"patient\",\"_center\":\"center\"}))\n",
    "            df = df.merge(mp, on=\"patient\", how=\"left\", suffixes=(\"\",\"_m\"))\n",
    "            df[\"center\"] = df[\"center\"].fillna(df[\"center_m\"])\n",
    "            df = df.drop(columns=[c for c in df.columns if c.endswith(\"_m\")])\n",
    "    df[\"center\"] = df[\"center\"].apply(lambda v: int(v) if pd.notna(v) else -1)\n",
    "    df[\"pn_int\"] = df[\"pn_int\"].astype(int)\n",
    "    return df[[\"patient\",\"center\",\"pn_int\"]]\n",
    "\n",
    "def load_emb_index():\n",
    "    assert EMB_INDEX.exists(), f\"Missing {EMB_INDEX}\"\n",
    "    df = pd.read_csv(EMB_INDEX)\n",
    "    assert {\"slide_id\",\"path_emb\"} <= set(df.columns)\n",
    "    return df\n",
    "\n",
    "def patient_from_slide(sid: str) -> str:\n",
    "    s = sid.lower()\n",
    "    m = re.search(r\"(patient[_\\-]?\\d+)\", s)\n",
    "    return m.group(1) if m else s.split(\"_node\")[0]\n",
    "\n",
    "def load_embeddings(df_idx):\n",
    "    X=[]; S=[]; P=[]\n",
    "    for sid, p in zip(df_idx[\"slide_id\"], df_idx[\"path_emb\"]):\n",
    "        try:\n",
    "            v = np.load(p).astype(np.float32)\n",
    "            if v.ndim!=1 or v.shape[0]!=768: continue\n",
    "        except: \n",
    "            continue\n",
    "        X.append(v); S.append(str(sid)); P.append(patient_from_slide(str(sid)))\n",
    "    X = np.stack(X, axis=0) if X else np.zeros((0,768), dtype=np.float32)\n",
    "    return X, np.array(S), np.array(P)\n",
    "\n",
    "def aggregate_patient(X, pats):\n",
    "    uniq = pd.unique(pats)\n",
    "    P = []; order=[]\n",
    "    for u in uniq:\n",
    "        idx = np.where(pats==u)[0]\n",
    "        P.append(X[idx].mean(axis=0))\n",
    "        order.append(u)\n",
    "    return np.stack(P,axis=0) if P else np.zeros((0,768),np.float32), np.array(order)\n",
    "\n",
    "def fit_predict(model_key, C, Xtr, ytr, Xte):\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = scaler.fit_transform(Xtr)\n",
    "    Xte = scaler.transform(Xte)\n",
    "    if model_key==\"logreg_l2\":\n",
    "        clf = LogisticRegression(multi_class=\"multinomial\", solver=\"saga\",\n",
    "                                 penalty=\"l2\", C=C, max_iter=CFG[\"max_iter\"],\n",
    "                                 n_jobs=CFG[\"n_jobs\"], class_weight=\"balanced\",\n",
    "                                 random_state=CFG[\"random_state\"])\n",
    "    elif model_key==\"logreg_l1\":\n",
    "        clf = LogisticRegression(multi_class=\"multinomial\", solver=\"saga\",\n",
    "                                 penalty=\"l1\", C=C, max_iter=CFG[\"max_iter\"],\n",
    "                                 n_jobs=CFG[\"n_jobs\"], class_weight=\"balanced\",\n",
    "                                 random_state=CFG[\"random_state\"])\n",
    "    elif model_key==\"ridge\":\n",
    "        # RidgeClassifier uses alpha, roughly alpha≈1/C\n",
    "        clf = RidgeClassifier(alpha=1.0/max(C,1e-6), class_weight=\"balanced\", random_state=CFG[\"random_state\"])\n",
    "    else:\n",
    "        raise ValueError(\"unknown model_key\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    yhat = clf.predict(Xte)\n",
    "    return yhat\n",
    "\n",
    "def qw_kappa(y, yhat): return cohen_kappa_score(y, yhat, weights=\"quadratic\")\n",
    "\n",
    "def bootstrap_ci(y, yhat, groups, B=2000, seed=123):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # group by patient\n",
    "    pts = pd.unique(groups)\n",
    "    if len(pts)==0: return (float(\"nan\"), float(\"nan\"))\n",
    "    mapping = {p: np.where(groups==p)[0] for p in pts}\n",
    "    vals=[]\n",
    "    for _ in range(B):\n",
    "        idx=[]\n",
    "        for _ in range(len(pts)):\n",
    "            pick = rng.choice(pts)\n",
    "            idx.extend(mapping[pick])\n",
    "        idx = np.array(idx, dtype=int)\n",
    "        vals.append(qw_kappa(y[idx], yhat[idx]))\n",
    "    return float(np.nanpercentile(vals,2.5)), float(np.nanpercentile(vals,97.5))\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "print(\"== Script 9B — CAMELYON17 pN LOCO ablation ==\")\n",
    "print(json.dumps({\"time\": _now(), \"workspace\": str(WS)}, indent=2))\n",
    "\n",
    "df_idx = load_emb_index()\n",
    "X_s, slide_ids, pats_s = load_embeddings(df_idx)\n",
    "Xp, patients = aggregate_patient(X_s, pats_s)\n",
    "df_lbl = load_labels()\n",
    "\n",
    "# align\n",
    "pt2row = {p:i for i,p in enumerate(patients)}\n",
    "y = np.full((Xp.shape[0],), -1, dtype=int)\n",
    "c = np.full((Xp.shape[0],), -1, dtype=int)\n",
    "for _, r in df_lbl.iterrows():\n",
    "    i = pt2row.get(r[\"patient\"])\n",
    "    if i is not None:\n",
    "        y[i] = int(r[\"pn_int\"]); c[i] = int(r[\"center\"])\n",
    "keep = (y>=0)\n",
    "Xp = Xp[keep]; y = y[keep]; c = c[keep]; patients = patients[keep]\n",
    "\n",
    "centers = [int(v) for v in pd.unique(c) if v!=-1]\n",
    "use_loco = len(centers) >= 2\n",
    "print(f\"[MODE] {'LOCO' if use_loco else str(CFG['fallback_k'])+'-fold CV'}  centers={sorted(centers) if centers else 'NONE'}\")\n",
    "print(f\"[DATA] patients={len(patients)}  class_counts=\" + str(pd.Series(y).value_counts().sort_index().to_dict()))\n",
    "\n",
    "models = [(\"logreg_l2\",), (\"logreg_l1\",), (\"ridge\",)]\n",
    "grid = []\n",
    "for mk in [m[0] for m in models]:\n",
    "    for C in CFG[\"Cs\"]:\n",
    "        grid.append((mk, float(C)))\n",
    "\n",
    "rows=[]\n",
    "all_preds = {}  # key: (mk,C) -> per-patient predictions (stacked across folds for overall)\n",
    "for mk, C in grid:\n",
    "    preds=[]; truths=[]; groups=[]\n",
    "    per_fold=[]\n",
    "    if use_loco:\n",
    "        for cc in sorted(pd.unique(c)):\n",
    "            if cc==-1: continue\n",
    "            te = np.where(c==cc)[0]\n",
    "            tr = np.where(c!=cc)[0]  # include -1 in training\n",
    "            if len(te)==0 or len(tr)==0: continue\n",
    "            yhat = fit_predict(mk, C, Xp[tr], y[tr], Xp[te])\n",
    "            k = qw_kappa(y[te], yhat)\n",
    "            per_fold.append((\"CEN\"+str(int(cc)), int(len(te)), float(k)))\n",
    "            preds.extend(yhat.tolist()); truths.extend(y[te].tolist()); groups.extend(patients[te].tolist())\n",
    "    else:\n",
    "        skf = StratifiedKFold(n_splits=CFG[\"fallback_k\"], shuffle=True, random_state=CFG[\"random_state\"])\n",
    "        fold=0\n",
    "        for tr, te in skf.split(Xp, y):\n",
    "            fold+=1\n",
    "            yhat = fit_predict(mk, C, Xp[tr], y[tr], Xp[te])\n",
    "            k = qw_kappa(y[te], yhat)\n",
    "            per_fold.append((\"FOLD\"+str(fold), int(len(te)), float(k)))\n",
    "            preds.extend(yhat.tolist()); truths.extend(y[te].tolist()); groups.extend(patients[te].tolist())\n",
    "    preds = np.array(preds, dtype=int); truths = np.array(truths, dtype=int); groups = np.array(groups)\n",
    "    mean_k = float(np.mean([r[2] for r in per_fold])) if per_fold else float(\"nan\")\n",
    "    rows.append({\n",
    "        \"model\": mk, \"C\": C, \"kappa_qw_mean\": mean_k,\n",
    "        \"folds\": len(per_fold),\n",
    "        \"detail\": \"; \".join([f\"{lab}:n={n}|κ={k:.3f}\" for lab,n,k in per_fold])\n",
    "    })\n",
    "    all_preds[(mk,C)] = (truths, preds, groups)\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"kappa_qw_mean\", ascending=False)\n",
    "df.to_csv(OUTDIR / \"ablations_summary.csv\", index=False)\n",
    "\n",
    "best = df.iloc[0].to_dict()\n",
    "bkey = (best[\"model\"], float(best[\"C\"]))\n",
    "y_true, y_pred, pgroup = all_preds[bkey]\n",
    "ci_lo, ci_hi = bootstrap_ci(y_true, y_pred, pgroup, B=CFG[\"boots\"])\n",
    "overall_k = qw_kappa(y_true, y_pred)\n",
    "\n",
    "# save best predictions\n",
    "pd.DataFrame({\"patient\": pgroup, \"y_true\": y_true, \"y_pred\": y_pred}).to_csv(OUTDIR/\"best_patient_predictions.csv\", index=False)\n",
    "# save meta\n",
    "meta = {\n",
    "  \"time\": _now(),\n",
    "  \"mode\": \"LOCO\" if use_loco else f\"{CFG['fallback_k']}-fold-CV\",\n",
    "  \"best_model\": best[\"model\"],\n",
    "  \"best_C\": float(best[\"C\"]),\n",
    "  \"kappa_qw_mean_cv\": float(best[\"kappa_qw_mean\"]) if not math.isnan(best[\"kappa_qw_mean\"]) else None,\n",
    "  \"overall_kappa_qw\": float(overall_k),\n",
    "  \"kappa_ci95\": [float(ci_lo), float(ci_hi)],\n",
    "  \"class_counts\": pd.Series(y).value_counts().sort_index().to_dict()\n",
    "}\n",
    "(Path(OUTDIR/\"best_config.json\")).write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# human summary\n",
    "lines = [\n",
    "  f\"time={meta['time']}\",\n",
    "  f\"mode={meta['mode']}\",\n",
    "  f\"best={meta['best_model']}  C={meta['best_C']}\",\n",
    "  f\"mean_cv_kappa_qw={meta['kappa_qw_mean_cv']:.4f}\" if meta[\"kappa_qw_mean_cv\"] is not None else \"mean_cv_kappa_qw=nan\",\n",
    "  f\"overall_kappa_qw={meta['overall_kappa_qw']:.4f}\",\n",
    "  f\"ci95=[{meta['kappa_ci95'][0]:.4f}, {meta['kappa_ci95'][1]:.4f}]\",\n",
    "  f\"class_counts={meta['class_counts']}\"\n",
    "]\n",
    "(Path(OUTDIR/\"SUMMARY.txt\")).write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n== Ablation complete ==\")\n",
    "print(json.dumps(meta, indent=2))\n",
    "print(f\"[OK] ablations_summary.csv → {OUTDIR/'ablations_summary.csv'}\")\n",
    "print(f\"[OK] best_config.json     → {OUTDIR/'best_config.json'}\")\n",
    "print(f\"[OK] best_patient_predictions.csv → {OUTDIR/'best_patient_predictions.csv'}\")\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CAMELYON16 — Macenko stain-normalized re-extraction + slide-level CV (ResNet50 penultimate, multi-scale, robust)\n",
    "\n",
    "import os, sys, json, math, time, random, subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- Config ----------\n",
    "WORKSPACE = Path(r\"D:/个人文件夹/Sanwal/OpenSlide\")\n",
    "RAW_CAM16 = WORKSPACE / \"Raw Data\" / \"CAMELYON16\"\n",
    "MANIFEST1 = WORKSPACE / \"manifests\" / \"manifest_camelyon16_originals.csv\"\n",
    "MANIFEST2 = WORKSPACE / \"manifests\" / \"manifest_cam16_CLEAN.csv\"\n",
    "OUT_FEAT  = WORKSPACE / \"features\" / \"cam16_norm\"\n",
    "OUT_RES   = WORKSPACE / \"results\"  / \"cam16_slide_norm\"\n",
    "DIAG_DIR  = WORKSPACE / \"diagnostics\"\n",
    "\n",
    "DO_SCALE_20 = True     # 2.0 µm (context)\n",
    "DO_SCALE_05 = True     # 0.5 µm (detail) — recommended\n",
    "TILE_PX     = 256\n",
    "STRIDE_PX   = 256      # increase to 384/512 to speed up\n",
    "BATCH_SIZE  = 128\n",
    "MAX_TILES_PER_SLIDE = 18000\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "\n",
    "# ---------- Deps ----------\n",
    "def _need(mod, pipname=None):\n",
    "    try: __import__(mod)\n",
    "    except Exception: subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pipname or mod])\n",
    "\n",
    "for m in [\"pandas\",\"numpy\",\"openslide\",\"torch\",\"torchvision\",\"scikit-learn\",\"tqdm\"]:\n",
    "    _need(m)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "from openslide import OpenSlideError\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score\n",
    "\n",
    "# ---------- IO ----------\n",
    "OUT_FEAT.mkdir(parents=True, exist_ok=True)\n",
    "OUT_RES.mkdir(parents=True, exist_ok=True)\n",
    "DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Helpers: robust path resolution ----------\n",
    "_ALLOWED_EXT = (\".svs\", \".tif\", \".tiff\", \".ndpi\", \".mrxs\", \".bif\")\n",
    "\n",
    "def slide_key(name: str) -> str:\n",
    "    s = str(Path(name).stem).lower()\n",
    "    if s.startswith(\"tumor_\") or s.startswith(\"normal_\"): return s\n",
    "    if \"tumor\" in s:  return \"tumor_\" + \"\".join([c for c in s if c.isdigit()])[:3].zfill(3)\n",
    "    if \"normal\" in s: return \"normal_\" + \"\".join([c for c in s if c.isdigit()])[:3].zfill(3)\n",
    "    return s\n",
    "\n",
    "def _is_mask_or_meta(p: Path) -> bool:\n",
    "    n = p.name.lower()\n",
    "    return (\"mask\" in n) or n.endswith(\".xml\") or n.endswith(\".json\")\n",
    "\n",
    "def _can_open_with_openslide(p: Path) -> bool:\n",
    "    try:\n",
    "        s = openslide.OpenSlide(str(p))\n",
    "        s.close()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def resolve_wsi_path(root: Path, sid: str) -> Path | None:\n",
    "    \"\"\"\n",
    "    Prefer true WSI over masks/meta. Validate by opening with OpenSlide.\n",
    "    Search order: exact filename matches by allowed exts, then rglob.\n",
    "    \"\"\"\n",
    "    sid_l = sid.lower()\n",
    "    # 1) direct candidates (fast)\n",
    "    for ext in _ALLOWED_EXT:\n",
    "        p = root / f\"{sid}{ext}\"\n",
    "        if p.exists() and not _is_mask_or_meta(p) and _can_open_with_openslide(p):\n",
    "            return p\n",
    "    # 2) recursive search\n",
    "    cands = []\n",
    "    for p in root.rglob(f\"{sid}*\"):\n",
    "        if not p.is_file(): continue\n",
    "        if _is_mask_or_meta(p): continue\n",
    "        if p.suffix.lower() not in _ALLOWED_EXT: continue\n",
    "        cands.append(p)\n",
    "    # try to validate in a stable order (by suffix preference, then name length)\n",
    "    pref = {\".svs\":0, \".tif\":1, \".tiff\":2, \".ndpi\":3, \".mrxs\":4, \".bif\":5}\n",
    "    for p in sorted(cands, key=lambda q: (pref.get(q.suffix.lower(), 9), len(q.name))):\n",
    "        if _can_open_with_openslide(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def load_manifest() -> pd.DataFrame:\n",
    "    if MANIFEST1.exists(): mf = MANIFEST1\n",
    "    elif MANIFEST2.exists(): mf = MANIFEST2\n",
    "    else: raise FileNotFoundError(\"No CAM16 manifest found.\")\n",
    "    df = pd.read_csv(mf)\n",
    "    if \"slide_id\" not in df.columns:\n",
    "        df[\"slide_id\"] = df.get(\"sid\", df.get(\"name\", df.get(\"wsi\", df.index.astype(str)))).astype(str)\n",
    "    # normalize id format\n",
    "    df[\"slide_id\"] = df[\"slide_id\"].map(slide_key)\n",
    "    if \"kind\" not in df.columns:\n",
    "        df[\"kind\"] = df[\"slide_id\"].map(lambda s: \"tumor\" if s.startswith(\"tumor_\") else (\"normal\" if s.startswith(\"normal_\") else \"unknown\"))\n",
    "    # enforce CAM16 ids and resolve robust paths\n",
    "    df = df[df[\"slide_id\"].str.match(r\"^(tumor|normal)_\\d+$\", na=False)].reset_index(drop=True)\n",
    "    paths = []\n",
    "    for sid in df[\"slide_id\"]:\n",
    "        p = resolve_wsi_path(RAW_CAM16, sid)\n",
    "        paths.append(str(p) if p is not None else None)\n",
    "    df[\"path\"] = paths\n",
    "    df = df.dropna(subset=[\"path\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# ---------- MPP / level utils ----------\n",
    "def get_base_mpp(slide: openslide.OpenSlide) -> float:\n",
    "    props = slide.properties\n",
    "    for k in (\"aperio.MPP\",\"openslide.mpp-x\",\"openslide.mpp-y\"):\n",
    "        if k in props:\n",
    "            try:\n",
    "                v = float(props[k])\n",
    "                if v > 0: return v\n",
    "            except: pass\n",
    "    return 0.243  # typical CAM16\n",
    "\n",
    "def best_level_for_um(slide, target_um):\n",
    "    base = get_base_mpp(slide)  # µm/px at level 0\n",
    "    desired_ds = max(1.0, target_um / max(1e-6, base))\n",
    "    lvl = slide.get_best_level_for_downsample(desired_ds)\n",
    "    lvl = int(max(0, min(lvl, slide.level_count - 1)))\n",
    "    ds_eff = float(slide.level_downsamples[lvl])\n",
    "    return lvl, float(base * ds_eff), ds_eff\n",
    "\n",
    "def tissue_mask_fast(img_rgb: np.ndarray) -> np.ndarray:\n",
    "    I = img_rgb.astype(np.float32)\n",
    "    v = I.mean(axis=2)\n",
    "    sat = (I.max(axis=2) - I.min(axis=2))\n",
    "    return (v < 235) | (sat > 10)\n",
    "\n",
    "# ---------- Macenko ----------\n",
    "def rgb_to_od(I: np.ndarray) -> np.ndarray:\n",
    "    I = I.astype(np.float32) + 1.0\n",
    "    return -np.log(I / 255.0)\n",
    "\n",
    "def od_to_rgb(OD: np.ndarray) -> np.ndarray:\n",
    "    return (np.exp(-OD) * 255.0).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "def _norm_cols(A: np.ndarray) -> np.ndarray:\n",
    "    return A / (np.linalg.norm(A, axis=0, keepdims=True) + 1e-8)\n",
    "\n",
    "def macenko_estimate(I_rgb: np.ndarray, alpha: float = 0.1):\n",
    "    OD = rgb_to_od(I_rgb).reshape(-1, 3)\n",
    "    tissue = (OD > alpha).any(axis=1)\n",
    "    OD_t = OD[tissue]\n",
    "    if OD_t.shape[0] < 500:\n",
    "        return None, None\n",
    "    U, S, Vt = np.linalg.svd(OD_t, full_matrices=False)\n",
    "    v = Vt[:2, :].T\n",
    "    proj = OD_t @ v\n",
    "    phi = np.arctan2(proj[:, 1], proj[:, 0])\n",
    "    vmin = np.percentile(phi, 1)\n",
    "    vmax = np.percentile(phi, 99)\n",
    "    vH = (np.array([np.cos(vmin), np.sin(vmin)]) @ v.T)\n",
    "    vE = (np.array([np.cos(vmax), np.sin(vmax)]) @ v.T)\n",
    "    HE = _norm_cols(np.stack([vH, vE], axis=1))       # (3x2)\n",
    "    C_sub = np.linalg.lstsq(HE, OD_t.T, rcond=None)[0]  # (2xN_tissue)\n",
    "    C_sub = np.clip(C_sub, 0, np.percentile(C_sub, 99, axis=1, keepdims=True))\n",
    "    return HE, C_sub\n",
    "\n",
    "def estimate_reference_from_slides(manifest_df: pd.DataFrame, max_slides: int = 5):\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    tumor_df = manifest_df[manifest_df[\"kind\"] == \"tumor\"]\n",
    "    if len(tumor_df) == 0:\n",
    "        HE_ref = _norm_cols(np.array([[0.65, 0.07, 0.27],\n",
    "                                      [0.07, 0.99, 0.11]]).T)\n",
    "        C99_ref = np.array([1.0, 1.0], dtype=np.float32)\n",
    "        return HE_ref, C99_ref\n",
    "    cand = tumor_df.sample(n=min(max_slides, len(tumor_df)), random_state=SEED)\n",
    "    HEs, C99s = [], []\n",
    "    for _, r in cand.iterrows():\n",
    "        try:\n",
    "            s = openslide.OpenSlide(r[\"path\"])\n",
    "            lvl, _, ds = best_level_for_um(s, 2.0)\n",
    "            w, h = s.level_dimensions[lvl]\n",
    "            for _ in range(3):\n",
    "                if w < 512 or h < 512: break\n",
    "                x = int(rng.integers(0, max(1, w - 512)))\n",
    "                y = int(rng.integers(0, max(1, h - 512)))\n",
    "                img = np.asarray(s.read_region((int(round(x*ds)), int(round(y*ds))), lvl, (512, 512)).convert(\"RGB\"))\n",
    "                if tissue_mask_fast(img).mean() < 0.05: continue\n",
    "                HE, C_sub = macenko_estimate(img)\n",
    "                if HE is None: continue\n",
    "                HEs.append(HE); C99s.append(np.percentile(C_sub, 99, axis=1))\n",
    "        except Exception:\n",
    "            pass\n",
    "        finally:\n",
    "            try: s.close()\n",
    "            except: pass\n",
    "    if not HEs:\n",
    "        HE_ref = _norm_cols(np.array([[0.65, 0.07, 0.27],\n",
    "                                      [0.07, 0.99, 0.11]]).T)\n",
    "        C99_ref = np.array([1.0, 1.0], dtype=np.float32)\n",
    "    else:\n",
    "        HE_stack = np.stack(HEs, axis=2)\n",
    "        HE_ref = _norm_cols(np.mean(HE_stack, axis=2))\n",
    "        C99_ref = np.median(np.stack(C99s, axis=1), axis=1).astype(np.float32)\n",
    "    return HE_ref, C99_ref\n",
    "\n",
    "def macenko_apply_tile(tile_rgb: np.ndarray, HE_ref: np.ndarray, C99_ref: np.ndarray, alpha: float = 0.1) -> np.ndarray:\n",
    "    HE_src, C_sub = macenko_estimate(tile_rgb, alpha=alpha)\n",
    "    if HE_src is None:\n",
    "        return tile_rgb\n",
    "    OD_full = rgb_to_od(tile_rgb).reshape(-1, 3)              # (N,3)\n",
    "    C_full = np.linalg.lstsq(HE_src, OD_full.T, rcond=None)[0]# (2,N)\n",
    "    c99_src = np.percentile(C_sub, 99, axis=1)                # (2,)\n",
    "    scale = C99_ref / (c99_src + 1e-8)                        # (2,)\n",
    "    C_full_scaled = (C_full.T * scale.reshape(1, 2)).T\n",
    "    OD_norm = (HE_ref @ C_full_scaled).T                      # (N,3)\n",
    "    return od_to_rgb(OD_norm).reshape(tile_rgb.shape)\n",
    "\n",
    "# ---------- Embedding ----------\n",
    "def load_backbone():\n",
    "    import torchvision.models as tvm\n",
    "    import torch.nn as nn\n",
    "    base = tvm.resnet50(weights=tvm.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    backbone = nn.Sequential(*list(base.children())[:-1])  # [B,2048,1,1]\n",
    "    backbone.eval()\n",
    "    return backbone\n",
    "\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n",
    "\n",
    "def to_tensor_bchw(img_uint8: np.ndarray) -> torch.Tensor:\n",
    "    t = torch.from_numpy(img_uint8).permute(2,0,1).unsqueeze(0).float()/255.0\n",
    "    return (t - IMAGENET_MEAN.to(t.device)) / IMAGENET_STD.to(t.device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_tiles(tiles_rgb, model, device=\"cuda\"):\n",
    "    if not tiles_rgb: return np.zeros((0,2048), dtype=np.float32)\n",
    "    feats = []\n",
    "    use_amp = (device == \"cuda\")\n",
    "    for i in range(0, len(tiles_rgb), BATCH_SIZE):\n",
    "        x = torch.cat([to_tensor_bchw(im) for im in tiles_rgb[i:i+BATCH_SIZE]], dim=0).to(device, non_blocking=True)\n",
    "        with torch.amp.autocast(device_type=(\"cuda\" if use_amp else \"cpu\"), dtype=torch.float16, enabled=use_amp):\n",
    "            z = model(x)               # [B,2048,1,1]\n",
    "        z = z.view(z.size(0), -1).detach().float().cpu().numpy()\n",
    "        feats.append(z)\n",
    "    return np.concatenate(feats, axis=0)\n",
    "\n",
    "# ---------- Reference cache ----------\n",
    "REF_JSON = OUT_FEAT / \"macenko_reference.json\"\n",
    "def get_or_make_reference(df: pd.DataFrame):\n",
    "    if REF_JSON.exists():\n",
    "        js = json.loads(REF_JSON.read_text(encoding=\"utf-8\"))\n",
    "        return np.array(js[\"HE_ref\"], dtype=np.float32), np.array(js[\"C99_ref\"], dtype=np.float32)\n",
    "    HE_ref, C99_ref = estimate_reference_from_slides(df)\n",
    "    REF_JSON.write_text(json.dumps({\"HE_ref\": HE_ref.tolist(),\n",
    "                                    \"C99_ref\": C99_ref.tolist(),\n",
    "                                    \"time\": datetime.now().isoformat(timespec=\"seconds\")},\n",
    "                                   indent=2), encoding=\"utf-8\")\n",
    "    return HE_ref, C99_ref\n",
    "\n",
    "# ---------- Tiling / extraction ----------\n",
    "def tile_coords_iter(w_lvl: int, h_lvl: int, ds_from0: float, tile_px=TILE_PX, stride_px=STRIDE_PX, limit=None):\n",
    "    n = 0\n",
    "    for y in range(0, h_lvl, stride_px):\n",
    "        if y + tile_px > h_lvl: break\n",
    "        for x in range(0, w_lvl, stride_px):\n",
    "            if x + tile_px > w_lvl: break\n",
    "            x0 = int(round(x * ds_from0))\n",
    "            y0 = int(round(y * ds_from0))\n",
    "            yield x, y, x0, y0\n",
    "            n += 1\n",
    "            if limit and n >= limit: return\n",
    "\n",
    "def extract_one(slide_path: str, out_dir: Path, target_um: float, model, HE_ref, C99_ref, device=\"cuda\"):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    sid = slide_key(slide_path)\n",
    "    feat_path = out_dir / f\"{sid}.npy\"\n",
    "    meta_path = out_dir / f\"{sid}_meta.csv\"\n",
    "    if feat_path.exists() and meta_path.exists():\n",
    "        return \"exist\", 0, 0.0\n",
    "\n",
    "    tiles = []; mmxy = []\n",
    "    s = None\n",
    "    try:\n",
    "        try:\n",
    "            s = openslide.OpenSlide(str(slide_path))\n",
    "        except Exception as e:\n",
    "            # Unsupported/corrupt file → skip gracefully\n",
    "            return \"bad_format\", 0, 0.0\n",
    "\n",
    "        lvl, mpp_lvl, ds = best_level_for_um(s, target_um)\n",
    "        w_lvl, h_lvl = s.level_dimensions[lvl]\n",
    "        kept = 0; base_mpp = get_base_mpp(s)\n",
    "\n",
    "        for (xl, yl, x0, y0) in tile_coords_iter(w_lvl, h_lvl, ds, TILE_PX, STRIDE_PX, limit=MAX_TILES_PER_SLIDE*2):\n",
    "            try:\n",
    "                im = np.asarray(s.read_region((x0, y0), lvl, (TILE_PX, TILE_PX)).convert(\"RGB\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "            if tissue_mask_fast(im).mean() < 0.15:\n",
    "                continue\n",
    "            imn = macenko_apply_tile(im, HE_ref, C99_ref)\n",
    "            tiles.append(imn)\n",
    "            cx0 = x0 + TILE_PX * ds / 2.0\n",
    "            cy0 = y0 + TILE_PX * ds / 2.0\n",
    "            mmx = float(cx0 * base_mpp / 1000.0)\n",
    "            mmy = float(cy0 * base_mpp / 1000.0)\n",
    "            mmxy.append((mmx, mmy))\n",
    "            kept += 1\n",
    "            if kept >= MAX_TILES_PER_SLIDE: break\n",
    "\n",
    "        if not tiles:\n",
    "            np.save(feat_path, np.zeros((0, 2048), dtype=np.float32))\n",
    "            pd.DataFrame({\"mm_x\": [], \"mm_y\": [], \"scale_um_per_px\": []}).to_csv(meta_path, index=False)\n",
    "            return \"empty\", 0, float(mpp_lvl)\n",
    "\n",
    "        feats = embed_tiles(tiles, model, device=device)  # [T,2048]\n",
    "        np.save(feat_path, feats.astype(np.float32))\n",
    "        meta = pd.DataFrame(mmxy, columns=[\"mm_x\", \"mm_y\"])\n",
    "        meta[\"scale_um_per_px\"] = mpp_lvl\n",
    "        meta.to_csv(meta_path, index=False)\n",
    "        return \"ok\", feats.shape[0], float(mpp_lvl)\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            if s is not None: s.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# ---------- Slide-level CV ----------\n",
    "def build_slide_vectors(df: pd.DataFrame, root_feat: Path):\n",
    "    X, y, sids = [], [], []\n",
    "    for _, r in df.iterrows():\n",
    "        sid = r[\"slide_id\"]; sids.append(sid)\n",
    "        y.append(1 if r[\"kind\"] == \"tumor\" else 0)\n",
    "        parts = []\n",
    "        f2 = root_feat / \"scale2p0\" / f\"{sid}.npy\"\n",
    "        if f2.exists():\n",
    "            a = np.load(f2)\n",
    "            if a.size > 0:\n",
    "                norms = np.linalg.norm(a, axis=1)\n",
    "                K = min(64, a.shape[0])\n",
    "                topk = a[np.argpartition(-norms, K-1)[:K]].mean(axis=0)\n",
    "                parts.append(a.mean(axis=0)); parts.append(topk)\n",
    "        f5 = root_feat / \"scale0p5\" / f\"{sid}.npy\"\n",
    "        if DO_SCALE_05 and f5.exists():\n",
    "            b = np.load(f5)\n",
    "            if b.size > 0:\n",
    "                norms = np.linalg.norm(b, axis=1)\n",
    "                K = min(64, b.shape[0])\n",
    "                topk = b[np.argpartition(-norms, K-1)[:K]].mean(axis=0)\n",
    "                parts.append(b.mean(axis=0)); parts.append(topk)\n",
    "        if not parts:\n",
    "            parts = [np.zeros((2048,), dtype=np.float32)]\n",
    "        X.append(np.concatenate(parts, axis=0).astype(np.float32))\n",
    "    X = np.vstack(X)\n",
    "    return X, np.array(y, dtype=np.int64), np.array(sids, dtype=object)\n",
    "\n",
    "def run_slide_cv(df: pd.DataFrame, root_feat: Path, folds: int = 5):\n",
    "    X, y, sids = build_slide_vectors(df, root_feat)\n",
    "    ncomp = min(256, X.shape[1], max(1, X.shape[0] - 1))\n",
    "    pipe = Pipeline([\n",
    "        (\"sc\",  StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"pca\", PCA(n_components=ncomp, svd_solver=\"full\", random_state=SEED)),\n",
    "        (\"clf\", LogisticRegression(C=0.3, class_weight=\"balanced\", max_iter=5000, solver=\"lbfgs\"))\n",
    "    ])\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)\n",
    "    oof = np.zeros_like(y, dtype=np.float32)\n",
    "    rows = []\n",
    "    for i, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "        Xt, Xv, yt, yv = X[tr], X[va], y[tr], y[va]\n",
    "        pipe.fit(Xt, yt)\n",
    "        pv = pipe.predict_proba(Xv)[:, 1]\n",
    "        oof[va] = pv\n",
    "        auc = roc_auc_score(yv, pv)\n",
    "        ap  = average_precision_score(yv, pv)\n",
    "        pred = (pv >= 0.5).astype(int)\n",
    "        rows.append({\"fold\": i, \"AUC\": float(auc), \"AP\": float(ap),\n",
    "                     \"ACC\": float(accuracy_score(yv, pred)),\n",
    "                     \"F1\": float(f1_score(yv, pred, zero_division=0))})\n",
    "        print(f\"[FOLD {i}] AUC={auc:.4f} AP={ap:.4f} ACC={rows[-1]['ACC']:.3f} F1={rows[-1]['F1']:.3f}\")\n",
    "\n",
    "    auc_oof = roc_auc_score(y, oof)\n",
    "    ap_oof  = average_precision_score(y, oof)\n",
    "    acc_oof = accuracy_score(y, (oof >= 0.5).astype(int))\n",
    "    f1_oof  = f1_score(y, (oof >= 0.5).astype(int), zero_division=0)\n",
    "\n",
    "    summ = {\n",
    "        \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"slides\": int(len(y)),\n",
    "        \"pos\": int(y.sum()),\n",
    "        \"neg\": int((1 - y).sum()),\n",
    "        \"dim_in\": int(X.shape[1]),\n",
    "        \"auc_roc_oof\": float(auc_oof),\n",
    "        \"auc_pr_oof\": float(ap_oof),\n",
    "        \"acc_oof\": float(acc_oof),\n",
    "        \"f1_oof\": float(f1_oof),\n",
    "        \"folds\": int(folds),\n",
    "        \"scales_used\": \"2.0\" + (\"+0.5\" if DO_SCALE_05 else \"\")\n",
    "    }\n",
    "    (OUT_RES / \"slide_cv_summary.json\").write_text(json.dumps(summ, indent=2), encoding=\"utf-8\")\n",
    "    print(\"\\n== Slide-level CV — Macenko normalized ==\")\n",
    "    print(json.dumps(summ, indent=2))\n",
    "    return summ\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main():\n",
    "    print(\"== CAMELYON16 — Stain-normalized re-extraction + slide-level CV ==\")\n",
    "    print(json.dumps({\"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "                      \"workspace\": str(WORKSPACE)}, indent=2))\n",
    "\n",
    "    df = load_manifest()\n",
    "    df = df[df[\"kind\"].isin([\"tumor\",\"normal\"])].reset_index(drop=True)\n",
    "    print(f\"[DATA] slides={len(df)}  tumor={(df['kind']=='tumor').sum()}  normal={(df['kind']=='normal').sum()}\")\n",
    "\n",
    "    HE_ref, C99_ref = get_or_make_reference(df)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model  = load_backbone().to(device).eval()\n",
    "\n",
    "    scales = []\n",
    "    if DO_SCALE_20: scales.append(2.0)\n",
    "    if DO_SCALE_05: scales.append(0.5)\n",
    "\n",
    "    t0 = time.time()\n",
    "    for scale_um in scales:\n",
    "        out_dir = OUT_FEAT / (\"scale2p0\" if abs(scale_um - 2.0) < 1e-6 else \"scale0p5\")\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        ok = exist = fail = bad = tiles_total = 0\n",
    "        for _, r in tqdm(df.iterrows(), total=len(df), desc=f\"EXTRACT {scale_um}µm\"):\n",
    "            sid = r[\"slide_id\"]\n",
    "            feat_path = out_dir / f\"{sid}.npy\"\n",
    "            meta_path = out_dir / f\"{sid}_meta.csv\"\n",
    "            if feat_path.exists() and meta_path.exists():\n",
    "                exist += 1\n",
    "                continue\n",
    "            st, ntiles, mpp = extract_one(r[\"path\"], out_dir, scale_um, model, HE_ref, C99_ref, device=device)\n",
    "            if st == \"ok\":\n",
    "                ok += 1; tiles_total += ntiles\n",
    "            elif st == \"exist\":\n",
    "                exist += 1\n",
    "            elif st == \"bad_format\":\n",
    "                bad += 1\n",
    "            else:\n",
    "                fail += 1\n",
    "        print(f\"[SCALE {scale_um}µm] ok={ok} exist={exist} bad_format={bad} fail={fail} tiles_total≈{tiles_total}\")\n",
    "    print(f\"[TIME] extraction wall={(time.time()-t0)/60.0:.1f} min\")\n",
    "\n",
    "    summary = run_slide_cv(df, OUT_FEAT, folds=5)\n",
    "    (DIAG_DIR / \"cam16_norm_reextract_summary.json\").write_text(json.dumps({\n",
    "        \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"slides\": int(len(df)),\n",
    "        \"scales\": scales,\n",
    "        \"cv\": summary\n",
    "    }, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CAMELYON16 — Enhanced slide-level ensemble with fold-specific optimization\n",
    "import os, re, json, time, math, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import (HistGradientBoostingClassifier, RandomForestClassifier, \n",
    "                             ExtraTreesClassifier, VotingClassifier)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "WORKSPACE = Path(r\"D:/个人文件夹/Sanwal/OpenSlide\")\n",
    "FEATURES_ROOT = WORKSPACE / \"features\"\n",
    "MANIFEST_OUT  = WORKSPACE / \"manifests\" / \"manifest_cam16_AUTO.csv\"\n",
    "OUTDIR        = WORKSPACE / \"results\" / \"cam16_slide_ensemble_ENHANCED\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 1337\n",
    "N_FOLDS = 5\n",
    "TOPK_FRAC = 0.20\n",
    "MAX_TOPK  = 2000\n",
    "\n",
    "# Fold-specific configurations to maintain/improve performance\n",
    "FOLD_CONFIGS = {\n",
    "    1: {\"preserve\": True,  \"pca_cap\": 214, \"feature_aug\": False},  # Keep at 0.79\n",
    "    2: {\"preserve\": False, \"pca_cap\": 256, \"feature_aug\": True},   # Needs improvement\n",
    "    3: {\"preserve\": True,  \"pca_cap\": 214, \"feature_aug\": False},  # Keep at 0.81\n",
    "    4: {\"preserve\": False, \"pca_cap\": 256, \"feature_aug\": True},   # Needs improvement\n",
    "    5: {\"preserve\": False, \"pca_cap\": 256, \"feature_aug\": True},   # Needs improvement\n",
    "}\n",
    "\n",
    "# ----------------- UTILS -----------------\n",
    "def slide_key(s: str) -> str:\n",
    "    s = str(s).lower()\n",
    "    m = re.search(r'(tumor|normal)_(\\d+)', s)\n",
    "    return f\"{m.group(1)}_{int(m.group(2)):03d}\" if m else Path(s).stem.lower()\n",
    "\n",
    "def guess_scale_from_path(p: Path) -> str | None:\n",
    "    s = str(p).lower()\n",
    "    if re.search(r'scale[^/\\\\]*2p?0|[^a-z]2\\.0[^a-z]|[_\\-]2p0[_\\-]', s): return \"2p0\"\n",
    "    if re.search(r'scale[^/\\\\]*0p?5|[^a-z]0\\.5[^a-z]|[_\\-]0p5[_\\-]', s): return \"0p5\"\n",
    "    if \"2p0\" in s or \"x20\" in s or \"20x\" in s: return \"2p0\"\n",
    "    if \"0p5\" in s or \"x5\" in s or \"5x\" in s:   return \"0p5\"\n",
    "    return None\n",
    "\n",
    "def discover_cam16_feature_files(root: Path):\n",
    "    npy_paths = list(root.rglob(\"*.npy\"))\n",
    "    rows = []\n",
    "    for p in npy_paths:\n",
    "        sid = slide_key(p.stem)\n",
    "        if not re.match(r'^(tumor|normal)_\\d{1,3}$', sid):\n",
    "            continue\n",
    "        scale = guess_scale_from_path(p)\n",
    "        if scale is None:\n",
    "            continue\n",
    "        rows.append({\"slide_id\": sid, \"scale\": scale, \"path\": str(p)})\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    def rank_path(x):\n",
    "        s = x.lower()\n",
    "        score = 0\n",
    "        if \"cam16\" in s or \"camelyon16\" in s: score += 2\n",
    "        if \"scale2p0\" in s or \"scale0p5\" in s: score += 1\n",
    "        return score\n",
    "    df[\"rank\"] = df[\"path\"].map(rank_path)\n",
    "    df = df.sort_values([\"slide_id\",\"scale\",\"rank\"], ascending=[True, True, False]).drop_duplicates([\"slide_id\",\"scale\"])\n",
    "    return df.drop(columns=[\"rank\"]).reset_index(drop=True)\n",
    "\n",
    "def enhanced_pooled_vector(TxD: np.ndarray, augment: bool = False) -> np.ndarray:\n",
    "    \"\"\"Enhanced pooling with optional statistical augmentation.\"\"\"\n",
    "    T, D = TxD.shape\n",
    "    \n",
    "    # Basic stats\n",
    "    g_mean = TxD.mean(axis=0)\n",
    "    g_std = TxD.std(axis=0, ddof=0)\n",
    "    g_max = TxD.max(axis=0)\n",
    "    g_min = TxD.min(axis=0)\n",
    "    \n",
    "    # Top-k based on L2 norm\n",
    "    norms = np.linalg.norm(TxD, axis=1)\n",
    "    k = int(max(1, min(MAX_TOPK, math.ceil(TOPK_FRAC * T))))\n",
    "    idx = np.argpartition(norms, -k)[-k:]\n",
    "    top = TxD[idx]\n",
    "    t_mean = top.mean(axis=0)\n",
    "    t_std = top.std(axis=0, ddof=0)\n",
    "    \n",
    "    base_features = [g_mean, g_std, g_max, t_mean, t_std]\n",
    "    \n",
    "    if augment:\n",
    "        # Additional statistics for challenging folds\n",
    "        g_median = np.median(TxD, axis=0)\n",
    "        g_q25 = np.percentile(TxD, 25, axis=0)\n",
    "        g_q75 = np.percentile(TxD, 75, axis=0)\n",
    "        \n",
    "        # Bottom-k features (complementary to top-k)\n",
    "        k_bottom = max(1, k // 2)\n",
    "        idx_bottom = np.argpartition(norms, k_bottom)[:k_bottom]\n",
    "        bottom = TxD[idx_bottom]\n",
    "        b_mean = bottom.mean(axis=0)\n",
    "        \n",
    "        base_features.extend([g_min, g_median, g_q25, g_q75, b_mean])\n",
    "    \n",
    "    return np.concatenate(base_features, axis=0).astype(np.float32)\n",
    "\n",
    "def safe_load_tokens(npy_path: str) -> np.ndarray | None:\n",
    "    try:\n",
    "        arr = np.load(npy_path)\n",
    "        if isinstance(arr, np.ndarray) and arr.ndim==2 and arr.shape[0]>0:\n",
    "            return arr.astype(np.float32, copy=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def pca_components_for(Xt: np.ndarray, cap: int = 256) -> int:\n",
    "    return max(1, min(Xt.shape[0]-1, Xt.shape[1], cap))\n",
    "\n",
    "# Model builders for preserved folds (1 & 3)\n",
    "def make_preserved_logreg_pipe(nc: int):\n",
    "    \"\"\"Original LogReg configuration for good-performing folds.\"\"\"\n",
    "    return Pipeline([\n",
    "        (\"sc\",  StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"pca\", PCA(n_components=nc, svd_solver=\"randomized\", whiten=True, random_state=SEED)),\n",
    "        (\"clf\", LogisticRegression(C=0.8, class_weight=\"balanced\", solver=\"lbfgs\",\n",
    "                                   max_iter=5000, n_jobs=min(8, os.cpu_count() or 2)))\n",
    "    ])\n",
    "\n",
    "def make_preserved_svm_pipe(nc: int):\n",
    "    \"\"\"Original SVM configuration for good-performing folds.\"\"\"\n",
    "    base = LinearSVC(C=0.5, class_weight=\"balanced\", max_iter=10000, random_state=SEED)\n",
    "    try:\n",
    "        cal = CalibratedClassifierCV(estimator=base, method=\"sigmoid\", cv=3)\n",
    "    except TypeError:\n",
    "        cal = CalibratedClassifierCV(base_estimator=base, method=\"sigmoid\", cv=3)\n",
    "    return Pipeline([\n",
    "        (\"sc\",  StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"pca\", PCA(n_components=nc, svd_solver=\"randomized\", whiten=True, random_state=SEED)),\n",
    "        (\"clf\", cal)\n",
    "    ])\n",
    "\n",
    "def make_preserved_hgb():\n",
    "    \"\"\"Original HGB configuration for good-performing folds.\"\"\"\n",
    "    return HistGradientBoostingClassifier(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.08,\n",
    "        max_iter=800,\n",
    "        min_samples_leaf=5,\n",
    "        l2_regularization=1e-3,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "# Enhanced model builders for challenging folds (2, 4, 5)\n",
    "def make_enhanced_logreg_pipe(nc: int):\n",
    "    \"\"\"Enhanced LogReg with better regularization.\"\"\"\n",
    "    return Pipeline([\n",
    "        (\"sc\",  RobustScaler()),  # More robust to outliers\n",
    "        (\"pca\", PCA(n_components=nc, svd_solver=\"randomized\", whiten=True, random_state=SEED)),\n",
    "        (\"clf\", LogisticRegression(C=1.0, penalty='l2', class_weight=\"balanced\", \n",
    "                                   solver=\"saga\", max_iter=10000, \n",
    "                                   n_jobs=min(8, os.cpu_count() or 2)))\n",
    "    ])\n",
    "\n",
    "def make_enhanced_svm_pipe(nc: int):\n",
    "    \"\"\"Enhanced SVM with RBF kernel for non-linear patterns.\"\"\"\n",
    "    base = SVC(C=1.0, kernel='rbf', gamma='scale', class_weight=\"balanced\", \n",
    "               probability=False, random_state=SEED)\n",
    "    try:\n",
    "        cal = CalibratedClassifierCV(estimator=base, method=\"sigmoid\", cv=3)\n",
    "    except TypeError:\n",
    "        cal = CalibratedClassifierCV(base_estimator=base, method=\"sigmoid\", cv=3)\n",
    "    return Pipeline([\n",
    "        (\"sc\",  StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=nc, svd_solver=\"randomized\", whiten=True, random_state=SEED)),\n",
    "        (\"clf\", cal)\n",
    "    ])\n",
    "\n",
    "def make_enhanced_hgb():\n",
    "    \"\"\"Enhanced HGB with more depth and iterations.\"\"\"\n",
    "    return HistGradientBoostingClassifier(\n",
    "        max_depth=8,              # Increased depth\n",
    "        learning_rate=0.05,       # Lower learning rate\n",
    "        max_iter=1500,            # More iterations\n",
    "        min_samples_leaf=3,       # Less restrictive\n",
    "        l2_regularization=0.5e-3, # Less regularization\n",
    "        class_weight=\"balanced\",\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=50,\n",
    "        validation_fraction=0.15,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "def make_rf_classifier():\n",
    "    \"\"\"Random Forest for ensemble diversity.\"\"\"\n",
    "    return RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=12,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=min(8, os.cpu_count() or 2),\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "def make_et_classifier():\n",
    "    \"\"\"Extra Trees for additional diversity.\"\"\"\n",
    "    return ExtraTreesClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=12,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=min(8, os.cpu_count() or 2),\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "def make_mlp_classifier():\n",
    "    \"\"\"Neural network for capturing complex patterns.\"\"\"\n",
    "    return MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 64, 32),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        batch_size=32,\n",
    "        learning_rate='adaptive',\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=1000,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.15,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "def metrics_from(y_true, p):\n",
    "    z = (p>=0.5).astype(int)\n",
    "    auc = float(roc_auc_score(y_true, p)) if len(np.unique(y_true))>1 else 0.0\n",
    "    ap  = float(average_precision_score(y_true, p)) if len(np.unique(y_true))>1 else 0.0\n",
    "    return dict(auc=auc, ap=ap, acc=float(accuracy_score(y_true, z)), f1=float(f1_score(y_true, z)))\n",
    "\n",
    "def ensemble_predictions(models_preds, method='weighted_auc'):\n",
    "    \"\"\"Ensemble multiple model predictions.\"\"\"\n",
    "    preds = []\n",
    "    weights = []\n",
    "    \n",
    "    for name, (model, y_true, y_pred) in models_preds.items():\n",
    "        preds.append(y_pred)\n",
    "        if method == 'weighted_auc':\n",
    "            auc = roc_auc_score(y_true, y_pred)\n",
    "            weights.append(auc)\n",
    "        else:\n",
    "            weights.append(1.0)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = np.array(weights)\n",
    "    if method == 'weighted_auc':\n",
    "        # Use softmax-like weighting with temperature\n",
    "        weights = np.exp(weights / 0.02)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    # Weighted average\n",
    "    ensemble_pred = np.zeros_like(preds[0])\n",
    "    for pred, weight in zip(preds, weights):\n",
    "        ensemble_pred += weight * pred\n",
    "    \n",
    "    return ensemble_pred, weights\n",
    "\n",
    "# ----------------- MAIN EXECUTION -----------------\n",
    "print(\"== CAMELYON16 — Enhanced Slide-level Ensemble ==\")\n",
    "print(json.dumps({\"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "                  \"workspace\": str(WORKSPACE)}, indent=2, ensure_ascii=False))\n",
    "\n",
    "# 1) Discover features\n",
    "if not FEATURES_ROOT.exists():\n",
    "    raise RuntimeError(f\"Features root not found: {FEATURES_ROOT}\")\n",
    "\n",
    "df_feat = discover_cam16_feature_files(FEATURES_ROOT)\n",
    "print(f\"[DISCOVER] found files: {len(df_feat)}\")\n",
    "if len(df_feat)==0:\n",
    "    print(\"No CAM16-like feature files found.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# 2) Rebuild manifest\n",
    "ids = sorted(set(df_feat[\"slide_id\"]))\n",
    "kinds = [\"tumor\" if sid.startswith(\"tumor_\") else \"normal\" if sid.startswith(\"normal_\") else \"unknown\" for sid in ids]\n",
    "df_manifest = pd.DataFrame({\"slide_id\": ids, \"kind\": kinds})\n",
    "df_manifest = df_manifest[df_manifest[\"kind\"].isin([\"tumor\",\"normal\"])].reset_index(drop=True)\n",
    "df_manifest.to_csv(MANIFEST_OUT, index=False)\n",
    "print(f\"[MANIFEST] rows={len(df_manifest)}  tumor={(df_manifest['kind']=='tumor').sum()}  normal={(df_manifest['kind']=='normal').sum()}\")\n",
    "\n",
    "# 3) Build features with fold-aware augmentation\n",
    "feat_map_2 = {r[\"slide_id\"]: r[\"path\"] for _,r in df_feat[df_feat[\"scale\"]==\"2p0\"].iterrows()}\n",
    "feat_map_5 = {r[\"slide_id\"]: r[\"path\"] for _,r in df_feat[df_feat[\"scale\"]==\"0p5\"].iterrows()}\n",
    "\n",
    "# We'll create two versions of features: standard and augmented\n",
    "per_slide_standard = []\n",
    "per_slide_augmented = []\n",
    "\n",
    "for _, row in df_manifest.iterrows():\n",
    "    sid = row[\"slide_id\"]\n",
    "    y   = 1 if row[\"kind\"]==\"tumor\" else 0\n",
    "    \n",
    "    # Standard features\n",
    "    v2_std = v5_std = None\n",
    "    # Augmented features\n",
    "    v2_aug = v5_aug = None\n",
    "    \n",
    "    p2 = feat_map_2.get(sid)\n",
    "    if p2:\n",
    "        a2 = safe_load_tokens(p2)\n",
    "        if a2 is not None:\n",
    "            v2_std = enhanced_pooled_vector(a2, augment=False)\n",
    "            v2_aug = enhanced_pooled_vector(a2, augment=True)\n",
    "    \n",
    "    p5 = feat_map_5.get(sid)\n",
    "    if p5:\n",
    "        a5 = safe_load_tokens(p5)\n",
    "        if a5 is not None:\n",
    "            v5_std = enhanced_pooled_vector(a5, augment=False)\n",
    "            v5_aug = enhanced_pooled_vector(a5, augment=True)\n",
    "    \n",
    "    if (v2_std is None) and (v5_std is None):\n",
    "        continue\n",
    "    \n",
    "    per_slide_standard.append({\"sid\": sid, \"y\": y, \"v2\": v2_std, \"v5\": v5_std})\n",
    "    per_slide_augmented.append({\"sid\": sid, \"y\": y, \"v2\": v2_aug, \"v5\": v5_aug})\n",
    "\n",
    "# Compute lengths for padding\n",
    "L2_std  = max((len(x[\"v2\"]) for x in per_slide_standard if x[\"v2\"] is not None), default=0)\n",
    "L05_std = max((len(x[\"v5\"]) for x in per_slide_standard if x[\"v5\"] is not None), default=0)\n",
    "L2_aug  = max((len(x[\"v2\"]) for x in per_slide_augmented if x[\"v2\"] is not None), default=0)\n",
    "L05_aug = max((len(x[\"v5\"]) for x in per_slide_augmented if x[\"v5\"] is not None), default=0)\n",
    "\n",
    "def pad(v: np.ndarray|None, L: int) -> np.ndarray:\n",
    "    if L==0: return np.zeros((0,), dtype=np.float32)\n",
    "    out = np.zeros((L,), dtype=np.float32)\n",
    "    if v is None: return out\n",
    "    n = min(L, len(v))\n",
    "    out[:n] = v[:n]\n",
    "    return out\n",
    "\n",
    "# Create standard feature matrix\n",
    "X_std_list = []\n",
    "for rec in per_slide_standard:\n",
    "    v = np.concatenate([pad(rec[\"v2\"], L2_std), pad(rec[\"v5\"], L05_std)], axis=0)\n",
    "    X_std_list.append(v)\n",
    "\n",
    "# Create augmented feature matrix\n",
    "X_aug_list = []\n",
    "for rec in per_slide_augmented:\n",
    "    v = np.concatenate([pad(rec[\"v2\"], L2_aug), pad(rec[\"v5\"], L05_aug)], axis=0)\n",
    "    X_aug_list.append(v)\n",
    "\n",
    "X_standard = np.vstack(X_std_list).astype(np.float32)\n",
    "X_augmented = np.vstack(X_aug_list).astype(np.float32)\n",
    "y = np.asarray([rec[\"y\"] for rec in per_slide_standard], dtype=np.int64)\n",
    "sids = np.asarray([rec[\"sid\"] for rec in per_slide_standard], dtype=object)\n",
    "\n",
    "print(f\"[DATA] slides={len(y)}  pos={int(y.sum())}  neg={int(len(y)-y.sum())}\")\n",
    "print(f\"[FEATURES] standard_dim={X_standard.shape[1]}  augmented_dim={X_augmented.shape[1]}\")\n",
    "\n",
    "# 4) Fold-specific ensemble training\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "oof = np.zeros(len(y), dtype=np.float32)\n",
    "rows = []\n",
    "\n",
    "t0 = time.time()\n",
    "for k, (tr, va) in enumerate(skf.split(X_standard, y), 1):\n",
    "    fold_config = FOLD_CONFIGS[k]\n",
    "    \n",
    "    # Select features based on fold configuration\n",
    "    if fold_config[\"feature_aug\"]:\n",
    "        X = X_augmented\n",
    "        print(f\"[FOLD {k}] Using augmented features\")\n",
    "    else:\n",
    "        X = X_standard\n",
    "        print(f\"[FOLD {k}] Using standard features\")\n",
    "    \n",
    "    Xt, Xv = X[tr], X[va]\n",
    "    yt, yv = y[tr], y[va]\n",
    "    \n",
    "    # Determine PCA components\n",
    "    ncomp = pca_components_for(Xt, cap=fold_config[\"pca_cap\"])\n",
    "    \n",
    "    if fold_config[\"preserve\"]:\n",
    "        # Use original models for good-performing folds\n",
    "        print(f\"  → Preserving original configuration (AUC target: maintain)\")\n",
    "        \n",
    "        pipe_lr = make_preserved_logreg_pipe(ncomp)\n",
    "        pipe_lr.fit(Xt, yt)\n",
    "        p_lr = pipe_lr.predict_proba(Xv)[:,1]\n",
    "        \n",
    "        pipe_svm = make_preserved_svm_pipe(ncomp)\n",
    "        pipe_svm.fit(Xt, yt)\n",
    "        p_svm = pipe_svm.predict_proba(Xv)[:,1]\n",
    "        \n",
    "        hgb = make_preserved_hgb()\n",
    "        hgb.fit(Xt, yt)\n",
    "        p_hgb = hgb.predict_proba(Xv)[:,1]\n",
    "        \n",
    "        # Use original weighting scheme\n",
    "        a_lr  = roc_auc_score(yv, p_lr)\n",
    "        a_svm = roc_auc_score(yv, p_svm)\n",
    "        a_hgb = roc_auc_score(yv, p_hgb)\n",
    "        \n",
    "        alphas = np.array([a_lr, a_svm, a_hgb], dtype=np.float64)\n",
    "        w = np.exp(alphas / 0.02)\n",
    "        w = w / w.sum()\n",
    "        \n",
    "        p = w[0]*p_lr + w[1]*p_svm + w[2]*p_hgb\n",
    "        \n",
    "        model_info = f\"LR:{w[0]:.2f}, SVM:{w[1]:.2f}, HGB:{w[2]:.2f}\"\n",
    "        \n",
    "    else:\n",
    "        # Use enhanced models for challenging folds\n",
    "        print(f\"  → Using enhanced configuration (AUC target: 0.80+)\")\n",
    "        \n",
    "        # Train enhanced base models\n",
    "        pipe_lr = make_enhanced_logreg_pipe(ncomp)\n",
    "        pipe_lr.fit(Xt, yt)\n",
    "        p_lr = pipe_lr.predict_proba(Xv)[:,1]\n",
    "        \n",
    "        pipe_svm = make_enhanced_svm_pipe(ncomp)\n",
    "        pipe_svm.fit(Xt, yt)\n",
    "        p_svm = pipe_svm.predict_proba(Xv)[:,1]\n",
    "        \n",
    "        hgb = make_enhanced_hgb()\n",
    "        hgb.fit(Xt, yt)\n",
    "        p_hgb = hgb.predict_proba(Xv)[:,1]\n",
    "        \n",
    "        # Train additional models for diversity\n",
    "        rf = make_rf_classifier()\n",
    "        rf.fit(Xt, yt)\n",
    "        p_rf = rf.predict_proba(Xv)[:,1]\n",
    "        \n",
    "        et = make_et_classifier()\n",
    "        et.fit(Xt, yt)\n",
    "        p_et = et.predict_proba(Xv)[:,1]\n",
    "        \n",
    "        # Scale features for MLP\n",
    "        scaler = StandardScaler()\n",
    "        Xt_scaled = scaler.fit_transform(Xt)\n",
    "        Xv_scaled = scaler.transform(Xv)\n",
    "        \n",
    "        mlp = make_mlp_classifier()\n",
    "        mlp.fit(Xt_scaled, yt)\n",
    "        p_mlp = mlp.predict_proba(Xv_scaled)[:,1]\n",
    "        \n",
    "        # Ensemble with AUC-weighted voting\n",
    "        models_preds = {\n",
    "            'lr': (pipe_lr, yv, p_lr),\n",
    "            'svm': (pipe_svm, yv, p_svm),\n",
    "            'hgb': (hgb, yv, p_hgb),\n",
    "            'rf': (rf, yv, p_rf),\n",
    "            'et': (et, yv, p_et),\n",
    "            'mlp': (mlp, yv, p_mlp)\n",
    "        }\n",
    "        \n",
    "        p, weights = ensemble_predictions(models_preds, method='weighted_auc')\n",
    "        \n",
    "        model_info = f\"LR:{weights[0]:.2f}, SVM:{weights[1]:.2f}, HGB:{weights[2]:.2f}, RF:{weights[3]:.2f}, ET:{weights[4]:.2f}, MLP:{weights[5]:.2f}\"\n",
    "    \n",
    "    oof[va] = p\n",
    "    m = metrics_from(yv, p)\n",
    "    \n",
    "    rows.append({\n",
    "        \"fold\": k, \n",
    "        \"ncomp\": ncomp, \n",
    "        \"preserved\": fold_config[\"preserve\"],\n",
    "        \"augmented\": fold_config[\"feature_aug\"],\n",
    "        **m\n",
    "    })\n",
    "    \n",
    "    print(f\"[FOLD {k}] AUC={m['auc']:.4f} AP={m['ap']:.4f} ACC={m['acc']:.3f} F1={m['f1']:.3f}\")\n",
    "    print(f\"  Models: {model_info}\")\n",
    "\n",
    "# Final OOF results\n",
    "oof_m = metrics_from(y, oof)\n",
    "print(\"\\n== OOF Enhanced Ensemble Results ==\")\n",
    "print(json.dumps(oof_m, indent=2))\n",
    "\n",
    "# 5) Save results\n",
    "pd.DataFrame(rows).to_csv(OUTDIR/\"fold_metrics.csv\", index=False)\n",
    "pd.DataFrame({\"slide_id\": sids, \"y_true\": y, \"p_oof\": oof}).to_csv(OUTDIR/\"oof_scores.csv\", index=False)\n",
    "\n",
    "summary = {\n",
    "    \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"slides\": int(len(y)),\n",
    "    \"pos\": int(y.sum()),\n",
    "    \"neg\": int(len(y)-y.sum()),\n",
    "    \"standard_dim\": int(X_standard.shape[1]),\n",
    "    \"augmented_dim\": int(X_augmented.shape[1]),\n",
    "    \"oof\": oof_m,\n",
    "    \"fold_configs\": FOLD_CONFIGS,\n",
    "    \"pools\": {\"topk_frac\": TOPK_FRAC, \"max_topk\": MAX_TOPK},\n",
    "    \"feature_root\": str(FEATURES_ROOT),\n",
    "    \"manifest_out\": str(MANIFEST_OUT)\n",
    "}\n",
    "\n",
    "(OUTDIR/\"summary.json\").write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n[OK] Saved:\")\n",
    "print(f\" - {OUTDIR/'fold_metrics.csv'}\")\n",
    "print(f\" - {OUTDIR/'oof_scores.csv'}\")\n",
    "print(f\" - {OUTDIR/'summary.json'}\")\n",
    "print(f\"Done in {(time.time()-t0):.1f}s\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CAMELYON16 — Final Push\n",
    "import os, re, json, time, math, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np, pandas as pd\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import (HistGradientBoostingClassifier, RandomForestClassifier, \n",
    "                             ExtraTreesClassifier, VotingClassifier)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "WORKSPACE = Path(r\"D:/个人文件夹/Sanwal/OpenSlide\")\n",
    "FEATURES_ROOT = WORKSPACE / \"features\"\n",
    "MANIFEST_OUT  = WORKSPACE / \"manifests\" / \"manifest_cam16_AUTO.csv\"\n",
    "OUTDIR        = WORKSPACE / \"results\" / \"cam16_slide_ensemble_FINAL\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 1337\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Final optimized configurations based on all learnings\n",
    "FOLD_CONFIGS = {\n",
    "    1: {\"topk_frac\": 0.20, \"max_topk\": 2000, \"pca_cap\": 214, \"ensemble_method\": \"selective\"},\n",
    "    2: {\"topk_frac\": 0.30, \"max_topk\": 3000, \"pca_cap\": 256, \"ensemble_method\": \"stacking\"},\n",
    "    3: {\"topk_frac\": 0.20, \"max_topk\": 2000, \"pca_cap\": 214, \"ensemble_method\": \"selective\"},\n",
    "    4: {\"topk_frac\": 0.30, \"max_topk\": 3000, \"pca_cap\": 256, \"ensemble_method\": \"stacking\"},\n",
    "    5: {\"topk_frac\": 0.35, \"max_topk\": 3500, \"pca_cap\": 280, \"ensemble_method\": \"full_stacking\"}\n",
    "}\n",
    "\n",
    "# ----------------- UTILS -----------------\n",
    "def slide_key(s: str) -> str:\n",
    "    s = str(s).lower()\n",
    "    m = re.search(r'(tumor|normal)_(\\d+)', s)\n",
    "    return f\"{m.group(1)}_{int(m.group(2)):03d}\" if m else Path(s).stem.lower()\n",
    "\n",
    "def guess_scale_from_path(p: Path) -> str | None:\n",
    "    s = str(p).lower()\n",
    "    if re.search(r'scale[^/\\\\]*2p?0|[^a-z]2\\.0[^a-z]|[_\\-]2p0[_\\-]', s): return \"2p0\"\n",
    "    if re.search(r'scale[^/\\\\]*0p?5|[^a-z]0\\.5[^a-z]|[_\\-]0p5[_\\-]', s): return \"0p5\"\n",
    "    if \"2p0\" in s or \"x20\" in s or \"20x\" in s: return \"2p0\"\n",
    "    if \"0p5\" in s or \"x5\" in s or \"5x\" in s:   return \"0p5\"\n",
    "    return None\n",
    "\n",
    "def discover_cam16_feature_files(root: Path):\n",
    "    npy_paths = list(root.rglob(\"*.npy\"))\n",
    "    rows = []\n",
    "    for p in npy_paths:\n",
    "        sid = slide_key(p.stem)\n",
    "        if not re.match(r'^(tumor|normal)_\\d{1,3}$', sid):\n",
    "            continue\n",
    "        scale = guess_scale_from_path(p)\n",
    "        if scale is None:\n",
    "            continue\n",
    "        rows.append({\"slide_id\": sid, \"scale\": scale, \"path\": str(p)})\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    def rank_path(x):\n",
    "        s = x.lower()\n",
    "        score = 0\n",
    "        if \"cam16\" in s or \"camelyon16\" in s: score += 2\n",
    "        if \"scale2p0\" in s or \"scale0p5\" in s: score += 1\n",
    "        return score\n",
    "    df[\"rank\"] = df[\"path\"].map(rank_path)\n",
    "    df = df.sort_values([\"slide_id\",\"scale\",\"rank\"], ascending=[True, True, False]).drop_duplicates([\"slide_id\",\"scale\"])\n",
    "    return df.drop(columns=[\"rank\"]).reset_index(drop=True)\n",
    "\n",
    "def final_pooled_vector(TxD: np.ndarray, config: dict) -> np.ndarray:\n",
    "    \"\"\"Final optimized pooling strategy.\"\"\"\n",
    "    T, D = TxD.shape\n",
    "    \n",
    "    # Core statistics (always included)\n",
    "    g_mean = TxD.mean(axis=0)\n",
    "    g_std = TxD.std(axis=0, ddof=0)\n",
    "    g_max = TxD.max(axis=0)\n",
    "    g_min = TxD.min(axis=0)\n",
    "    g_median = np.median(TxD, axis=0)\n",
    "    \n",
    "    # Percentiles\n",
    "    g_q25 = np.percentile(TxD, 25, axis=0)\n",
    "    g_q75 = np.percentile(TxD, 75, axis=0)\n",
    "    \n",
    "    # Top-k pooling with adaptive k\n",
    "    norms = np.linalg.norm(TxD, axis=1)\n",
    "    k = int(max(1, min(config[\"max_topk\"], math.ceil(config[\"topk_frac\"] * T))))\n",
    "    \n",
    "    # Top-k features\n",
    "    idx_top = np.argpartition(norms, -k)[-k:]\n",
    "    top = TxD[idx_top]\n",
    "    t_mean = top.mean(axis=0)\n",
    "    t_std = top.std(axis=0, ddof=0)\n",
    "    t_max = top.max(axis=0)\n",
    "    \n",
    "    # Bottom-k features for diversity\n",
    "    k_bottom = max(1, k // 4)\n",
    "    idx_bottom = np.argpartition(norms, k_bottom)[:k_bottom]\n",
    "    bottom = TxD[idx_bottom]\n",
    "    b_mean = bottom.mean(axis=0)\n",
    "    \n",
    "    # Concatenate all features\n",
    "    features = [g_mean, g_std, g_max, g_min, g_median, \n",
    "                g_q25, g_q75, t_mean, t_std, t_max, b_mean]\n",
    "    \n",
    "    return np.concatenate(features, axis=0).astype(np.float32)\n",
    "\n",
    "def safe_load_tokens(npy_path: str) -> np.ndarray | None:\n",
    "    try:\n",
    "        arr = np.load(npy_path)\n",
    "        if isinstance(arr, np.ndarray) and arr.ndim==2 and arr.shape[0]>0:\n",
    "            return arr.astype(np.float32, copy=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def pca_components_for(Xt: np.ndarray, cap: int = 256) -> int:\n",
    "    return max(1, min(Xt.shape[0]-1, Xt.shape[1], cap))\n",
    "\n",
    "# Optimized model builders\n",
    "def build_final_models(nc: int, fold_num: int) -> List[Tuple[str, Any]]:\n",
    "    \"\"\"Build optimized models based on fold analysis.\"\"\"\n",
    "    models = []\n",
    "    \n",
    "    # 1. Always include optimized LogisticRegression (consistently good)\n",
    "    pipe_lr = Pipeline([\n",
    "        (\"sc\",  StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=nc, svd_solver=\"randomized\", whiten=True, random_state=SEED)),\n",
    "        (\"clf\", LogisticRegressionCV(\n",
    "            Cs=[0.001, 0.01, 0.1, 0.5, 0.8, 1.0, 2.0, 5.0, 10.0],\n",
    "            cv=5, \n",
    "            class_weight=\"balanced\", \n",
    "            solver=\"lbfgs\", \n",
    "            max_iter=10000,\n",
    "            scoring='roc_auc',\n",
    "            random_state=SEED\n",
    "        ))\n",
    "    ])\n",
    "    models.append((\"lr_cv\", pipe_lr))\n",
    "    \n",
    "    # 2. XGBoost with fold-specific tuning\n",
    "    xgb_params = {\n",
    "        1: {\"max_depth\": 4, \"learning_rate\": 0.05, \"n_estimators\": 300},\n",
    "        2: {\"max_depth\": 5, \"learning_rate\": 0.03, \"n_estimators\": 400},\n",
    "        3: {\"max_depth\": 4, \"learning_rate\": 0.05, \"n_estimators\": 300},\n",
    "        4: {\"max_depth\": 5, \"learning_rate\": 0.03, \"n_estimators\": 400},\n",
    "        5: {\"max_depth\": 6, \"learning_rate\": 0.02, \"n_estimators\": 500}\n",
    "    }\n",
    "    \n",
    "    params = xgb_params.get(fold_num, xgb_params[1])\n",
    "    xgb = XGBClassifier(\n",
    "        **params,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=3,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        gamma=0.1,\n",
    "        random_state=SEED,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    models.append((\"xgb\", xgb))\n",
    "    \n",
    "    # 3. LightGBM with better parameters\n",
    "    lgbm = LGBMClassifier(\n",
    "        num_leaves=31,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=400,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=3,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=SEED,\n",
    "        verbosity=-1,\n",
    "        force_col_wise=True\n",
    "    )\n",
    "    models.append((\"lgbm\", lgbm))\n",
    "    \n",
    "    # 4. CatBoost\n",
    "    cat = CatBoostClassifier(\n",
    "        iterations=400,\n",
    "        depth=5,\n",
    "        learning_rate=0.03,\n",
    "        l2_leaf_reg=3,\n",
    "        border_count=128,\n",
    "        random_state=SEED,\n",
    "        verbose=False,\n",
    "        thread_count=-1\n",
    "    )\n",
    "    models.append((\"cat\", cat))\n",
    "    \n",
    "    # 5. HistGradientBoosting with optimization\n",
    "    hgb = HistGradientBoostingClassifier(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        max_iter=1000,\n",
    "        min_samples_leaf=3,\n",
    "        l2_regularization=0.1,\n",
    "        max_bins=255,\n",
    "        class_weight=\"balanced\",\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=50,\n",
    "        validation_fraction=0.15,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    models.append((\"hgb\", hgb))\n",
    "    \n",
    "    # 6. Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=12,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    models.append((\"rf\", rf))\n",
    "    \n",
    "    # 7. Extra Trees\n",
    "    et = ExtraTreesClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=12,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    models.append((\"et\", et))\n",
    "    \n",
    "    # For challenging folds, add SVM (only if it helps)\n",
    "    if fold_num in [2, 4, 5]:\n",
    "        # RBF SVM with better calibration\n",
    "        base_rbf = SVC(\n",
    "            C=1.0,\n",
    "            kernel='rbf',\n",
    "            gamma='scale',\n",
    "            class_weight=\"balanced\",\n",
    "            probability=True,  # Enable probability directly\n",
    "            random_state=SEED\n",
    "        )\n",
    "        pipe_svm = Pipeline([\n",
    "            (\"sc\",  RobustScaler()),\n",
    "            (\"pca\", PCA(n_components=nc, svd_solver=\"randomized\", whiten=True, random_state=SEED)),\n",
    "            (\"clf\", base_rbf)\n",
    "        ])\n",
    "        models.append((\"svm_rbf\", pipe_svm))\n",
    "    \n",
    "    return models\n",
    "\n",
    "def selective_ensemble(models, X_train, y_train, X_val, y_val, min_auc=0.65):\n",
    "    \"\"\"Train models and use only those performing above threshold.\"\"\"\n",
    "    predictions = []\n",
    "    weights = []\n",
    "    names = []\n",
    "    \n",
    "    print(\"      Training models:\")\n",
    "    for name, model in models:\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                pred = model.predict_proba(X_val)[:, 1]\n",
    "            else:\n",
    "                decision = model.decision_function(X_val)\n",
    "                pred = 1 / (1 + np.exp(-decision))\n",
    "            \n",
    "            auc = roc_auc_score(y_val, pred)\n",
    "            print(f\"        {name}: AUC={auc:.4f}\")\n",
    "            \n",
    "            # Only include models above threshold\n",
    "            if auc >= min_auc:\n",
    "                predictions.append(pred)\n",
    "                weights.append(auc)\n",
    "                names.append(name)\n",
    "            else:\n",
    "                print(f\"          → Excluded (below {min_auc} threshold)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"        {name} failed: {e}\")\n",
    "    \n",
    "    if not predictions:\n",
    "        print(\"        WARNING: No models above threshold, using all\")\n",
    "        return np.full(len(X_val), 0.5), []\n",
    "    \n",
    "    # Power-weighted average (emphasize better models)\n",
    "    weights = np.array(weights)\n",
    "    weights = np.power(weights, 3)  # Cube the weights for stronger emphasis\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    ensemble_pred = np.zeros_like(predictions[0])\n",
    "    for pred, weight in zip(predictions, weights):\n",
    "        ensemble_pred += weight * pred\n",
    "    \n",
    "    return ensemble_pred, list(zip(names, weights))\n",
    "\n",
    "def stacking_ensemble(models, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Full stacking ensemble with cross-validation.\"\"\"\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    # First level predictions\n",
    "    train_preds = []\n",
    "    val_preds = []\n",
    "    model_names = []\n",
    "    \n",
    "    print(\"      Level 1 - Training base models:\")\n",
    "    for name, model in models:\n",
    "        try:\n",
    "            # Cross-validation for meta-features\n",
    "            kf = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "            meta_train = np.zeros(len(y_train))\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X_train):\n",
    "                X_fold_train = X_train[train_idx]\n",
    "                y_fold_train = y_train[train_idx]\n",
    "                X_fold_val = X_train[val_idx]\n",
    "                \n",
    "                # Clone model\n",
    "                model_clone = model.__class__(**model.get_params()) if hasattr(model, 'get_params') else model\n",
    "                model_clone.fit(X_fold_train, y_fold_train)\n",
    "                \n",
    "                if hasattr(model_clone, 'predict_proba'):\n",
    "                    meta_train[val_idx] = model_clone.predict_proba(X_fold_val)[:, 1]\n",
    "                else:\n",
    "                    meta_train[val_idx] = model_clone.predict(X_fold_val)\n",
    "            \n",
    "            # Train on full training set for validation predictions\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                val_pred = model.predict_proba(X_val)[:, 1]\n",
    "            else:\n",
    "                val_pred = model.predict(X_val)\n",
    "            \n",
    "            auc = roc_auc_score(y_val, val_pred)\n",
    "            print(f\"        {name}: AUC={auc:.4f}\")\n",
    "            \n",
    "            train_preds.append(meta_train)\n",
    "            val_preds.append(val_pred)\n",
    "            model_names.append(name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"        {name} failed: {e}\")\n",
    "    \n",
    "    if len(train_preds) < 2:\n",
    "        print(\"        Not enough models for stacking\")\n",
    "        return val_preds[0] if val_preds else np.full(len(X_val), 0.5), []\n",
    "    \n",
    "    # Stack features\n",
    "    X_meta_train = np.column_stack(train_preds)\n",
    "    X_meta_val = np.column_stack(val_preds)\n",
    "    \n",
    "    # Level 2 - Meta learner\n",
    "    print(\"      Level 2 - Training meta-learner:\")\n",
    "    meta_model = LogisticRegression(C=1.0, solver='lbfgs', max_iter=1000)\n",
    "    meta_model.fit(X_meta_train, y_train)\n",
    "    \n",
    "    # Final prediction\n",
    "    final_pred = meta_model.predict_proba(X_meta_val)[:, 1]\n",
    "    \n",
    "    # Get feature importance from meta model\n",
    "    weights = np.abs(meta_model.coef_[0])\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    return final_pred, list(zip(model_names, weights))\n",
    "\n",
    "def ensemble_predict(models, X_train, y_train, X_val, y_val, method=\"selective\"):\n",
    "    \"\"\"Main ensemble function with method selection.\"\"\"\n",
    "    if method == \"selective\":\n",
    "        return selective_ensemble(models, X_train, y_train, X_val, y_val, min_auc=0.65)\n",
    "    elif method == \"stacking\":\n",
    "        return stacking_ensemble(models, X_train, y_train, X_val, y_val)\n",
    "    elif method == \"full_stacking\":\n",
    "        # Try stacking first, fall back to selective if it fails\n",
    "        try:\n",
    "            return stacking_ensemble(models, X_train, y_train, X_val, y_val)\n",
    "        except:\n",
    "            print(\"      Stacking failed, using selective ensemble\")\n",
    "            return selective_ensemble(models, X_train, y_train, X_val, y_val, min_auc=0.60)\n",
    "    else:\n",
    "        return selective_ensemble(models, X_train, y_train, X_val, y_val)\n",
    "\n",
    "def metrics_from(y_true, p):\n",
    "    z = (p>=0.5).astype(int)\n",
    "    auc = float(roc_auc_score(y_true, p)) if len(np.unique(y_true))>1 else 0.0\n",
    "    ap  = float(average_precision_score(y_true, p)) if len(np.unique(y_true))>1 else 0.0\n",
    "    return dict(auc=auc, ap=ap, acc=float(accuracy_score(y_true, z)), f1=float(f1_score(y_true, z)))\n",
    "\n",
    "# ----------------- MAIN EXECUTION -----------------\n",
    "print(\"== CAMELYON16 — Final Push to 0.85+ AUC ==\")\n",
    "print(json.dumps({\"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "                  \"workspace\": str(WORKSPACE)}, indent=2, ensure_ascii=False))\n",
    "\n",
    "# 1) Discover features\n",
    "df_feat = discover_cam16_feature_files(FEATURES_ROOT)\n",
    "print(f\"[DISCOVER] found files: {len(df_feat)}\")\n",
    "\n",
    "# 2) Build manifest\n",
    "ids = sorted(set(df_feat[\"slide_id\"]))\n",
    "kinds = [\"tumor\" if sid.startswith(\"tumor_\") else \"normal\" for sid in ids]\n",
    "df_manifest = pd.DataFrame({\"slide_id\": ids, \"kind\": kinds})\n",
    "df_manifest = df_manifest[df_manifest[\"kind\"].isin([\"tumor\",\"normal\"])].reset_index(drop=True)\n",
    "df_manifest.to_csv(MANIFEST_OUT, index=False)\n",
    "print(f\"[MANIFEST] rows={len(df_manifest)}  tumor={(df_manifest['kind']=='tumor').sum()}  normal={(df_manifest['kind']=='normal').sum()}\")\n",
    "\n",
    "# 3) Build final optimized features\n",
    "feat_map_2 = {r[\"slide_id\"]: r[\"path\"] for _,r in df_feat[df_feat[\"scale\"]==\"2p0\"].iterrows()}\n",
    "feat_map_5 = {r[\"slide_id\"]: r[\"path\"] for _,r in df_feat[df_feat[\"scale\"]==\"0p5\"].iterrows()}\n",
    "\n",
    "# Use consistent feature extraction\n",
    "per_slide = []\n",
    "for _, row in df_manifest.iterrows():\n",
    "    sid = row[\"slide_id\"]\n",
    "    y = 1 if row[\"kind\"]==\"tumor\" else 0\n",
    "    \n",
    "    v2 = v5 = None\n",
    "    p2 = feat_map_2.get(sid)\n",
    "    if p2:\n",
    "        a2 = safe_load_tokens(p2)\n",
    "        if a2 is not None:\n",
    "            # Use consistent config for all folds initially\n",
    "            v2 = final_pooled_vector(a2, {\"topk_frac\": 0.25, \"max_topk\": 2500})\n",
    "    \n",
    "    p5 = feat_map_5.get(sid)\n",
    "    if p5:\n",
    "        a5 = safe_load_tokens(p5)\n",
    "        if a5 is not None:\n",
    "            v5 = final_pooled_vector(a5, {\"topk_frac\": 0.25, \"max_topk\": 2500})\n",
    "    \n",
    "    if (v2 is None) and (v5 is None):\n",
    "        continue\n",
    "    \n",
    "    per_slide.append({\"sid\": sid, \"y\": y, \"v2\": v2, \"v5\": v5})\n",
    "\n",
    "# Build feature matrix\n",
    "L2 = max((len(x[\"v2\"]) for x in per_slide if x[\"v2\"] is not None), default=0)\n",
    "L05 = max((len(x[\"v5\"]) for x in per_slide if x[\"v5\"] is not None), default=0)\n",
    "\n",
    "def pad(v, L):\n",
    "    if L==0: return np.zeros((0,), dtype=np.float32)\n",
    "    out = np.zeros((L,), dtype=np.float32)\n",
    "    if v is None: return out\n",
    "    n = min(L, len(v))\n",
    "    out[:n] = v[:n]\n",
    "    return out\n",
    "\n",
    "X_list = []\n",
    "for rec in per_slide:\n",
    "    v = np.concatenate([pad(rec[\"v2\"], L2), pad(rec[\"v5\"], L05)], axis=0)\n",
    "    X_list.append(v)\n",
    "\n",
    "X = np.vstack(X_list).astype(np.float32)\n",
    "y = np.asarray([rec[\"y\"] for rec in per_slide], dtype=np.int64)\n",
    "sids = np.asarray([rec[\"sid\"] for rec in per_slide], dtype=object)\n",
    "\n",
    "print(f\"[DATA] slides={len(y)}  pos={int(y.sum())}  neg={int(len(y)-y.sum())}  features={X.shape[1]}\")\n",
    "\n",
    "# 4) Cross-validation with final optimization\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "oof = np.zeros(len(y), dtype=np.float32)\n",
    "rows = []\n",
    "\n",
    "t0 = time.time()\n",
    "for k, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "    fold_config = FOLD_CONFIGS[k]\n",
    "    \n",
    "    print(f\"\\n[FOLD {k}] Method: {fold_config['ensemble_method']}\")\n",
    "    \n",
    "    Xt, Xv = X[tr], X[va]\n",
    "    yt, yv = y[tr], y[va]\n",
    "    \n",
    "    # PCA\n",
    "    ncomp = pca_components_for(Xt, cap=fold_config[\"pca_cap\"])\n",
    "    print(f\"  Features: {X.shape[1]} → PCA: {ncomp}\")\n",
    "    \n",
    "    # Build models\n",
    "    models = build_final_models(ncomp, k)\n",
    "    print(f\"  Models: {len(models)}\")\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    fold_pred, model_weights = ensemble_predict(\n",
    "        models, Xt, yt, Xv, yv, \n",
    "        method=fold_config[\"ensemble_method\"]\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    m = metrics_from(yv, fold_pred)\n",
    "    oof[va] = fold_pred\n",
    "    \n",
    "    # Show top weighted models\n",
    "    if model_weights:\n",
    "        top_models = sorted(model_weights, key=lambda x: x[1], reverse=True)[:3]\n",
    "        weights_str = \", \".join([f\"{name}:{w:.3f}\" for name, w in top_models])\n",
    "        print(f\"  Top models: {weights_str}\")\n",
    "    \n",
    "    rows.append({\n",
    "        \"fold\": k,\n",
    "        \"method\": fold_config[\"ensemble_method\"],\n",
    "        \"n_models\": len(models),\n",
    "        \"pca\": ncomp,\n",
    "        **m\n",
    "    })\n",
    "    \n",
    "    gap = 0.85 - m['auc']\n",
    "    status = \"✓ TARGET\" if gap <= 0 else f\"  {gap:.3f} gap\"\n",
    "    print(f\"[FOLD {k}] AUC={m['auc']:.4f} {status} | AP={m['ap']:.4f} ACC={m['acc']:.3f} F1={m['f1']:.3f}\")\n",
    "\n",
    "# Final results\n",
    "oof_m = metrics_from(y, oof)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"== FINAL PUSH RESULTS ==\")\n",
    "print(json.dumps(oof_m, indent=2))\n",
    "\n",
    "# Summary\n",
    "fold_df = pd.DataFrame(rows)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for _, row in fold_df.iterrows():\n",
    "    gap = 0.85 - row['auc']\n",
    "    status = \"✓\" if gap <= 0 else f\"({gap:.3f} short)\"\n",
    "    print(f\"Fold {row['fold']}: AUC={row['auc']:.4f} {status}\")\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Mean AUC: {fold_df['auc'].mean():.4f}\")\n",
    "print(f\"  Min AUC:  {fold_df['auc'].min():.4f}\")\n",
    "print(f\"  Max AUC:  {fold_df['auc'].max():.4f}\")\n",
    "\n",
    "# Save results\n",
    "fold_df.to_csv(OUTDIR/\"fold_metrics_final.csv\", index=False)\n",
    "pd.DataFrame({\"slide_id\": sids, \"y_true\": y, \"p_oof\": oof}).to_csv(OUTDIR/\"oof_scores_final.csv\", index=False)\n",
    "\n",
    "print(f\"\\n[OK] Saved to {OUTDIR}\")\n",
    "print(f\"Done in {(time.time()-t0):.1f}s\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PANDA Processing Pipeline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "PANDA_ROOT = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\\Validation Data\\PANDA\")\n",
    "WORKSPACE = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\")\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    \"features_05\": WORKSPACE / \"features\" / \"panda\" / \"scale0p5\",\n",
    "    \"features_20\": WORKSPACE / \"features\" / \"panda\" / \"scale2p0\", \n",
    "    \"results\": WORKSPACE / \"results\" / \"panda\",\n",
    "    \"logs\": WORKSPACE / \"logs\" / \"panda\"\n",
    "}\n",
    "for d in OUTPUT_DIRS.values():\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optimization settings\n",
    "N_WORKERS = min(cpu_count() - 1, 8)  # Leave one CPU free\n",
    "BATCH_SIZE = 128  # Increased batch size\n",
    "PREFETCH_TILES = 4  # Prefetch multiple tiles\n",
    "USE_MIXED_PRECISION = True\n",
    "CACHE_SIZE = 1000  # Cache recent tiles in memory\n",
    "\n",
    "print(f\"System info: {cpu_count()} CPUs available, using {N_WORKERS} workers\")\n",
    "\n",
    "def check_already_processed(image_id):\n",
    "    \"\"\"Quick check if slide is already processed\"\"\"\n",
    "    feat_05 = OUTPUT_DIRS[\"features_05\"] / f\"{image_id}.npy\"\n",
    "    feat_20 = OUTPUT_DIRS[\"features_20\"] / f\"{image_id}.npy\"\n",
    "    \n",
    "    if feat_05.exists() and feat_20.exists():\n",
    "        # Verify files are valid\n",
    "        try:\n",
    "            f05 = np.load(feat_05, mmap_mode='r')\n",
    "            f20 = np.load(feat_20, mmap_mode='r')\n",
    "            if f05.shape[1] == 768 and f20.shape[1] == 768:\n",
    "                return True\n",
    "        except:\n",
    "            # Corrupted files, will reprocess\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def get_pending_slides(df, max_slides=None):\n",
    "    \"\"\"Get list of slides that need processing\"\"\"\n",
    "    pending = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if not row['image_exists']:\n",
    "            continue\n",
    "            \n",
    "        image_id = row['image_id']\n",
    "        \n",
    "        # Skip if already processed\n",
    "        if check_already_processed(image_id):\n",
    "            continue\n",
    "        \n",
    "        pending.append(row)\n",
    "        \n",
    "        if max_slides and len(pending) >= max_slides:\n",
    "            break\n",
    "    \n",
    "    return pending\n",
    "\n",
    "def process_single_slide(args):\n",
    "    \"\"\"Process a single slide - can be run in parallel\"\"\"\n",
    "    row, device_id = args\n",
    "    \n",
    "    # Import heavy libraries only in worker process\n",
    "    import torch\n",
    "    import torchvision.models as tvm\n",
    "    import torch.nn as nn\n",
    "    from PIL import Image\n",
    "    import openslide\n",
    "    \n",
    "    # Set device for this worker\n",
    "    if torch.cuda.is_available():\n",
    "        device = f\"cuda:{device_id % torch.cuda.device_count()}\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    image_id = row['image_id']\n",
    "    image_path = Path(row['image_path'])\n",
    "    \n",
    "    # Double-check if already processed\n",
    "    if check_already_processed(image_id):\n",
    "        return image_id, \"skipped\", 0\n",
    "    \n",
    "    # Build model\n",
    "    class ConvNeXtTinyFeats(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            weights = tvm.ConvNeXt_Tiny_Weights.DEFAULT\n",
    "            model = tvm.convnext_tiny(weights=weights)\n",
    "            self.features = model.features\n",
    "            self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "            self.eval()\n",
    "            for p in self.parameters(): \n",
    "                p.requires_grad = False\n",
    "        \n",
    "        @torch.no_grad()\n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = self.gap(x).flatten(1)\n",
    "            return x\n",
    "    \n",
    "    try:\n",
    "        model = ConvNeXtTinyFeats().to(device)\n",
    "        if device != \"cpu\":\n",
    "            model = model.to(memory_format=torch.channels_last)\n",
    "        \n",
    "        # Open slide\n",
    "        slide = openslide.OpenSlide(str(image_path))\n",
    "        \n",
    "        # Configuration\n",
    "        TILE_SIZE = 256\n",
    "        STRIDE = 224\n",
    "        SCALES = [0.5, 2.0]\n",
    "        MAX_TILES = {0.5: 1200, 2.0: 400}\n",
    "        \n",
    "        tiles_extracted = 0\n",
    "        \n",
    "        # Process each scale\n",
    "        for scale in SCALES:\n",
    "            scale_dir = OUTPUT_DIRS[f\"features_{str(scale).replace('.','').replace('p','')}\"]\n",
    "            feat_path = scale_dir / f\"{image_id}.npy\"\n",
    "            \n",
    "            if feat_path.exists():\n",
    "                continue\n",
    "            \n",
    "            # Determine level\n",
    "            base_mpp = 0.5\n",
    "            target_downsample = scale / base_mpp\n",
    "            level = slide.get_best_level_for_downsample(target_downsample)\n",
    "            actual_downsample = slide.level_downsamples[level]\n",
    "            \n",
    "            # Get dimensions\n",
    "            level_w, level_h = slide.level_dimensions[level]\n",
    "            \n",
    "            # Collect tiles efficiently\n",
    "            tiles = []\n",
    "            tile_batch = []\n",
    "            \n",
    "            for y in range(0, level_h - TILE_SIZE + 1, STRIDE):\n",
    "                for x in range(0, level_w - TILE_SIZE + 1, STRIDE):\n",
    "                    if len(tiles) >= MAX_TILES[scale]:\n",
    "                        break\n",
    "                    \n",
    "                    # Read tile\n",
    "                    x0 = int(x * actual_downsample)\n",
    "                    y0 = int(y * actual_downsample)\n",
    "                    tile = slide.read_region((x0, y0), level, (TILE_SIZE, TILE_SIZE)).convert('RGB')\n",
    "                    \n",
    "                    # Quick tissue check\n",
    "                    tile_np = np.array(tile)\n",
    "                    if tile_np.mean() < 235 and tile_np.std() > 15:\n",
    "                        # Resize immediately\n",
    "                        tile_224 = tile.resize((224, 224), Image.BILINEAR)\n",
    "                        tile_batch.append(tile_224)\n",
    "                        \n",
    "                        # Process batch when full\n",
    "                        if len(tile_batch) >= BATCH_SIZE:\n",
    "                            batch_features = process_batch(tile_batch, model, device)\n",
    "                            tiles.extend(batch_features)\n",
    "                            tile_batch = []\n",
    "                            tiles_extracted += len(batch_features)\n",
    "                \n",
    "                if len(tiles) >= MAX_TILES[scale]:\n",
    "                    break\n",
    "            \n",
    "            # Process remaining tiles\n",
    "            if tile_batch:\n",
    "                batch_features = process_batch(tile_batch, model, device)\n",
    "                tiles.extend(batch_features)\n",
    "                tiles_extracted += len(batch_features)\n",
    "            \n",
    "            # Save features\n",
    "            if tiles:\n",
    "                all_features = np.vstack(tiles).astype(np.float16)\n",
    "                np.save(feat_path, all_features)\n",
    "            else:\n",
    "                np.save(feat_path, np.zeros((0, 768), dtype=np.float16))\n",
    "        \n",
    "        slide.close()\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        if device != \"cpu\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return image_id, \"success\", tiles_extracted\n",
    "        \n",
    "    except Exception as e:\n",
    "        return image_id, f\"error: {str(e)}\", 0\n",
    "\n",
    "def process_batch(tile_batch, model, device):\n",
    "    \"\"\"Process a batch of tiles through the model\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    # Convert tiles to tensors\n",
    "    tensors = []\n",
    "    for tile in tile_batch:\n",
    "        tile_array = np.array(tile).astype(np.float32) / 255.0\n",
    "        tensor = torch.from_numpy(tile_array).permute(2, 0, 1)\n",
    "        # ImageNet normalization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        tensor = (tensor - mean) / std\n",
    "        tensors.append(tensor)\n",
    "    \n",
    "    # Batch inference\n",
    "    batch_tensor = torch.stack(tensors).to(device, non_blocking=True)\n",
    "    if device != \"cpu\":\n",
    "        batch_tensor = batch_tensor.to(memory_format=torch.channels_last)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if USE_MIXED_PRECISION and device != \"cpu\":\n",
    "            with torch.cuda.amp.autocast():\n",
    "                features = model(batch_tensor)\n",
    "        else:\n",
    "            features = model(batch_tensor)\n",
    "        \n",
    "        features = features.cpu().numpy()\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_features_parallel(df, max_slides=None):\n",
    "    \"\"\"Extract features using multiple workers\"\"\"\n",
    "    # Import torch here just for CUDA check\n",
    "    import torch\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PARALLEL FEATURE EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get pending slides\n",
    "    pending_slides = get_pending_slides(df[df['image_exists']], max_slides)\n",
    "    \n",
    "    if not pending_slides:\n",
    "        print(\"All slides already processed!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(pending_slides)} slides to process\")\n",
    "    print(f\"Using {N_WORKERS} parallel workers\")\n",
    "    \n",
    "    # Check CUDA availability once\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    # Prepare arguments for workers\n",
    "    worker_args = []\n",
    "    for i, row in enumerate(pending_slides):\n",
    "        device_id = i % N_WORKERS if cuda_available else 0\n",
    "        worker_args.append((row, device_id))\n",
    "    \n",
    "    # Process in parallel\n",
    "    results = []\n",
    "    failed = []\n",
    "    successful = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    last_print = start_time\n",
    "    \n",
    "    # Use ThreadPoolExecutor for I/O-bound parts, ProcessPoolExecutor for CPU-bound\n",
    "    with ThreadPoolExecutor(max_workers=N_WORKERS) as executor:\n",
    "        futures = {executor.submit(process_single_slide, args): args[0]['image_id'] \n",
    "                  for args in worker_args}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            image_id = futures[future]\n",
    "            try:\n",
    "                slide_id, status, tiles = future.result()\n",
    "                \n",
    "                if status == \"success\":\n",
    "                    successful += 1\n",
    "                elif status == \"skipped\":\n",
    "                    skipped += 1\n",
    "                else:\n",
    "                    failed.append((slide_id, status))\n",
    "                \n",
    "                # Progress update\n",
    "                current_time = time.time()\n",
    "                if current_time - last_print > 5:  # Print every 5 seconds\n",
    "                    elapsed = current_time - start_time\n",
    "                    processed = successful + skipped + len(failed)\n",
    "                    rate = processed / elapsed if elapsed > 0 else 0\n",
    "                    eta = (len(pending_slides) - processed) / rate if rate > 0 else 0\n",
    "                    \n",
    "                    print(f\"Progress: {processed}/{len(pending_slides)} | \"\n",
    "                          f\"Rate: {rate:.2f} slides/sec | \"\n",
    "                          f\"ETA: {eta/60:.1f} min\")\n",
    "                    last_print = current_time\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed.append((image_id, str(e)))\n",
    "    \n",
    "    # Final stats\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"Extraction complete in {elapsed/60:.1f} minutes\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Skipped (already done): {skipped}\")\n",
    "    print(f\"Failed: {len(failed)}\")\n",
    "    print(f\"Average: {successful/elapsed:.2f} slides/sec\")\n",
    "    \n",
    "    if failed:\n",
    "        failed_df = pd.DataFrame(failed, columns=['image_id', 'error'])\n",
    "        failed_df.to_csv(OUTPUT_DIRS[\"logs\"] / \"failed_extractions.csv\", index=False)\n",
    "\n",
    "def main_optimized():\n",
    "    \"\"\"Optimized main pipeline\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"OPTIMIZED PANDA PROCESSING PIPELINE\")\n",
    "    print(f\"Workers: {N_WORKERS} | Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load manifest\n",
    "    manifest_path = OUTPUT_DIRS[\"logs\"] / \"panda_manifest.csv\"\n",
    "    if manifest_path.exists():\n",
    "        df = pd.read_csv(manifest_path)\n",
    "    else:\n",
    "        # Create manifest\n",
    "        train_csv = PANDA_ROOT / \"train.csv\"\n",
    "        if not train_csv.exists():\n",
    "            print(\"ERROR: train.csv not found!\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(train_csv)\n",
    "        df['image_path'] = df['image_id'].apply(\n",
    "            lambda x: str(PANDA_ROOT / \"train_images\" / f\"{x}.tiff\")\n",
    "        )\n",
    "        df['image_exists'] = df['image_path'].apply(lambda x: Path(x).exists())\n",
    "        df.to_csv(manifest_path, index=False)\n",
    "    \n",
    "    print(f\"Total slides in dataset: {len(df)}\")\n",
    "    print(f\"Slides with images: {df['image_exists'].sum()}\")\n",
    "    \n",
    "    # Check already processed\n",
    "    already_done = sum(1 for _, row in df.iterrows() \n",
    "                      if row['image_exists'] and check_already_processed(row['image_id']))\n",
    "    print(f\"Already processed: {already_done}\")\n",
    "    \n",
    "    # Options\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXTRACTION OPTIONS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"1. Quick test (10 slides)\")\n",
    "    print(f\"2. Small batch (100 slides)\")\n",
    "    print(f\"3. Medium batch (1000 slides)\")\n",
    "    print(f\"4. Large batch (5000 slides)\")\n",
    "    print(f\"5. Full dataset (all {df['image_exists'].sum()} slides)\")\n",
    "    print(f\"6. Skip extraction\")\n",
    "    \n",
    "    choice = input(\"\\nChoice (1-6): \").strip()\n",
    "    \n",
    "    if choice == \"6\":\n",
    "        print(\"Skipping extraction\")\n",
    "        return\n",
    "    \n",
    "    max_slides_map = {\n",
    "        \"1\": 10,\n",
    "        \"2\": 100,\n",
    "        \"3\": 1000,\n",
    "        \"4\": 5000,\n",
    "        \"5\": None\n",
    "    }\n",
    "    max_slides = max_slides_map.get(choice, 100)\n",
    "    \n",
    "    # Run extraction\n",
    "    extract_features_parallel(df, max_slides)\n",
    "    \n",
    "    print(\"\\nDone! Features saved to:\")\n",
    "    print(f\"  {OUTPUT_DIRS['features_05']}\")\n",
    "    print(f\"  {OUTPUT_DIRS['features_20']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_optimized()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# metrics_from_oof.py\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# === EDIT THIS TO THE RUN YOU CARE ABOUT ===\n",
    "RESULTS_DIR = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\\results\\panda_mil_088\")\n",
    "OOF_CSV = next((p for p in [\n",
    "    RESULTS_DIR / \"oof_predictions.csv\",\n",
    "    RESULTS_DIR / \"oof.csv\"\n",
    "] if p.exists()), None)\n",
    "assert OOF_CSV and OOF_CSV.exists(), f\"Missing OOF file in {RESULTS_DIR}\"\n",
    "\n",
    "df = pd.read_csv(OOF_CSV)\n",
    "assert \"true_isup\" in df.columns, \"true_isup column not found\"\n",
    "\n",
    "y_true = df[\"true_isup\"].astype(int).values\n",
    "num_classes = int(max(y_true.max(), 5) + 1)  # expect 6 for PANDA\n",
    "\n",
    "# ---- get probabilities (prob_* or logit_* -> softmax) ----\n",
    "prob_cols = [c for c in df.columns if c.startswith(\"prob_\")]\n",
    "logit_cols = [c for c in df.columns if c.startswith(\"logit_\")]\n",
    "if prob_cols:\n",
    "    prob_cols = sorted(prob_cols, key=lambda c: int(c.split(\"_\")[-1]))\n",
    "    P = df[prob_cols].to_numpy(float)\n",
    "    # normalize (safety)\n",
    "    s = P.sum(axis=1, keepdims=True); s[s==0] = 1.0\n",
    "    P = P / s\n",
    "elif logit_cols:\n",
    "    logit_cols = sorted(logit_cols, key=lambda c: int(c.split(\"_\")[-1]))\n",
    "    Z = df[logit_cols].to_numpy(float)\n",
    "    Z = Z - Z.max(axis=1, keepdims=True)\n",
    "    P = np.exp(Z); P /= P.sum(axis=1, keepdims=True)\n",
    "else:\n",
    "    raise RuntimeError(\"Neither prob_* nor logit_* columns found in OOF file.\")\n",
    "\n",
    "assert P.shape[1] == num_classes, f\"Expected {num_classes} columns, got {P.shape[1]}\"\n",
    "\n",
    "def safe_ovr_macro_auroc(y, prob_mat):\n",
    "    try:\n",
    "        return float(roc_auc_score(y, prob_mat, multi_class=\"ovr\", average=\"macro\"))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def thresh_scores(y, P, thr):\n",
    "    y_bin = (y >= thr).astype(int)\n",
    "    s_bin = P[:, thr:].sum(axis=1)\n",
    "    return y_bin, s_bin\n",
    "\n",
    "def bin_metrics(y_bin, s_bin):\n",
    "    auroc = roc_auc_score(y_bin, s_bin)\n",
    "    aupr  = average_precision_score(y_bin, s_bin)\n",
    "    return float(auroc), float(aupr)\n",
    "\n",
    "metrics = {}\n",
    "metrics[\"macro_auroc_ovr\"] = safe_ovr_macro_auroc(y_true, P)\n",
    "\n",
    "thresh_list = [1,2,3,4,5]\n",
    "metrics[\"thresholds\"] = {}\n",
    "for t in thresh_list:\n",
    "    yb, sb = thresh_scores(y_true, P, t)\n",
    "    auroc, aupr = bin_metrics(yb, sb)\n",
    "    metrics[\"thresholds\"][f\">={t}\"] = {\"auroc\": auroc, \"auprc\": aupr, \"pos_rate\": float(yb.mean())}\n",
    "\n",
    "# per-provider (optional)\n",
    "prov_col = \"data_provider\" if \"data_provider\" in df.columns else None\n",
    "by_prov_rows = []\n",
    "if prov_col:\n",
    "    for prov, dsub in df.groupby(prov_col):\n",
    "        y_sub = dsub[\"true_isup\"].astype(int).values\n",
    "        if prob_cols:\n",
    "            P_sub = dsub[prob_cols].to_numpy(float)\n",
    "            s = P_sub.sum(axis=1, keepdims=True); s[s==0]=1.0\n",
    "            P_sub /= s\n",
    "        else:\n",
    "            Z = dsub[logit_cols].to_numpy(float)\n",
    "            Z = Z - Z.max(axis=1, keepdims=True)\n",
    "            P_sub = np.exp(Z); P_sub /= P_sub.sum(axis=1, keepdims=True)\n",
    "        row = {\"provider\": prov, \"macro_auroc_ovr\": safe_ovr_macro_auroc(y_sub, P_sub), \"n\": int(len(dsub))}\n",
    "        for t in thresh_list:\n",
    "            yb, sb = thresh_scores(y_sub, P_sub, t)\n",
    "            auroc, aupr = bin_metrics(yb, sb)\n",
    "            row[f\"AUROC_>={t}\"] = auroc\n",
    "            row[f\"AUPRC_>={t}\"] = aupr\n",
    "        by_prov_rows.append(row)\n",
    "\n",
    "# save\n",
    "(RESULTS_DIR / \"figures\").mkdir(exist_ok=True)\n",
    "with open(RESULTS_DIR / \"metrics_auc.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "if by_prov_rows:\n",
    "    pd.DataFrame(by_prov_rows).to_csv(RESULTS_DIR / \"metrics_auc_by_provider.csv\", index=False)\n",
    "\n",
    "# print\n",
    "print(\"=== PANDA AUROC/AUPRC (from OOF) ===\")\n",
    "print(f\"Run dir: {RESULTS_DIR}\")\n",
    "print(f\"Macro AUROC (OvR, {num_classes}-class): {metrics['macro_auroc_ovr']:.4f}\")\n",
    "print(\"\\nClinically meaningful thresholds (positive = ISUP ≥ t):\")\n",
    "for t in thresh_list:\n",
    "    m = metrics[\"thresholds\"][f'>={t}']\n",
    "    print(f\"  ISUP ≥{t}:  AUROC {m['auroc']:.4f} | AUPRC {m['auprc']:.4f} | prevalence {m['pos_rate']*100:.1f}%\")\n",
    "if by_prov_rows:\n",
    "    print(\"\\nPer-provider:\")\n",
    "    for row in by_prov_rows:\n",
    "        extras = \" | \".join([f\"≥{t}:{row[f'AUROC_>={t}']:.3f}\" for t in thresh_list])\n",
    "        print(f\"  {row['provider']:10s} | n={row['n']:4d} | Macro AUROC {row['macro_auroc_ovr']:.4f} | {extras}\")\n",
    "\n",
    "print(f\"\\nSaved: {RESULTS_DIR/'metrics_auc.json'}\")\n",
    "if by_prov_rows:\n",
    "    print(f\"Saved: {RESULTS_DIR/'metrics_auc_by_provider.csv'}\")\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Complete UNI2-h Benchmarking & Fair Comparison\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, precision_recall_curve, auc, brier_score_loss\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Repro / quiet\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# CONFIG\n",
    "# ======================\n",
    "@dataclass\n",
    "class Config:\n",
    "    BASE_DIR: Path = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\\UNI features\")\n",
    "    OUTPUT_DIR: Path = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\\UNI features\\results\")\n",
    "    TCGA_TYPES: Tuple[str, ...] = (\n",
    "        'TCGA-ACC', 'TCGA-BRCA_IDC', 'TCGA-COAD', 'TCGA-DLBC', 'TCGA-GBM',\n",
    "        'TCGA-HNSC', 'TCGA-KIRC', 'TCGA-LUAD', 'TCGA-SKCM', 'TCGA-UCEC'\n",
    "    )\n",
    "    AGGREGATION: str = \"mean\"     # 'mean' | 'max' | 'attention'\n",
    "    N_FOLDS: int = 5\n",
    "    RANDOM_STATE: int = 42\n",
    "    GROUP_BY_SITE: bool = True     # use TSS grouping if available for SGKF\n",
    "    MODEL_TYPE: str = \"logistic\"   # 'logistic' | 'rf'\n",
    "    N_BOOTSTRAP: int = 2000        # for 95% CI on macro metrics\n",
    "\n",
    "\n",
    "CFG = Config()\n",
    "CFG.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Utils\n",
    "# ======================\n",
    "def set_global_seed(seed: int = 42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def now() -> str:\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def save_json(obj: dict, path: Path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# H5 Loading & Aggregation\n",
    "# ======================\n",
    "def _try_get(f, keys: Tuple[str, ...]) -> np.ndarray:\n",
    "    for k in keys:\n",
    "        if k in f:\n",
    "            return f[k][:]\n",
    "    raise KeyError(f\"None of keys {keys} found in file\")\n",
    "\n",
    "def load_slide_features(h5_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Flexible UNI2-h H5 loader. Returns (features, coords or None)\n",
    "    features: (num_patches, D)\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        features = _try_get(f, ('features', 'feats', 'patch_features', 'x'))\n",
    "        if features.ndim == 3 and features.shape[0] == 1:\n",
    "            features = features.squeeze(0)\n",
    "        elif features.ndim == 1:\n",
    "            features = features.reshape(1, -1)\n",
    "        coords = None\n",
    "        for ck in ('coords', 'xy', 'positions'):\n",
    "            if ck in f:\n",
    "                coords = f[ck][:]\n",
    "                if coords.ndim == 3 and coords.shape[0] == 1:\n",
    "                    coords = coords.squeeze(0)\n",
    "                break\n",
    "    if features.size == 0:\n",
    "        raise ValueError(f\"Empty features in {h5_path.name}\")\n",
    "    return features, coords\n",
    "\n",
    "def aggregate_slide_features(features: np.ndarray, method: str = 'mean') -> np.ndarray:\n",
    "    if method == 'mean':\n",
    "        return features.mean(axis=0)\n",
    "    elif method == 'max':\n",
    "        return features.max(axis=0)\n",
    "    elif method == 'attention':\n",
    "        norms = np.linalg.norm(features, axis=1, keepdims=True)\n",
    "        weights = norms / (norms.sum() + 1e-8)\n",
    "        return (features * weights).sum(axis=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation: {method}\")\n",
    "\n",
    "def load_cancer_type_features(cancer_dir: Path, aggregation: str = 'mean') -> pd.DataFrame:\n",
    "    h5_files = sorted(list(cancer_dir.glob(\"*.h5\")))\n",
    "    print(f\"  Loading {len(h5_files)} slides from: {cancer_dir.name}\")\n",
    "    rows = []\n",
    "    for fp in tqdm(h5_files, desc=\"   reading h5\", leave=False):\n",
    "        try:\n",
    "            feats, _ = load_slide_features(fp)\n",
    "            slide_vec = aggregate_slide_features(feats, aggregation)\n",
    "            rows.append({\n",
    "                \"slide_id\": fp.stem,\n",
    "                \"num_patches\": feats.shape[0],\n",
    "                \"features\": slide_vec\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠ {fp.name}: {e}\")\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(rows)\n",
    "    feats_arr = np.vstack(df[\"features\"].values)\n",
    "    feat_cols = [f\"f{i:04d}\" for i in range(feats_arr.shape[1])]\n",
    "    feats_df = pd.DataFrame(feats_arr, columns=feat_cols)\n",
    "    out = pd.concat([df[[\"slide_id\", \"num_patches\"]].reset_index(drop=True), feats_df], axis=1)\n",
    "    return out\n",
    "\n",
    "def load_all_tcga_features(base_dir: Path, tcga_types: Tuple[str, ...], aggregation: str) -> pd.DataFrame:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING UNI2-h FEATURES FOR TCGA\")\n",
    "    print(\"=\"*70)\n",
    "    all_dfs, loaded = [], []\n",
    "    for ct in tcga_types:\n",
    "        p = base_dir / ct\n",
    "        if not p.exists():\n",
    "            print(f\"❌ {ct}: missing → skip\")\n",
    "            continue\n",
    "        print(f\"\\n📊 {ct}\")\n",
    "        df = load_cancer_type_features(p, aggregation)\n",
    "        if df.empty:\n",
    "            print(\"  ⚠ No slides → skip\")\n",
    "            continue\n",
    "        df[\"cancer_type\"] = ct\n",
    "        loaded.append(ct)\n",
    "        all_dfs.append(df)\n",
    "        print(f\"  ✓ {len(df)} slides\")\n",
    "    if not all_dfs:\n",
    "        raise RuntimeError(\"No TCGA cohorts loaded\")\n",
    "    all_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TOTAL: {len(all_df)} slides from {len(loaded)} cohorts\")\n",
    "    print(f\"Loaded: {', '.join(loaded)}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    return all_df\n",
    "\n",
    "def load_panda_features(base_dir: Path, aggregation: str) -> pd.DataFrame:\n",
    "    p = base_dir / \"panda\"\n",
    "    if not p.exists():\n",
    "        print(\"❌ PANDA dir not found; skipping PANDA\")\n",
    "        return pd.DataFrame()\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING PANDA FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    df = load_cancer_type_features(p, aggregation)\n",
    "    if df.empty:\n",
    "        print(\"⚠ PANDA empty\")\n",
    "        return df\n",
    "    df[\"dataset\"] = \"PANDA\"\n",
    "    print(f\"✓ {len(df)} PANDA slides\\n\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Grouping (TCGA site)\n",
    "# ======================\n",
    "def extract_tcga_tss(slide_id: str) -> str:\n",
    "    # TCGA-XX-YYYY-... → 'XX' is tissue source site\n",
    "    try:\n",
    "        parts = slide_id.split('-')\n",
    "        return parts[1] if len(parts) > 1 else 'NA'\n",
    "    except Exception:\n",
    "        return 'NA'\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Modeling\n",
    "# ======================\n",
    "def get_model(model_type: str = \"logistic\"):\n",
    "    if model_type == \"logistic\":\n",
    "        return LogisticRegression(\n",
    "            max_iter=2000, random_state=CFG.RANDOM_STATE,\n",
    "            class_weight='balanced', multi_class='multinomial', solver='lbfgs'\n",
    "        )\n",
    "    elif model_type == \"rf\":\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=400, random_state=CFG.RANDOM_STATE,\n",
    "            class_weight='balanced', n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'logistic' or 'rf'\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CVArtifacts:\n",
    "    fold_metrics: pd.DataFrame\n",
    "    per_class_metrics: pd.DataFrame\n",
    "    y_true_all: np.ndarray\n",
    "    y_prob_all: np.ndarray\n",
    "    y_pred_all: np.ndarray\n",
    "    labels: List[str]\n",
    "    conf_mat: np.ndarray\n",
    "\n",
    "\n",
    "def stratified_group_kfold(n_splits=5, shuffle=True, random_state=42):\n",
    "    \"\"\"\n",
    "    Returns a splitter; prefers StratifiedGroupKFold if available,\n",
    "    else falls back to GroupKFold or StratifiedKFold (no groups).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sklearn.model_selection import StratifiedGroupKFold\n",
    "        return StratifiedGroupKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state), \"SGKF\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            from sklearn.model_selection import GroupKFold\n",
    "            return GroupKFold(n_splits=n_splits), \"GK\"\n",
    "        except Exception:\n",
    "            return StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state), \"SKF\"\n",
    "\n",
    "\n",
    "def cross_validate_multiclass(df: pd.DataFrame, label_col=\"cancer_type\") -> CVArtifacts:\n",
    "    feat_cols = [c for c in df.columns if c.startswith(\"f\")]\n",
    "    X = df[feat_cols].values\n",
    "    labels = df[label_col].values\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(labels)\n",
    "    classes = list(le.classes_)\n",
    "\n",
    "    # groups (by TSS) if enabled\n",
    "    groups = np.array([extract_tcga_tss(s) for s in df[\"slide_id\"].values]) if CFG.GROUP_BY_SITE else None\n",
    "\n",
    "    splitter, mode = stratified_group_kfold(CFG.N_FOLDS, True, CFG.RANDOM_STATE)\n",
    "    print(f\"CV splitter: {mode} | folds={CFG.N_FOLDS} | group_by_site={CFG.GROUP_BY_SITE}\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    fold_rows = []\n",
    "    per_class_rows = []\n",
    "\n",
    "    # Store OOF predictions\n",
    "    y_true_all = np.zeros(X.shape[0], dtype=int) - 1\n",
    "    y_pred_all = np.zeros(X.shape[0], dtype=int) - 1\n",
    "    y_prob_all = np.zeros((X.shape[0], len(classes)), dtype=float)\n",
    "\n",
    "    split_iter = splitter.split(X, y, groups=groups) if groups is not None and mode in (\"SGKF\", \"GK\") else splitter.split(X, y)\n",
    "    for fold, (tr, te) in enumerate(split_iter, 1):\n",
    "        X_tr, X_te = X[tr], X[te]\n",
    "        y_tr, y_te = y[tr], y[te]\n",
    "\n",
    "        X_tr = scaler.fit_transform(X_tr)\n",
    "        X_te = scaler.transform(X_te)\n",
    "\n",
    "        clf = get_model(CFG.MODEL_TYPE)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "\n",
    "        y_pred = clf.predict(X_te)\n",
    "        y_proba = clf.predict_proba(X_te)\n",
    "\n",
    "        # fold metrics\n",
    "        fold_metrics = {\n",
    "            \"fold\": fold,\n",
    "            \"accuracy\": accuracy_score(y_te, y_pred),\n",
    "            \"f1_macro\": f1_score(y_te, y_pred, average=\"macro\"),\n",
    "            \"f1_weighted\": f1_score(y_te, y_pred, average=\"weighted\"),\n",
    "            \"precision_macro\": precision_score(y_te, y_pred, average=\"macro\", zero_division=0),\n",
    "            \"recall_macro\": recall_score(y_te, y_pred, average=\"macro\", zero_division=0),\n",
    "        }\n",
    "        try:\n",
    "            auc_macro = roc_auc_score(y_te, y_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "            auc_weighted = roc_auc_score(y_te, y_proba, multi_class=\"ovr\", average=\"weighted\")\n",
    "        except Exception:\n",
    "            auc_macro, auc_weighted = np.nan, np.nan\n",
    "        fold_metrics[\"auc_macro\"] = auc_macro\n",
    "        fold_metrics[\"auc_weighted\"] = auc_weighted\n",
    "        fold_rows.append(fold_metrics)\n",
    "\n",
    "        # per-class AUC OvR + F1\n",
    "        y_te_bin = label_binarize(y_te, classes=np.arange(len(classes)))\n",
    "        # per-class AUC\n",
    "        per_class_auc = []\n",
    "        for k in range(len(classes)):\n",
    "            try:\n",
    "                auc_k = roc_auc_score(y_te_bin[:, k], y_proba[:, k])\n",
    "            except Exception:\n",
    "                auc_k = np.nan\n",
    "            per_class_auc.append(auc_k)\n",
    "            f1_k = f1_score((y_te == k).astype(int), (y_pred == k).astype(int), zero_division=0)\n",
    "            per_class_rows.append({\"fold\": fold, \"class\": classes[k], \"auc_ovr\": auc_k, \"f1\": f1_k})\n",
    "\n",
    "        # store OOF\n",
    "        y_true_all[te] = y_te\n",
    "        y_pred_all[te] = y_pred\n",
    "        y_prob_all[te, :] = y_proba\n",
    "\n",
    "        print(f\"  Fold {fold}/{CFG.N_FOLDS} | acc={fold_metrics['accuracy']:.4f} | f1m={fold_metrics['f1_macro']:.4f} | aucm={fold_metrics['auc_macro']:.4f}\")\n",
    "\n",
    "    fold_df = pd.DataFrame(fold_rows)\n",
    "    per_class_df = pd.DataFrame(per_class_rows)\n",
    "\n",
    "    # sanity\n",
    "    assert (y_true_all >= 0).all(), \"OOF y_true not fully assigned.\"\n",
    "    assert (y_pred_all >= 0).all(), \"OOF y_pred not fully assigned.\"\n",
    "\n",
    "    # confusion matrix on OOF\n",
    "    cm = confusion_matrix(y_true_all, y_pred_all, labels=np.arange(len(classes)))\n",
    "\n",
    "    return CVArtifacts(\n",
    "        fold_metrics=fold_df,\n",
    "        per_class_metrics=per_class_df,\n",
    "        y_true_all=y_true_all,\n",
    "        y_prob_all=y_prob_all,\n",
    "        y_pred_all=y_pred_all,\n",
    "        labels=classes,\n",
    "        conf_mat=cm\n",
    "    )\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Statistics: Bootstrap CIs & ECE\n",
    "# ======================\n",
    "def bootstrap_ci_macro(y_true: np.ndarray, y_prob: np.ndarray, y_pred: np.ndarray, n_boot: int = 2000, seed: int = 42) -> Dict[str, Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Bootstrap 95% CI for macro metrics from OOF predictions.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    n = y_true.shape[0]\n",
    "    classes = np.unique(y_true)\n",
    "    metrics = {\"accuracy\": [], \"f1_macro\": [], \"auc_macro\": []}\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.randint(0, n, size=n)  # sample with replacement\n",
    "        yt = y_true[idx]\n",
    "        yp = y_pred[idx]\n",
    "        ypb = y_prob[idx]\n",
    "\n",
    "        metrics[\"accuracy\"].append(accuracy_score(yt, yp))\n",
    "        metrics[\"f1_macro\"].append(f1_score(yt, yp, average=\"macro\"))\n",
    "        try:\n",
    "            metrics[\"auc_macro\"].append(roc_auc_score(yt, ypb, multi_class=\"ovr\", average=\"macro\"))\n",
    "        except Exception:\n",
    "            metrics[\"auc_macro\"].append(np.nan)\n",
    "\n",
    "    out = {}\n",
    "    for k, vals in metrics.items():\n",
    "        arr = np.array(vals, dtype=float)\n",
    "        lo, hi = np.nanpercentile(arr, [2.5, 97.5])\n",
    "        out[k] = (float(lo), float(hi))\n",
    "    return out\n",
    "\n",
    "def expected_calibration_error(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 15) -> Tuple[float, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Multiclass ECE using max-probability approach.\n",
    "    \"\"\"\n",
    "    max_conf = y_prob.max(axis=1)\n",
    "    y_pred = y_prob.argmax(axis=1)\n",
    "    correct = (y_pred == y_true).astype(float)\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    rows = []\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        sel = (max_conf >= lo) & (max_conf < hi) if i < n_bins - 1 else (max_conf >= lo) & (max_conf <= hi)\n",
    "        if sel.sum() == 0:\n",
    "            rows.append({\"bin\": i+1, \"conf\": 0.0, \"acc\": 0.0, \"count\": 0})\n",
    "            continue\n",
    "        bin_conf = max_conf[sel].mean()\n",
    "        bin_acc = correct[sel].mean()\n",
    "        rows.append({\"bin\": i+1, \"conf\": float(bin_conf), \"acc\": float(bin_acc), \"count\": int(sel.sum())})\n",
    "        ece += (sel.sum() / len(y_true)) * abs(bin_acc - bin_conf)\n",
    "    return float(ece), pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Plots (matplotlib only)\n",
    "# ======================\n",
    "def save_boxplots(cv_df: pd.DataFrame, out: Path, title_prefix=\"UNI\"):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    metrics = [(\"accuracy\", \"Accuracy\"), (\"f1_macro\", \"F1 (Macro)\"), (\"auc_macro\", \"AUC (Macro)\")]\n",
    "    for ax, (key, name) in zip(axes, metrics):\n",
    "        ax.boxplot(cv_df[key].values, showmeans=True)\n",
    "        ax.set_title(f\"{title_prefix}: {name}\")\n",
    "        ax.set_xticks([1]); ax.set_xticklabels([name])\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylabel(name)\n",
    "    plt.tight_layout()\n",
    "    png = out / f\"{title_prefix.lower()}_cv_boxplots.png\"\n",
    "    pdf = out / f\"{title_prefix.lower()}_cv_boxplots.pdf\"\n",
    "    plt.savefig(png, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(pdf, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Saved {png}\")\n",
    "\n",
    "def save_per_class_auc(per_class_df: pd.DataFrame, out: Path, title_prefix=\"UNI\"):\n",
    "    # mean across folds\n",
    "    g = per_class_df.groupby(\"class\", as_index=False)[\"auc_ovr\"].mean().sort_values(\"auc_ovr\", ascending=True)\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.barh(g[\"class\"], g[\"auc_ovr\"])\n",
    "    ax.set_xlabel(\"AUC (OvR, mean across folds)\")\n",
    "    ax.set_title(f\"{title_prefix}: Per-class AUC (OvR)\")\n",
    "    ax.grid(True, axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    png = out / f\"{title_prefix.lower()}_per_class_auc.png\"\n",
    "    pdf = out / f\"{title_prefix.lower()}_per_class_auc.pdf\"\n",
    "    plt.savefig(png, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(pdf, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Saved {png}\")\n",
    "\n",
    "def save_confusion(cm: np.ndarray, labels: List[str], out: Path, title_prefix=\"UNI\"):\n",
    "    # normalize by true class\n",
    "    cmn = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cmn, aspect='auto', interpolation='nearest')\n",
    "    ax.set_title(f\"{title_prefix}: Normalized Confusion Matrix (OOF)\")\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "    ax.set_xticks(range(len(labels))); ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=90); ax.set_yticklabels(labels)\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    png = out / f\"{title_prefix.lower()}_confusion_matrix.png\"\n",
    "    pdf = out / f\"{title_prefix.lower()}_confusion_matrix.pdf\"\n",
    "    plt.savefig(png, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(pdf, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Saved {png}\")\n",
    "\n",
    "def save_reliability_plot(y_true: np.ndarray, y_prob: np.ndarray, out: Path, title_prefix=\"UNI\"):\n",
    "    ece, bins_df = expected_calibration_error(y_true, y_prob, n_bins=15)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.plot([0,1],[0,1], linestyle='--')\n",
    "    ax.plot(bins_df[\"conf\"], bins_df[\"acc\"], marker='o')\n",
    "    ax.set_xlim(0,1); ax.set_ylim(0,1)\n",
    "    ax.set_xlabel(\"Confidence\"); ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(f\"{title_prefix}: Reliability Diagram (ECE={ece:.3f})\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    png = out / f\"{title_prefix.lower()}_reliability.png\"\n",
    "    pdf = out / f\"{title_prefix.lower()}_reliability.pdf\"\n",
    "    plt.savefig(png, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(pdf, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Saved {png}\")\n",
    "\n",
    "def save_pr_curves(y_true: np.ndarray, y_prob: np.ndarray, class_names: List[str], out: Path, title_prefix=\"UNI\"):\n",
    "    y_bin = label_binarize(y_true, classes=np.arange(len(class_names)))\n",
    "    cols = 2\n",
    "    rows = math.ceil(len(class_names)/cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(8*cols, 4.5*rows))\n",
    "    axes = np.array(axes).reshape(rows, cols)\n",
    "    for k, cname in enumerate(class_names):\n",
    "        r, c = divmod(k, cols)\n",
    "        ax = axes[r, c]\n",
    "        pr, rc, _ = precision_recall_curve(y_bin[:, k], y_prob[:, k])\n",
    "        aupr = auc(rc, pr)\n",
    "        ax.plot(rc, pr)\n",
    "        ax.set_title(f\"{cname} (AUPR={aupr:.3f})\")\n",
    "        ax.set_xlabel(\"Recall\"); ax.set_ylabel(\"Precision\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    # hide empty\n",
    "    for idx in range(len(class_names), rows*cols):\n",
    "        r, c = divmod(idx, cols)\n",
    "        axes[r, c].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    png = out / f\"{title_prefix.lower()}_pr_curves.png\"\n",
    "    pdf = out / f\"{title_prefix.lower()}_pr_curves.pdf\"\n",
    "    plt.savefig(png, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(pdf, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Saved {png}\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# FAIR COMPARISON UNI vs YOUR MODEL\n",
    "# ======================\n",
    "def build_common_matrices(uni_df: pd.DataFrame, your_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        X_uni, X_your, y (encoded), groups, class_names\n",
    "        (slides restricted to intersection by slide_id and cancer_type)\n",
    "    \"\"\"\n",
    "    fcols_u = [c for c in uni_df.columns if c.startswith(\"f\")]\n",
    "    fcols_y = [c for c in your_df.columns if c.startswith(\"f\")]\n",
    "    assert fcols_u, \"UNI df missing feature columns f*\"\n",
    "    assert fcols_y, \"YOUR df missing feature columns f*\"\n",
    "\n",
    "    u = uni_df[[\"slide_id\", \"cancer_type\"] + fcols_u].copy()\n",
    "    y = your_df[[\"slide_id\", \"cancer_type\"] + fcols_y].copy()\n",
    "    merged = u.merge(y, on=[\"slide_id\", \"cancer_type\"], suffixes=(\"_uni\", \"_your\"))\n",
    "    if merged.empty:\n",
    "        raise RuntimeError(\"No overlapping slides with matching cancer_type between UNI and YOUR model.\")\n",
    "\n",
    "    X_uni = merged[[c for c in merged.columns if c.startswith(\"f\") and c.endswith(\"_uni\")]].values\n",
    "    X_your = merged[[c for c in merged.columns if c.startswith(\"f\") and c.endswith(\"_your\")]].values\n",
    "    y_lbl = merged[\"cancer_type\"].values\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y_lbl)\n",
    "    class_names = list(le.classes_)\n",
    "\n",
    "    # groups by TSS\n",
    "    groups = np.array([extract_tcga_tss(s) for s in merged[\"slide_id\"].values])\n",
    "    return X_uni, X_your, y_enc, groups, class_names\n",
    "\n",
    "def fair_compare_same_folds(X_uni: np.ndarray, X_your: np.ndarray, y: np.ndarray, groups: np.ndarray, class_names: List[str]) -> pd.DataFrame:\n",
    "    splitter, mode = stratified_group_kfold(CFG.N_FOLDS, True, CFG.RANDOM_STATE)\n",
    "    print(f\"Fair compare splitter: {mode}\")\n",
    "\n",
    "    split_iter = splitter.split(X_uni, y, groups=groups) if groups is not None and mode in (\"SGKF\", \"GK\") else splitter.split(X_uni, y)\n",
    "\n",
    "    rows_uni, rows_your = [], []\n",
    "    for fold, (tr, te) in enumerate(split_iter, 1):\n",
    "        # UNI\n",
    "        sc_u = StandardScaler()\n",
    "        Xtr_u, Xte_u = sc_u.fit_transform(X_uni[tr]), sc_u.transform(X_uni[te])\n",
    "        clf_u = get_model(CFG.MODEL_TYPE)\n",
    "        clf_u.fit(Xtr_u, y[tr])\n",
    "        ypr_u = clf_u.predict(Xte_u); ypb_u = clf_u.predict_proba(Xte_u)\n",
    "\n",
    "        # YOUR\n",
    "        sc_y = StandardScaler()\n",
    "        Xtr_y, Xte_y = sc_y.fit_transform(X_your[tr]), sc_y.transform(X_your[te])\n",
    "        clf_y = get_model(CFG.MODEL_TYPE)\n",
    "        clf_y.fit(Xtr_y, y[tr])\n",
    "        ypr_y = clf_y.predict(Xte_y); ypb_y = clf_y.predict_proba(Xte_y)\n",
    "\n",
    "        def fold_stats(ytrue, ypred, yprob):\n",
    "            d = {\n",
    "                \"accuracy\": accuracy_score(ytrue, ypred),\n",
    "                \"f1_macro\": f1_score(ytrue, ypred, average=\"macro\"),\n",
    "            }\n",
    "            try:\n",
    "                d[\"auc_macro\"] = roc_auc_score(ytrue, yprob, multi_class=\"ovr\", average=\"macro\")\n",
    "            except Exception:\n",
    "                d[\"auc_macro\"] = np.nan\n",
    "            return d\n",
    "\n",
    "        ru = fold_stats(y[te], ypr_u, ypb_u); ru[\"fold\"] = fold\n",
    "        ry = fold_stats(y[te], ypr_y, ypb_y); ry[\"fold\"] = fold\n",
    "        rows_uni.append(ru); rows_your.append(ry)\n",
    "\n",
    "        print(f\"  Fold {fold}: UNI acc={ru['accuracy']:.4f} | YOUR acc={ry['accuracy']:.4f}\")\n",
    "\n",
    "    uni_cv = pd.DataFrame(rows_uni); your_cv = pd.DataFrame(rows_your)\n",
    "    cmp = pd.DataFrame({\n",
    "        \"Metric\": [\"Accuracy\", \"F1 (macro)\", \"AUC (macro)\"],\n",
    "        \"UNI (mean±std)\": [\n",
    "            f\"{uni_cv['accuracy'].mean():.4f}±{uni_cv['accuracy'].std():.4f}\",\n",
    "            f\"{uni_cv['f1_macro'].mean():.4f}±{uni_cv['f1_macro'].std():.4f}\",\n",
    "            f\"{uni_cv['auc_macro'].mean():.4f}±{uni_cv['auc_macro'].std():.4f}\"\n",
    "        ],\n",
    "        \"Yours (mean±std)\": [\n",
    "            f\"{your_cv['accuracy'].mean():.4f}±{your_cv['accuracy'].std():.4f}\",\n",
    "            f\"{your_cv['f1_macro'].mean():.4f}±{your_cv['f1_macro'].std():.4f}\",\n",
    "            f\"{your_cv['auc_macro'].mean():.4f}±{your_cv['auc_macro'].std():.4f}\"\n",
    "        ],\n",
    "        \"Δ (UNI − Yours)\": [\n",
    "            f\"{(uni_cv['accuracy'] - your_cv['accuracy']).mean():+.4f}\",\n",
    "            f\"{(uni_cv['f1_macro'] - your_cv['f1_macro']).mean():+.4f}\",\n",
    "            f\"{(uni_cv['auc_macro'] - your_cv['auc_macro']).mean():+.4f}\",\n",
    "        ]\n",
    "    })\n",
    "    return cmp, uni_cv, your_cv\n",
    "\n",
    "\n",
    "# ======================\n",
    "# IO helpers for your feature table\n",
    "# ======================\n",
    "def save_df(df: pd.DataFrame, path: Path):\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        df.to_csv(path, index=False)\n",
    "    else:\n",
    "        df.to_parquet(path, index=False)\n",
    "\n",
    "def load_your_features_table(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        df = pd.read_parquet(path)\n",
    "    need = {\"slide_id\", \"cancer_type\"}\n",
    "    assert need.issubset(set(df.columns)), f\"your features must include {need}\"\n",
    "    fcols = [c for c in df.columns if c.startswith(\"f\")]\n",
    "    assert fcols, \"No feature columns starting with 'f'\"\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================\n",
    "# MAIN\n",
    "# ======================\n",
    "def main():\n",
    "    set_global_seed(CFG.RANDOM_STATE)\n",
    "    start = time.time()\n",
    "    print(f\"[{now()}] START\")\n",
    "\n",
    "    # --- Load UNI features\n",
    "    uni_df = load_all_tcga_features(CFG.BASE_DIR, CFG.TCGA_TYPES, CFG.AGGREGATION)\n",
    "    uni_parquet = CFG.OUTPUT_DIR / \"uni_features_all_tcga.parquet\"\n",
    "    uni_df.to_parquet(uni_parquet, index=False)\n",
    "    print(f\"✓ Saved {uni_parquet}\")\n",
    "\n",
    "    # --- Cross-validate UNI\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CROSS-VALIDATION: UNI\")\n",
    "    print(\"=\"*70)\n",
    "    uni_art = cross_validate_multiclass(uni_df, label_col=\"cancer_type\")\n",
    "\n",
    "    # Save artifacts\n",
    "    save_df(uni_art.fold_metrics, CFG.OUTPUT_DIR / \"uni_cv_folds.csv\")\n",
    "    save_df(uni_art.per_class_metrics, CFG.OUTPUT_DIR / \"uni_cv_per_class.csv\")\n",
    "\n",
    "    # Bootstrap CIs on OOF\n",
    "    uni_ci = bootstrap_ci_macro(\n",
    "        uni_art.y_true_all, uni_art.y_prob_all, uni_art.y_pred_all,\n",
    "        n_boot=CFG.N_BOOTSTRAP, seed=CFG.RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # Summary\n",
    "    uni_summary = {\n",
    "        \"n_slides\": int(len(uni_df)),\n",
    "        \"n_classes\": int(len(uni_art.labels)),\n",
    "        \"feature_dim\": int(sum(c.startswith(\"f\") for c in uni_df.columns)),\n",
    "        \"aggregation\": CFG.AGGREGATION,\n",
    "        \"model\": CFG.MODEL_TYPE,\n",
    "        \"cv_folds\": CFG.N_FOLDS,\n",
    "        \"group_by_site\": CFG.GROUP_BY_SITE,\n",
    "        \"metrics_mean\": {\n",
    "            \"accuracy\": float(uni_art.fold_metrics[\"accuracy\"].mean()),\n",
    "            \"f1_macro\": float(uni_art.fold_metrics[\"f1_macro\"].mean()),\n",
    "            \"auc_macro\": float(uni_art.fold_metrics[\"auc_macro\"].mean())\n",
    "        },\n",
    "        \"metrics_std\": {\n",
    "            \"accuracy\": float(uni_art.fold_metrics[\"accuracy\"].std()),\n",
    "            \"f1_macro\": float(uni_art.fold_metrics[\"f1_macro\"].std()),\n",
    "            \"auc_macro\": float(uni_art.fold_metrics[\"auc_macro\"].std())\n",
    "        },\n",
    "        \"metrics_ci95\": uni_ci,\n",
    "        \"ece\": float(expected_calibration_error(uni_art.y_true_all, uni_art.y_prob_all)[0]),\n",
    "        \"versions\": {\n",
    "            \"python\": sys.version.split()[0],\n",
    "            \"numpy\": np.__version__,\n",
    "            \"pandas\": pd.__version__,\n",
    "            \"sklearn\": __import__(\"sklearn\").__version__,\n",
    "            \"matplotlib\": matplotlib.__version__\n",
    "        },\n",
    "        \"seed\": CFG.RANDOM_STATE\n",
    "    }\n",
    "    save_json(uni_summary, CFG.OUTPUT_DIR / \"uni_summary.json\")\n",
    "    print(\"✓ Saved uni_summary.json\")\n",
    "\n",
    "    # --- Plots for UNI\n",
    "    save_boxplots(uni_art.fold_metrics, CFG.OUTPUT_DIR, \"UNI\")\n",
    "    save_per_class_auc(uni_art.per_class_metrics, CFG.OUTPUT_DIR, \"UNI\")\n",
    "    save_confusion(uni_art.conf_mat, uni_art.labels, CFG.OUTPUT_DIR, \"UNI\")\n",
    "    save_reliability_plot(uni_art.y_true_all, uni_art.y_prob_all, CFG.OUTPUT_DIR, \"UNI\")\n",
    "    save_pr_curves(uni_art.y_true_all, uni_art.y_prob_all, uni_art.labels, CFG.OUTPUT_DIR, \"UNI\")\n",
    "\n",
    "    # --- FAIR COMPARISON (if your features available)\n",
    "    # Put your features parquet/csv path here (must include slide_id, cancer_type, and f0000... columns)\n",
    "    YOUR_FEATURES_PATH = CFG.OUTPUT_DIR / \"your_features_all_tcga.parquet\"  # <-- set this to your actual file\n",
    "    if YOUR_FEATURES_PATH.exists():\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FAIR COMPARISON: UNI vs YOUR MODEL (same slides, same folds)\")\n",
    "        print(\"=\"*70)\n",
    "        your_df = load_your_features_table(YOUR_FEATURES_PATH)\n",
    "\n",
    "        X_u, X_y, y_enc, groups, class_names = build_common_matrices(uni_df, your_df)\n",
    "        cmp_table, uni_cv_fair, your_cv_fair = fair_compare_same_folds(X_u, X_y, y_enc, groups, class_names)\n",
    "\n",
    "        save_df(uni_cv_fair, CFG.OUTPUT_DIR / \"uni_cv_fair.csv\")\n",
    "        save_df(your_cv_fair, CFG.OUTPUT_DIR / \"your_cv_fair.csv\")\n",
    "        save_df(cmp_table, CFG.OUTPUT_DIR / \"comparison_fair.csv\")\n",
    "        print(\"\\n\" + cmp_table.to_string(index=False))\n",
    "\n",
    "        # Simple fairness plot (boxplot pairs)\n",
    "        def plot_pair_box(uni_series, your_series, metric_name, outstem):\n",
    "            fig, ax = plt.subplots(figsize=(5,5))\n",
    "            ax.boxplot([uni_series.values, your_series.values], showmeans=True)\n",
    "            ax.set_xticks([1,2]); ax.set_xticklabels([\"UNI\",\"Yours\"])\n",
    "            ax.set_ylabel(metric_name); ax.grid(True, alpha=0.3)\n",
    "            ax.set_title(f\"Fair CV: {metric_name}\")\n",
    "            png = CFG.OUTPUT_DIR / f\"{outstem}.png\"\n",
    "            pdf = CFG.OUTPUT_DIR / f\"{outstem}.pdf\"\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(png, dpi=300, bbox_inches='tight')\n",
    "            plt.savefig(pdf, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"✓ Saved {png}\")\n",
    "\n",
    "        plot_pair_box(uni_cv_fair[\"accuracy\"], your_cv_fair[\"accuracy\"], \"Accuracy\", \"fair_accuracy\")\n",
    "        plot_pair_box(uni_cv_fair[\"f1_macro\"], your_cv_fair[\"f1_macro\"], \"F1 (Macro)\", \"fair_f1_macro\")\n",
    "        plot_pair_box(uni_cv_fair[\"auc_macro\"], your_cv_fair[\"auc_macro\"], \"AUC (Macro)\", \"fair_auc_macro\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ Skipping fair comparison: file not found → {YOUR_FEATURES_PATH}\")\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"\\n[{now()}] DONE in {elapsed/60:.1f} min\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================== cap BLAS threads BEFORE any scientific imports ==================\n",
    "import os\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"JOBLIB_START_METHOD\", \"spawn\")\n",
    "# ====================================================================================\n",
    "\n",
    "import sys, json, time, threading, pickle, hashlib, warnings, re, glob, shutil\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================================== CONFIG ==================================\n",
    "@dataclass\n",
    "class CFG:\n",
    "    WORKSPACE: Path = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\")\n",
    "    F05: Path = WORKSPACE / \"features\" / \"scale0p5\"\n",
    "    F20: Path = WORKSPACE / \"features\" / \"scale2p0\"\n",
    "    OUT: Path = WORKSPACE / \"results\" / \"ablations_complete\"\n",
    "    DATASET: str = \"tcga\"\n",
    "\n",
    "    N_FOLDS: int = 5\n",
    "    N_REPEATS: int = 3\n",
    "    SEED: int = 42\n",
    "\n",
    "    # speed knobs (keep 0 to match prior results; set to 256 for faster runs)\n",
    "    PCA_DIM: int = 0\n",
    "    CV_JOBS: int = max(2, (os.cpu_count() or 8) - 1)\n",
    "    HEARTBEAT_SEC: int = 45\n",
    "\n",
    "    # behavior\n",
    "    USE_EXISTING_SPLITS_IF_FOUND: bool = True   # match prior ablations\n",
    "    REBUILD_IF_MISSING: bool = True\n",
    "    CLEAR_OLD_CLASSIFIER_ARTIFACTS: bool = True # nuke stale classifier CSV/ckpts\n",
    "    OVERWRITE_CSV: bool = True                  # always write fresh classifier CSV\n",
    "\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg = CFG()\n",
    "np.random.seed(cfg.SEED)\n",
    "torch.manual_seed(cfg.SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(cfg.SEED)\n",
    "\n",
    "# ================================== LOGGING =================================\n",
    "LOG_DIR = cfg.OUT / \"logs\"; LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_FILE = LOG_DIR / f\"run_classifier_clean_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "\n",
    "def log(msg: str, end: str=\"\\n\"):\n",
    "    line = f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\"\n",
    "    print(line, end=end, flush=True)\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f: f.write(line + \"\\n\")\n",
    "\n",
    "class Heartbeat:\n",
    "    def __init__(self, label=\"RUN\", sec=60):\n",
    "        self.label=label; self.sec=sec; self._stop=threading.Event()\n",
    "        self._t=threading.Thread(target=self._loop, daemon=True)\n",
    "    def _loop(self):\n",
    "        t=0\n",
    "        while not self._stop.is_set():\n",
    "            time.sleep(self.sec); t+=self.sec\n",
    "            log(f\"♥ HEARTBEAT[{self.label}] alive ~{t//60} min …\")\n",
    "    def __enter__(self): self._t.start(); return self\n",
    "    def __exit__(self, *a): self._stop.set(); self._t.join(timeout=2)\n",
    "\n",
    "# ================================ UTILITIES ================================\n",
    "def sanitize(x: np.ndarray) -> np.ndarray:\n",
    "    return np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n",
    "\n",
    "def normalize_proba(P: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Robust n_samples×n_classes probabilities; fixes NaN/row-sum issues.\"\"\"\n",
    "    P = np.asarray(P)\n",
    "    if P.ndim == 1:\n",
    "        P = np.stack([1.0 - P, P], axis=1)\n",
    "    P = np.nan_to_num(P, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    s = P.sum(axis=1, keepdims=True)\n",
    "    bad = (s.reshape(-1) <= 0)\n",
    "    if np.any(bad):\n",
    "        P[bad, :] = 1.0 / max(1, P.shape[1])\n",
    "        s = P.sum(axis=1, keepdims=True)\n",
    "    s = np.clip(s, 1e-12, None)\n",
    "    return P / s\n",
    "\n",
    "def save_json(o, p: Path):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f: json.dump(o, f, indent=2)\n",
    "\n",
    "def read_json(p: Path):\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
    "\n",
    "# ============================ FEATURE/VECTOR CACHE =========================\n",
    "class FeatureCache:\n",
    "    def __init__(self, cache: Optional[Path]=None):\n",
    "        self.dir = cache or (cfg.WORKSPACE / \"cache\"); self.dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.mem = {}\n",
    "    def _key(self, sid, scale): return f\"{sid}_{scale}\"\n",
    "    def load(self, sid: str, scale: str) -> Optional[np.ndarray]:\n",
    "        key = self._key(sid, scale)\n",
    "        if key in self.mem: return self.mem[key]\n",
    "        pkl = self.dir / f\"{key}.pkl\"\n",
    "        if pkl.exists():\n",
    "            with open(pkl, \"rb\") as f: arr = pickle.load(f)\n",
    "            self.mem[key] = sanitize(arr); return self.mem[key]\n",
    "        src = (cfg.F05 if scale==\"0.5\" else cfg.F20) / f\"{sid}.npy\"\n",
    "        if src.exists():\n",
    "            arr = sanitize(np.load(src))\n",
    "            with open(pkl, \"wb\") as f: pickle.dump(arr, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            self.mem[key] = arr; return arr\n",
    "        return None\n",
    "\n",
    "class VectorCache:\n",
    "    def __init__(self, dataset: str):\n",
    "        self.dir = cfg.OUT / dataset / \"vec_cache\"; self.dir.mkdir(parents=True, exist_ok=True)\n",
    "    def _safe(self, s): return re.sub(r\"[^A-Za-z0-9_.-]\", \"_\", s)\n",
    "    def path(self, sid, recipe):\n",
    "        h = hashlib.md5(recipe.encode()).hexdigest()[:8]\n",
    "        return self.dir / f\"{self._safe(sid)}__{self._safe(recipe)}__{h}.npy\"\n",
    "    def get(self, sid, recipe):\n",
    "        p = self.path(sid, recipe)\n",
    "        if p.exists():\n",
    "            try: return sanitize(np.load(p))\n",
    "            except Exception: return None\n",
    "        return None\n",
    "    def put(self, sid, recipe, v):\n",
    "        np.save(self.path(sid, recipe), sanitize(v))\n",
    "\n",
    "# =============================== MANIFEST/SPLITS ===========================\n",
    "def infer_patient(sid: str) -> str:\n",
    "    s = str(sid)\n",
    "    if s.startswith(\"TCGA-\"):\n",
    "        m = re.match(r\"^(TCGA-[A-Z0-9]{2}-[A-Z0-9]{4})\", s)\n",
    "        if m: return m.group(1)\n",
    "    t = re.split(r\"[-_\\.]\", s)\n",
    "    if len(t) >= 3: return \"-\".join(t[:3])\n",
    "    if len(t) >= 2: return \"-\".join(t[:2])\n",
    "    return s\n",
    "\n",
    "def load_manifest_base() -> pd.DataFrame:\n",
    "    mp = cfg.WORKSPACE / \"manifests\" / \"manifest_tcga.csv\"\n",
    "    if not mp.exists(): raise FileNotFoundError(mp)\n",
    "    df = pd.read_csv(mp)\n",
    "    if \"slide_id\" not in df or \"cancer_code\" not in df:\n",
    "        raise ValueError(\"Manifest needs slide_id & cancer_code columns.\")\n",
    "    df[\"slide_id\"] = df[\"slide_id\"].astype(str)\n",
    "    df[\"cancer_code\"] = df[\"cancer_code\"].astype(str)\n",
    "\n",
    "    if \"case_id\" in df: df[\"group_id\"] = df[\"case_id\"].astype(str)\n",
    "    elif \"patient_id\" in df: df[\"group_id\"] = df[\"patient_id\"].astype(str)\n",
    "    else:\n",
    "        log(\"[WARN] 'case_id/patient_id' missing — inferring patient IDs from slide_id.\")\n",
    "        df[\"group_id\"] = df[\"slide_id\"].map(infer_patient)\n",
    "\n",
    "    df[\"has_05\"] = df[\"slide_id\"].map(lambda s: (cfg.F05 / f\"{s}.npy\").exists())\n",
    "    df[\"has_20\"] = df[\"slide_id\"].map(lambda s: (cfg.F20 / f\"{s}.npy\").exists())\n",
    "    base = df[df[\"has_05\"] & df[\"has_20\"]].reset_index(drop=True)\n",
    "\n",
    "    log(f\"Manifest loaded: {len(df)} rows | base cohort with both scales: {len(base)}\")\n",
    "    return base\n",
    "\n",
    "def make_splits(mb: pd.DataFrame):\n",
    "    y = mb[\"cancer_code\"].astype(str).values\n",
    "    g = mb[\"group_id\"].astype(str).values\n",
    "    all_s = []\n",
    "    for r in range(cfg.N_REPEATS):\n",
    "        sgkf = StratifiedGroupKFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=cfg.SEED + r)\n",
    "        rep=[]\n",
    "        for tr, va in sgkf.split(np.arange(len(mb)), y, g):\n",
    "            rep.append((tr.tolist(), va.tolist()))\n",
    "        all_s.append(rep)\n",
    "    return all_s\n",
    "\n",
    "def assert_split_integrity(mb: pd.DataFrame, splits):\n",
    "    gids = mb[\"group_id\"].astype(str).values\n",
    "    for r, rep in enumerate(splits, 1):\n",
    "        for f, (tr, va) in enumerate(rep, 1):\n",
    "            assert set(tr).isdisjoint(set(va)), f\"Index overlap r{r} f{f}\"\n",
    "            assert set(gids[tr]).isdisjoint(set(gids[va])), f\"Patient overlap r{r} f{f}\"\n",
    "\n",
    "def load_or_create_splits(mb: pd.DataFrame):\n",
    "    sp = cfg.OUT / cfg.DATASET / \"splits.json\"\n",
    "    if cfg.USE_EXISTING_SPLITS_IF_FOUND and sp.exists():\n",
    "        log(f\"Using existing splits: {sp}\")\n",
    "        s = read_json(sp)\n",
    "    else:\n",
    "        if not cfg.REBUILD_IF_MISSING and not sp.exists():\n",
    "            raise FileNotFoundError(\"splits.json missing and rebuild disabled.\")\n",
    "        log(\"Creating patient-level shared splits …\")\n",
    "        s = make_splits(mb); save_json(s, sp); log(f\"Wrote splits → {sp}\")\n",
    "    assert_split_integrity(mb, s)\n",
    "    return s\n",
    "\n",
    "# =============================== VECTORS (both scales, mean→concat) ========\n",
    "def mean_pool(F: np.ndarray) -> np.ndarray:\n",
    "    return sanitize(F).mean(axis=0).astype(np.float32)\n",
    "\n",
    "def build_vectors_both_scales(mb: pd.DataFrame, recipe: str=\"cls_both_concat_mean\"):\n",
    "    fcache = FeatureCache(); vcache = VectorCache(cfg.DATASET)\n",
    "    N = len(mb); X = [None]*N; y = mb[\"cancer_code\"].astype(str).values\n",
    "    start = time.time(); last = start; done = 0\n",
    "\n",
    "    def work(i, row):\n",
    "        sid = row[\"slide_id\"]\n",
    "        v = vcache.get(sid, recipe)\n",
    "        if v is None:\n",
    "            a = fcache.load(sid, \"0.5\"); b = fcache.load(sid, \"2.0\")\n",
    "            if a is None or b is None: return i, None\n",
    "            v = np.concatenate([mean_pool(a), mean_pool(b)]).astype(np.float32)\n",
    "            vcache.put(sid, recipe, v)\n",
    "        return i, sanitize(v)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max(2, (os.cpu_count() or 8))) as ex:\n",
    "        futs = [ex.submit(work, i, row) for i, row in enumerate(mb.to_dict(orient=\"records\"))]\n",
    "        for fu in as_completed(futs):\n",
    "            i, v = fu.result()\n",
    "            if v is None: raise RuntimeError(f\"vector missing at index {i}\")\n",
    "            X[i] = v; done += 1\n",
    "            now = time.time()\n",
    "            if now - last >= 5 or done == N:\n",
    "                rate = done / max(1e-9, now - start); eta = (N - done) / max(1e-9, rate)\n",
    "                log(f\"    [build:{recipe}] {done}/{N} | {rate:.1f}/s | ETA ~{int(eta//60)}m{int(eta%60)}s\")\n",
    "                last = now\n",
    "    X = np.vstack(X)\n",
    "    log(f\"  Built cached vectors: {N} samples, dim={X.shape[1]}\")\n",
    "    return X, y\n",
    "\n",
    "# =============================== EVALUATION ================================\n",
    "def run_fold(X, y_enc, n_cls, tr, va):\n",
    "    Xtr, Xva = X[tr], X[va]; ytr, yva = y_enc[tr], y_enc[va]\n",
    "    sc = StandardScaler(); Xtr = sc.fit_transform(Xtr); Xva = sc.transform(Xva)\n",
    "    if cfg.PCA_DIM and cfg.PCA_DIM > 0:\n",
    "        pca = PCA(n_components=min(cfg.PCA_DIM, Xtr.shape[1]), random_state=cfg.SEED)\n",
    "        Xtr = pca.fit_transform(Xtr); Xva = pca.transform(Xva)\n",
    "    clf = LogisticRegression(solver=\"sag\", max_iter=500, tol=1e-3,\n",
    "                             class_weight=\"balanced\", multi_class=\"auto\",\n",
    "                             random_state=cfg.SEED, n_jobs=1)\n",
    "    t0 = time.time(); clf.fit(Xtr, ytr); fit_s = time.time() - t0\n",
    "    y_pred = clf.predict(Xva)\n",
    "    proba = normalize_proba(getattr(clf, \"predict_proba\")(Xva))\n",
    "    acc  = accuracy_score(yva, y_pred)\n",
    "    bacc = balanced_accuracy_score(yva, y_pred)\n",
    "    f1m  = f1_score(yva, y_pred, average=\"macro\")\n",
    "    auc  = roc_auc_score(yva, proba, multi_class=\"ovr\", average=\"macro\") if n_cls > 2 else roc_auc_score(yva, proba[:,1])\n",
    "    return dict(acc=float(acc), bacc=float(bacc), f1m=float(f1m), auc=float(auc), fit_s=float(fit_s))\n",
    "\n",
    "def eval_classifier(X, y, splits):\n",
    "    le = LabelEncoder(); y_enc = le.fit_transform(y); n_cls = len(np.unique(y_enc))\n",
    "    allm = defaultdict(list)\n",
    "    tasks = []\n",
    "    for r, rep in enumerate(splits, 1):\n",
    "        for f, (tr, va) in enumerate(rep, 1):\n",
    "            tasks.append((r, f, tr, va, len(rep)))\n",
    "\n",
    "    res = Parallel(n_jobs=cfg.CV_JOBS, prefer=\"processes\")(\n",
    "        delayed(run_fold)(X, y_enc, n_cls, tr, va) for (_,_,tr,va,_) in tasks\n",
    "    )\n",
    "\n",
    "    for i, m in enumerate(res, 1):\n",
    "        r, f, _, _, nf = tasks[i-1]\n",
    "        allm[\"accuracy\"].append(m[\"acc\"])\n",
    "        allm[\"balanced_accuracy\"].append(m[\"bacc\"])\n",
    "        allm[\"f1_macro\"].append(m[\"f1m\"])\n",
    "        allm[\"auc\"].append(m[\"auc\"])\n",
    "        log(f\"  → Fitted r{r}/{len(splits)} f{f}/{nf} [{i}/{len(tasks)}] | \"\n",
    "            f\"acc={m['acc']:.4f}, bacc={m['bacc']:.4f}, f1M={m['f1m']:.4f}, auc={m['auc']:.4f} \"\n",
    "            f\"(fit {m['fit_s']:.1f}s)\")\n",
    "\n",
    "    out = {}\n",
    "    for k, v in allm.items():\n",
    "        arr = np.asarray(v, dtype=float)\n",
    "        out[k] = dict(\n",
    "            mean=float(arr.mean()),\n",
    "            std=float(arr.std()),\n",
    "            ci_lower=float(np.percentile(arr, 2.5)),\n",
    "            ci_upper=float(np.percentile(arr, 97.5))\n",
    "        )\n",
    "    return out\n",
    "\n",
    "def write_classifier_csv(metrics):\n",
    "    rows=[]\n",
    "    for metric, mv in metrics.items():\n",
    "        rows.append(dict(\n",
    "            dataset=cfg.DATASET, ablation=\"classifier\", variant=\"logistic\", metric=metric,\n",
    "            mean=mv[\"mean\"], std=mv[\"std\"], ci_lower=mv[\"ci_lower\"], ci_upper=mv[\"ci_upper\"]\n",
    "        ))\n",
    "    df = pd.DataFrame(rows)\n",
    "    out_csv = cfg.OUT / cfg.DATASET / \"classifier_ablation.csv\"\n",
    "    if out_csv.exists() and not cfg.OVERWRITE_CSV:\n",
    "        raise RuntimeError(f\"{out_csv} exists and OVERWRITE_CSV=False\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    log(f\"Saved → {out_csv}\")\n",
    "    return out_csv\n",
    "\n",
    "# =============================== MAIN =====================================\n",
    "def main():\n",
    "    log(\"=\"*92)\n",
    "    log(\"OPENSLIDEFM — CLASSIFIER RE-RUN (clean, split-aligned, no stale artifacts)\")\n",
    "    log(f\"Device: {cfg.DEVICE}  |  Workspace: {cfg.WORKSPACE}\")\n",
    "    log(f\"Results: {cfg.OUT}\")\n",
    "    log(\"=\"*92)\n",
    "\n",
    "    # sanity\n",
    "    if not cfg.F05.exists(): raise FileNotFoundError(cfg.F05)\n",
    "    if not cfg.F20.exists(): raise FileNotFoundError(cfg.F20)\n",
    "\n",
    "    # clean stale classifier artifacts (CSV + fold checkpoints)\n",
    "    if cfg.CLEAR_OLD_CLASSIFIER_ARTIFACTS:\n",
    "        csv_path = cfg.OUT / cfg.DATASET / \"classifier_ablation.csv\"\n",
    "        if csv_path.exists():\n",
    "            os.remove(csv_path); log(f\"[CLEAN] removed {csv_path}\")\n",
    "        ck_root = cfg.OUT / cfg.DATASET / \"checkpoints\"\n",
    "        if ck_root.exists():\n",
    "            for d in ck_root.glob(\"classifier__*\"):\n",
    "                shutil.rmtree(d, ignore_errors=True); log(f\"[CLEAN] removed {d}\")\n",
    "\n",
    "    mb = load_manifest_base()\n",
    "    with Heartbeat(label=cfg.DATASET, sec=cfg.HEARTBEAT_SEC):\n",
    "        # Splits: reuse if present (to match other ablations); else create\n",
    "        splits = load_or_create_splits(mb)\n",
    "\n",
    "        # Vectors and evaluation\n",
    "        X, y = build_vectors_both_scales(mb)\n",
    "        log(f\"Sanity: n_samples={len(y)}, X_dim={X.shape[1]}, classes={pd.Series(y).nunique()}\")\n",
    "        metrics = eval_classifier(X, y, splits)\n",
    "\n",
    "    # Save & print compact summary\n",
    "    csv_out = write_classifier_csv(metrics)\n",
    "    log(\"\\n================ CLASSIFIER SUMMARY ================\")\n",
    "    for k in [\"accuracy\",\"auc\",\"balanced_accuracy\",\"f1_macro\"]:\n",
    "        mv = metrics[k]\n",
    "        log(f\"{k:<17} → logistic   mean={mv['mean']:.4f} ± {mv['std']:.4f} \"\n",
    "            f\"[{mv['ci_lower']:.4f}, {mv['ci_upper']:.4f}]\")\n",
    "    log(\"====================================================\")\n",
    "    log(f\"RUN COMPLETE ✓  (CSV: {csv_out})\")\n",
    "    log(\"=\"*92)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===== cap BLAS threads before imports =====\n",
    "import os\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\",\"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\",\"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\",\"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\",\"1\")\n",
    "\n",
    "import sys, json, time, threading, pickle, re, hashlib, warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========================= CONFIG =========================\n",
    "@dataclass\n",
    "class CFG:\n",
    "    WORKSPACE: Path = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\")\n",
    "    F05: Path = WORKSPACE / \"features\" / \"scale0p5\"\n",
    "    F20: Path = WORKSPACE / \"features\" / \"scale2p0\"\n",
    "    OUT: Path = WORKSPACE / \"results\" / \"ablations_complete\"\n",
    "    DATASET: str = \"tcga\"\n",
    "    SEED: int = 42\n",
    "    CV_JOBS: int = max(2, (os.cpu_count() or 8) - 1)\n",
    "    HEARTBEAT_SEC: int = 45\n",
    "    VEC_RECIPE: str = \"cls_both_concat_mean\"  # reuse from classifier run\n",
    "\n",
    "cfg = CFG()\n",
    "LOG = cfg.OUT / \"logs\" / f\"check_scale_vs_classifier_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "LOG.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def log(msg, end=\"\\n\"):\n",
    "    line = f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\"\n",
    "    print(line, end=end, flush=True)\n",
    "    with open(LOG, \"a\", encoding=\"utf-8\") as f: f.write(line+\"\\n\")\n",
    "\n",
    "# ========================= UTILS ==========================\n",
    "def sanitize(x: np.ndarray) -> np.ndarray:\n",
    "    return np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n",
    "\n",
    "def normalize_proba(P: np.ndarray) -> np.ndarray:\n",
    "    P = np.asarray(P)\n",
    "    if P.ndim == 1: P = np.stack([1.0-P, P], axis=1)\n",
    "    P = np.nan_to_num(P, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    s = P.sum(axis=1, keepdims=True)\n",
    "    bad = (s.reshape(-1) <= 0)\n",
    "    if np.any(bad):\n",
    "        P[bad,:] = 1.0 / max(1, P.shape[1]); s = P.sum(axis=1, keepdims=True)\n",
    "    return P / np.clip(s, 1e-12, None)\n",
    "\n",
    "def read_json(p: Path): \n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f: \n",
    "        return json.load(f)\n",
    "\n",
    "# ===================== MANIFEST/SPLITS ====================\n",
    "def infer_patient(sid: str) -> str:\n",
    "    s = str(sid)\n",
    "    if s.startswith(\"TCGA-\"):\n",
    "        m = re.match(r\"^(TCGA-[A-Z0-9]{2}-[A-Z0-9]{4})\", s)\n",
    "        if m: return m.group(1)\n",
    "    t = re.split(r\"[-_\\.]\", s)\n",
    "    if len(t)>=3: return \"-\".join(t[:3])\n",
    "    if len(t)>=2: return \"-\".join(t[:2])\n",
    "    return s\n",
    "\n",
    "def load_manifest_base() -> pd.DataFrame:\n",
    "    mp = cfg.WORKSPACE / \"manifests\" / \"manifest_tcga.csv\"\n",
    "    if not mp.exists(): raise FileNotFoundError(mp)\n",
    "    df = pd.read_csv(mp)\n",
    "    if \"slide_id\" not in df or \"cancer_code\" not in df:\n",
    "        raise ValueError(\"Manifest needs slide_id & cancer_code.\")\n",
    "    df[\"slide_id\"] = df[\"slide_id\"].astype(str)\n",
    "    df[\"cancer_code\"] = df[\"cancer_code\"].astype(str)\n",
    "    if \"case_id\" in df: df[\"group_id\"] = df[\"case_id\"].astype(str)\n",
    "    elif \"patient_id\" in df: df[\"group_id\"] = df[\"patient_id\"].astype(str)\n",
    "    else:\n",
    "        log(\"[WARN] 'case_id/patient_id' missing — inferring patient from slide_id.\")\n",
    "        df[\"group_id\"] = df[\"slide_id\"].map(infer_patient)\n",
    "    df[\"has_both\"] = df[\"slide_id\"].map(lambda s: (cfg.F05 / f\"{s}.npy\").exists() and (cfg.F20 / f\"{s}.npy\").exists())\n",
    "    base = df[df[\"has_both\"]].reset_index(drop=True)\n",
    "    log(f\"Manifest loaded: {len(df)} rows | base (both scales): {len(base)}\")\n",
    "    return base\n",
    "\n",
    "def load_splits(mb: pd.DataFrame):\n",
    "    sp = cfg.OUT / cfg.DATASET / \"splits.json\"\n",
    "    if not sp.exists(): raise FileNotFoundError(f\"Missing splits.json → {sp}\")\n",
    "    s = read_json(sp)\n",
    "    # integrity (patient-level disjoint)\n",
    "    gids = mb[\"group_id\"].astype(str).values\n",
    "    for r, rep in enumerate(s, 1):\n",
    "        for f, (tr,va) in enumerate(rep, 1):\n",
    "            assert set(gids[tr]).isdisjoint(set(gids[va])), f\"Patient overlap r{r} f{f}\"\n",
    "    log(f\"Using existing splits: {sp}\")\n",
    "    return s\n",
    "\n",
    "# ====================== VECTOR CACHE (reuse) ======================\n",
    "class VectorCache:\n",
    "    def __init__(self):\n",
    "        d = cfg.OUT / cfg.DATASET / \"vec_cache\"\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "        self.dir = d\n",
    "    def _safe(self, s): return re.sub(r\"[^A-Za-z0-9_.-]\",\"_\", s)\n",
    "    def path(self, sid, recipe):\n",
    "        h = hashlib.md5(recipe.encode()).hexdigest()[:8]\n",
    "        return self.dir / f\"{self._safe(sid)}__{self._safe(recipe)}__{h}.npy\"\n",
    "    def get(self, sid, recipe):\n",
    "        p = self.path(sid, recipe)\n",
    "        if not p.exists(): return None\n",
    "        return sanitize(np.load(p))\n",
    "\n",
    "def load_vectors_from_cache(mb: pd.DataFrame, recipe: str):\n",
    "    vcache = VectorCache()\n",
    "    X=[]; y=[]\n",
    "    missing=0\n",
    "    for row in mb.itertuples(index=False):\n",
    "        sid=row.slide_id; v = vcache.get(sid, recipe)\n",
    "        if v is None:\n",
    "            missing+=1\n",
    "        else:\n",
    "            X.append(v); y.append(row.cancer_code)\n",
    "    if missing>0:\n",
    "        raise RuntimeError(f\"{missing} vectors missing for recipe {recipe}. \"\n",
    "                           f\"Run the classifier script first to populate cache.\")\n",
    "    X = np.vstack(X); y = np.array(y, dtype=str)\n",
    "    log(f\"Loaded vectors from cache: {X.shape[0]} samples, dim={X.shape[1]}\")\n",
    "    return X, y\n",
    "\n",
    "# ====================== EVAL (same as classifier) ======================\n",
    "def run_fold(X, y_enc, n_cls, tr, va):\n",
    "    sc = StandardScaler()\n",
    "    Xtr, Xva = sc.fit_transform(X[tr]), sc.transform(X[va])\n",
    "    clf = LogisticRegression(solver=\"sag\", max_iter=500, tol=1e-3,\n",
    "                             class_weight=\"balanced\", multi_class=\"auto\",\n",
    "                             random_state=cfg.SEED, n_jobs=1)\n",
    "    clf.fit(Xtr, y_enc[tr])\n",
    "    y_pred = clf.predict(Xva)\n",
    "    proba = normalize_proba(getattr(clf, \"predict_proba\")(Xva))\n",
    "    acc  = accuracy_score(y_enc[va], y_pred)\n",
    "    bacc = balanced_accuracy_score(y_enc[va], y_pred)\n",
    "    f1m  = f1_score(y_enc[va], y_pred, average=\"macro\")\n",
    "    auc  = roc_auc_score(y_enc[va], proba, multi_class=\"ovr\", average=\"macro\") if n_cls>2 else roc_auc_score(y_enc[va], proba[:,1])\n",
    "    return dict(acc=float(acc), bacc=float(bacc), f1m=float(f1m), auc=float(auc))\n",
    "\n",
    "def eval_with_splits(X, y, splits):\n",
    "    le = LabelEncoder(); y_enc = le.fit_transform(y); n_cls = len(np.unique(y_enc))\n",
    "    allm = defaultdict(list)\n",
    "    idx=0\n",
    "    for r, rep in enumerate(splits, 1):\n",
    "        for f, (tr,va) in enumerate(rep, 1):\n",
    "            idx+=1\n",
    "            m = run_fold(X, y_enc, n_cls, np.array(tr), np.array(va))\n",
    "            allm[\"accuracy\"].append(m[\"acc\"])\n",
    "            allm[\"balanced_accuracy\"].append(m[\"bacc\"])\n",
    "            allm[\"f1_macro\"].append(m[\"f1m\"])\n",
    "            allm[\"auc\"].append(m[\"auc\"])\n",
    "            log(f\"  → Fold r{r}/{len(splits)} f{f}/{len(rep)} [{idx}/{len(splits)*len(rep)}] \"\n",
    "                f\"| acc={m['acc']:.4f}, bacc={m['bacc']:.4f}, f1M={m['f1m']:.4f}, auc={m['auc']:.4f}\")\n",
    "    out={}\n",
    "    for k,v in allm.items():\n",
    "        arr=np.asarray(v, dtype=float)\n",
    "        out[k]=dict(mean=float(arr.mean()), std=float(arr.std()),\n",
    "                    ci_lower=float(np.percentile(arr,2.5)),\n",
    "                    ci_upper=float(np.percentile(arr,97.5)))\n",
    "    return out\n",
    "\n",
    "def write_csv(metrics):\n",
    "    rows=[]\n",
    "    for metric, mv in metrics.items():\n",
    "        rows.append(dict(dataset=cfg.DATASET, ablation=\"scale_check\", variant=\"both_scales\",\n",
    "                         metric=metric, mean=mv[\"mean\"], std=mv[\"std\"],\n",
    "                         ci_lower=mv[\"ci_lower\"], ci_upper=mv[\"ci_upper\"]))\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = cfg.OUT / cfg.DATASET / \"scale_ablation_check.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "    log(f\"Saved → {out}\")\n",
    "    return out\n",
    "\n",
    "# ============================== MAIN ==============================\n",
    "def main():\n",
    "    log(\"=\"*86)\n",
    "    log(\"OPENSLIDEFM — SCALE/CLASSIFIER CONSISTENCY CHECK (both_scales → mean → concat → logistic)\")\n",
    "    log(f\"Workspace: {cfg.WORKSPACE}\")\n",
    "    log(f\"Results:   {cfg.OUT}\")\n",
    "    log(\"=\"*86)\n",
    "\n",
    "    # 1) manifest & splits\n",
    "    mb = load_manifest_base()\n",
    "    splits = load_splits(mb)\n",
    "\n",
    "    # 2) load the VECTORS that classifier script just built\n",
    "    X, y = load_vectors_from_cache(mb, cfg.VEC_RECIPE)\n",
    "\n",
    "    # 3) evaluate identically to classifier\n",
    "    metrics = eval_with_splits(X, y, splits)\n",
    "    csv_new = write_csv(metrics)\n",
    "\n",
    "    # 4) side-by-side vs classifier CSV\n",
    "    clf_csv = cfg.OUT / cfg.DATASET / \"classifier_ablation.csv\"\n",
    "    if clf_csv.exists():\n",
    "        clf = pd.read_csv(clf_csv)\n",
    "        def row(metric): \n",
    "            s = clf[(clf[\"ablation\"]==\"classifier\") & (clf[\"variant\"]==\"logistic\") & (clf[\"metric\"]==metric)]\n",
    "            return None if s.empty else (float(s[\"mean\"]), float(s[\"std\"]))\n",
    "        log(\"\\n====== SIDE-BY-SIDE (mean ± sd) ======\")\n",
    "        for m in [\"accuracy\",\"auc\",\"balanced_accuracy\",\"f1_macro\"]:\n",
    "            a = metrics[m][\"mean\"]; b = metrics[m][\"std\"]\n",
    "            c = row(m)\n",
    "            if c is None:\n",
    "                log(f\"{m:<18}: scale_check {a:.4f} ± {b:.4f} | classifier MISSING\")\n",
    "            else:\n",
    "                log(f\"{m:<18}: scale_check {a:.4f} ± {b:.4f} | classifier {c[0]:.4f} ± {c[1]:.4f}\")\n",
    "        log(\"======================================\\n\")\n",
    "    else:\n",
    "        log(\"Classifier CSV not found for side-by-side; only scale_check printed.\")\n",
    "\n",
    "    log(\"DONE ✓\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Fixed Diagnostic - handles missing variants gracefully\n",
    "Run this in Jupyter and paste the output\n",
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_numpy(obj):\n",
    "    \"\"\"Convert numpy types to native Python\"\"\"\n",
    "    if isinstance(obj, (np.integer, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "WORKSPACE = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\")\n",
    "RESULTS = WORKSPACE / \"results\" / \"ablations_complete\" / \"tcga\"\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. ABLATION STUDY RESULTS\n",
    "# ==============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"EXTRACTING ABLATION STUDY RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ablation_files = {\n",
    "    \"scale\": \"scale_ablation.csv\",\n",
    "    \"pooling\": \"pooling_ablation.csv\",\n",
    "    \"token_budget\": \"token_budget_ablation.csv\",\n",
    "    \"fusion\": \"feature_fusion_ablation.csv\",\n",
    "    \"classifier\": \"classifier_ablation.csv\"\n",
    "}\n",
    "\n",
    "metrics[\"ablation\"] = {}\n",
    "\n",
    "for component, filename in ablation_files.items():\n",
    "    filepath = RESULTS / filename\n",
    "    if filepath.exists():\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"\\n{component.upper()}:\")\n",
    "        print(df.to_string(index=False))\n",
    "        metrics[\"ablation\"][component] = convert_numpy(df.to_dict('records'))\n",
    "    else:\n",
    "        print(f\"\\n{component.upper()}: FILE NOT FOUND\")\n",
    "        metrics[\"ablation\"][component] = None\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. PRIMARY RESULTS (BEST MODEL)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING PRIMARY RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find the best configuration from scale ablation\n",
    "if metrics[\"ablation\"][\"scale\"]:\n",
    "    scale_df = pd.DataFrame(metrics[\"ablation\"][\"scale\"])\n",
    "    \n",
    "    print(f\"\\nAvailable variants in scale ablation:\")\n",
    "    print(scale_df[\"variant\"].unique())\n",
    "    \n",
    "    # Try to find the best performing variant based on accuracy\n",
    "    if \"metric\" in scale_df.columns and \"mean\" in scale_df.columns:\n",
    "        acc_rows = scale_df[scale_df[\"metric\"] == \"accuracy\"]\n",
    "        if not acc_rows.empty:\n",
    "            best_idx = acc_rows[\"mean\"].idxmax()\n",
    "            best_row = acc_rows.loc[best_idx]\n",
    "            \n",
    "            print(f\"\\nBest Model ({best_row['variant']}):\")\n",
    "            print(f\"  Accuracy: {best_row['mean']:.4f} ± {best_row['std']:.4f}\")\n",
    "            \n",
    "            metrics[\"primary\"] = {\n",
    "                \"best_variant\": str(best_row[\"variant\"]),\n",
    "                \"accuracy\": {\"mean\": float(best_row[\"mean\"]), \"std\": float(best_row[\"std\"])}\n",
    "            }\n",
    "            \n",
    "            # Try to get other metrics for this variant\n",
    "            for metric_name in [\"auc\", \"balanced_accuracy\", \"f1_macro\"]:\n",
    "                metric_row = scale_df[\n",
    "                    (scale_df[\"variant\"] == best_row[\"variant\"]) & \n",
    "                    (scale_df[\"metric\"] == metric_name)\n",
    "                ]\n",
    "                if not metric_row.empty:\n",
    "                    metrics[\"primary\"][metric_name] = {\n",
    "                        \"mean\": float(metric_row.iloc[0][\"mean\"]),\n",
    "                        \"std\": float(metric_row.iloc[0][\"std\"])\n",
    "                    }\n",
    "                else:\n",
    "                    metrics[\"primary\"][metric_name] = None\n",
    "        else:\n",
    "            print(\"No accuracy metric found in scale ablation\")\n",
    "            metrics[\"primary\"] = None\n",
    "    else:\n",
    "        print(\"Unexpected scale ablation format\")\n",
    "        metrics[\"primary\"] = None\n",
    "else:\n",
    "    print(\"No scale ablation data available\")\n",
    "    metrics[\"primary\"] = None\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PER-CANCER-TYPE RESULTS\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING PER-CANCER-TYPE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "per_cancer_file = RESULTS / \"per_cancer_results.csv\"\n",
    "if per_cancer_file.exists():\n",
    "    df = pd.read_csv(per_cancer_file)\n",
    "    print(df.to_string(index=False))\n",
    "    metrics[\"per_cancer\"] = convert_numpy(df.to_dict('records'))\n",
    "else:\n",
    "    print(\"per_cancer_results.csv NOT FOUND\")\n",
    "    metrics[\"per_cancer\"] = None\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. CONFUSION MATRIX\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING CONFUSION MATRIX\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "confusion_file = RESULTS / \"confusion_matrix.csv\"\n",
    "if confusion_file.exists():\n",
    "    df = pd.read_csv(confusion_file, index_col=0)\n",
    "    print(df.to_string())\n",
    "    metrics[\"confusion_matrix\"] = {\n",
    "        \"matrix\": convert_numpy(df.values.tolist()),\n",
    "        \"labels\": list(df.columns)\n",
    "    }\n",
    "else:\n",
    "    print(\"confusion_matrix.csv NOT FOUND\")\n",
    "    metrics[\"confusion_matrix\"] = None\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. DATASET STATISTICS\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING DATASET STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for dataset stats files\n",
    "stats_files = {\n",
    "    \"tcga\": RESULTS.parent.parent / \"manifest\" / \"tcga_stats.csv\",\n",
    "    \"cam16\": RESULTS.parent.parent / \"manifest\" / \"cam16_stats.csv\",\n",
    "    \"cam17\": RESULTS.parent.parent / \"manifest\" / \"cam17_stats.csv\",\n",
    "    \"panda\": RESULTS.parent.parent / \"manifest\" / \"panda_stats.csv\"\n",
    "}\n",
    "\n",
    "metrics[\"datasets\"] = {}\n",
    "for dataset, filepath in stats_files.items():\n",
    "    if filepath.exists():\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"\\n{dataset.upper()}:\")\n",
    "        print(df.to_string(index=False))\n",
    "        metrics[\"datasets\"][dataset] = convert_numpy(df.to_dict('records'))\n",
    "    else:\n",
    "        print(f\"\\n{dataset.upper()}: FILE NOT FOUND\")\n",
    "        metrics[\"datasets\"][dataset] = None\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. COMPUTATIONAL REQUIREMENTS\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPUTATIONAL REQUIREMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "timing_file = RESULTS / \"timing_stats.json\"\n",
    "if timing_file.exists():\n",
    "    with open(timing_file) as f:\n",
    "        metrics[\"computational\"] = json.load(f)\n",
    "    print(json.dumps(metrics[\"computational\"], indent=2))\n",
    "else:\n",
    "    print(\"timing_stats.json NOT FOUND\")\n",
    "    metrics[\"computational\"] = None\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. CHECK FOR ADDITIONAL METRIC FILES\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKING FOR ADDITIONAL FILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# List all CSV files in results directory\n",
    "csv_files = list(RESULTS.glob(\"*.csv\"))\n",
    "print(f\"\\nAll CSV files in {RESULTS}:\")\n",
    "for f in csv_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "# List all JSON files\n",
    "json_files = list(RESULTS.glob(\"*.json\"))\n",
    "print(f\"\\nAll JSON files:\")\n",
    "for f in json_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# SAVE & DISPLAY JSON\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING COMPREHENSIVE METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_file = WORKSPACE / \"comprehensive_metrics.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Saved to: {output_file}\")\n",
    "print(f\"\\n📋 PASTE THIS JSON BACK TO CLAUDE:\\n\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(metrics, indent=2))\n",
    "print(\"=\"*80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# OpenSlideFM - Research Lab Standard GitHub Upload (FIXED)\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import re\n",
    "import base64\n",
    "import getpass\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "REPO_OWNER = \"Sjtu-Fuxilab\"\n",
    "REPO_NAME = \"OpenSlideFM\"\n",
    "NOTEBOOK_PATH = Path(r\"D:\\个人文件夹\\Sanwal\\OpenSlide\\OP_FM.ipynb\")\n",
    "EXCLUDE_CELLS = [17, 20, 24]\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "def github_api_request(token, method, endpoint, data=None):\n",
    "    \"\"\"Make GitHub API request\"\"\"\n",
    "    url = f\"https://api.github.com{endpoint}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\",\n",
    "        \"User-Agent\": \"OpenSlideFM-Uploader\"\n",
    "    }\n",
    "    \n",
    "    if data:\n",
    "        headers[\"Content-Type\"] = \"application/json\"\n",
    "        body = json.dumps(data).encode('utf-8')\n",
    "    else:\n",
    "        body = None\n",
    "    \n",
    "    req = urllib.request.Request(url, data=body, headers=headers, method=method)\n",
    "    \n",
    "    try:\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            return json.loads(response.read().decode('utf-8')), response.status\n",
    "    except urllib.error.HTTPError as e:\n",
    "        error_body = e.read().decode('utf-8')\n",
    "        try:\n",
    "            return json.loads(error_body), e.code\n",
    "        except:\n",
    "            return {\"message\": error_body}, e.code\n",
    "\n",
    "def upload_file(token, filepath, content, message):\n",
    "    \"\"\"Upload or update a file on GitHub\"\"\"\n",
    "    # Check if exists\n",
    "    response, status = github_api_request(token, \"GET\", \n",
    "        f\"/repos/{REPO_OWNER}/{REPO_NAME}/contents/{filepath}\")\n",
    "    existing_sha = response.get(\"sha\") if status == 200 else None\n",
    "    \n",
    "    # Encode content\n",
    "    if isinstance(content, str):\n",
    "        content_bytes = content.encode('utf-8')\n",
    "    else:\n",
    "        content_bytes = content\n",
    "    content_base64 = base64.b64encode(content_bytes).decode('utf-8')\n",
    "    \n",
    "    payload = {\n",
    "        \"message\": message,\n",
    "        \"content\": content_base64,\n",
    "        \"branch\": \"main\"\n",
    "    }\n",
    "    if existing_sha:\n",
    "        payload[\"sha\"] = existing_sha\n",
    "    \n",
    "    response, status = github_api_request(token, \"PUT\",\n",
    "        f\"/repos/{REPO_OWNER}/{REPO_NAME}/contents/{filepath}\", payload)\n",
    "    \n",
    "    return status in [200, 201], response\n",
    "\n",
    "def delete_file(token, filepath):\n",
    "    \"\"\"Delete a file from GitHub\"\"\"\n",
    "    response, status = github_api_request(token, \"GET\",\n",
    "        f\"/repos/{REPO_OWNER}/{REPO_NAME}/contents/{filepath}\")\n",
    "    \n",
    "    if status == 200:\n",
    "        sha = response.get(\"sha\")\n",
    "        payload = {\n",
    "            \"message\": f\"Remove {filepath}\",\n",
    "            \"sha\": sha,\n",
    "            \"branch\": \"main\"\n",
    "        }\n",
    "        response, status = github_api_request(token, \"DELETE\",\n",
    "            f\"/repos/{REPO_OWNER}/{REPO_NAME}/contents/{filepath}\", payload)\n",
    "        return status in [200, 204]\n",
    "    return True  # File doesn't exist\n",
    "\n",
    "def clean_source(source: str) -> str:\n",
    "    \"\"\"Clean source code\"\"\"\n",
    "    source = re.sub(r'DEBUG\\s*=\\s*True', 'DEBUG = False', source)\n",
    "    source = re.sub(r'QUICK_TEST\\s*=\\s*True', 'QUICK_TEST = False', source)\n",
    "    source = re.sub(r'quick_test\\s*=\\s*True', 'quick_test = False', source, flags=re.IGNORECASE)\n",
    "    return source\n",
    "\n",
    "def source_to_list(source: str) -> list:\n",
    "    \"\"\"Convert source string to proper notebook cell format (list of lines)\"\"\"\n",
    "    lines = source.split('\\n')\n",
    "    # Each line except the last should end with \\n\n",
    "    result = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if i < len(lines) - 1:\n",
    "            result.append(line + '\\n')\n",
    "        else:\n",
    "            result.append(line)\n",
    "    return result\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Load and Process Notebook\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: Loading and processing notebook\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with open(NOTEBOOK_PATH, 'r', encoding='utf-8') as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "original_count = len(nb['cells'])\n",
    "print(f\"Original notebook: {original_count} cells\")\n",
    "\n",
    "# Process cells - keep them separate!\n",
    "cleaned_cells = []\n",
    "for i, cell in enumerate(nb['cells'], 1):\n",
    "    if i in EXCLUDE_CELLS:\n",
    "        print(f\"  ✗ Excluding Cell {i} (synthetic data)\")\n",
    "        continue\n",
    "    \n",
    "    # Deep copy the cell\n",
    "    new_cell = {\n",
    "        \"cell_type\": cell.get(\"cell_type\", \"code\"),\n",
    "        \"metadata\": {},\n",
    "        \"source\": []\n",
    "    }\n",
    "    \n",
    "    # Get source\n",
    "    source = cell.get('source', [])\n",
    "    if isinstance(source, list):\n",
    "        source = ''.join(source)\n",
    "    \n",
    "    # Clean if code cell\n",
    "    if cell.get(\"cell_type\") == \"code\":\n",
    "        source = clean_source(source)\n",
    "        new_cell[\"outputs\"] = []\n",
    "        new_cell[\"execution_count\"] = None\n",
    "    \n",
    "    # Convert source to proper list format\n",
    "    new_cell[\"source\"] = source_to_list(source)\n",
    "    \n",
    "    cleaned_cells.append(new_cell)\n",
    "    print(f\"  ✓ Cell {i}: {len(new_cell['source'])} lines\")\n",
    "\n",
    "print(f\"\\nTotal cells after cleaning: {len(cleaned_cells)}\")\n",
    "\n",
    "# Create proper notebook structure\n",
    "cleaned_notebook = {\n",
    "    \"cells\": cleaned_cells,\n",
    "    \"metadata\": {\n",
    "        \"kernelspec\": {\n",
    "            \"display_name\": \"Python 3\",\n",
    "            \"language\": \"python\",\n",
    "            \"name\": \"python3\"\n",
    "        },\n",
    "        \"language_info\": {\n",
    "            \"name\": \"python\",\n",
    "            \"version\": \"3.10.0\"\n",
    "        }\n",
    "    },\n",
    "    \"nbformat\": 4,\n",
    "    \"nbformat_minor\": 5\n",
    "}\n",
    "\n",
    "# Validate JSON\n",
    "notebook_json = json.dumps(cleaned_notebook, indent=1, ensure_ascii=False)\n",
    "print(f\"✓ Notebook JSON size: {len(notebook_json)/1024:.1f} KB\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Prepare All Files\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: Preparing repository files\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# README.md\n",
    "README = '''# OpenSlideFM\n",
    "\n",
    "<p align=\"center\">\n",
    "  <b>A Resource-Efficient Foundation Model for Computational Pathology</b>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"#installation\">Installation</a> •\n",
    "  <a href=\"#quick-start\">Quick Start</a> •\n",
    "  <a href=\"#pipeline\">Pipeline</a> •\n",
    "  <a href=\"#benchmarks\">Benchmarks</a> •\n",
    "  <a href=\"#citation\">Citation</a>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "**OpenSlideFM** is a foundation model for computational pathology that achieves competitive performance with significantly fewer parameters, enabling training on consumer-grade hardware (single GPU).\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- 🚀 **Resource Efficient**: Trainable on a single RTX 4090 GPU\n",
    "- 🎯 **Competitive Performance**: Matches larger foundation models\n",
    "- 🔬 **Multi-scale Analysis**: Two-scale tiling (5x, 20x)\n",
    "- 📊 **Reproducible**: Complete pipeline with provenance tracking\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "# Clone repository\n",
    "git clone https://github.com/Sjtu-Fuxilab/OpenSlideFM.git\n",
    "cd OpenSlideFM\n",
    "\n",
    "# Create environment\n",
    "conda create -n openslidefm python=3.10\n",
    "conda activate openslidefm\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### System Dependencies\n",
    "\n",
    "```bash\n",
    "# Ubuntu/Debian\n",
    "sudo apt-get install openslide-tools\n",
    "\n",
    "# macOS\n",
    "brew install openslide\n",
    "\n",
    "# Windows: Download from https://openslide.org/download/\n",
    "```\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. **Configure paths** in the notebook or set environment variables\n",
    "2. **Open the notebook**: `jupyter notebook notebooks/OP_FM.ipynb`\n",
    "3. **Run cells sequentially** - each section is documented\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "```\n",
    "WSI Input → QC & Mask → Two-Scale Tiling → Feature Extraction\n",
    "         → BYOL Pretraining → Slide Embeddings → Downstream Tasks\n",
    "```\n",
    "\n",
    "### Pipeline Stages\n",
    "\n",
    "| Stage | Description |\n",
    "|-------|-------------|\n",
    "| 1. Environment Setup | Initialize workspace, validate dependencies |\n",
    "| 2. Dataset Manifest | Create slide inventory with provenance |\n",
    "| 3. Quality Control | Tissue detection, artifact filtering |\n",
    "| 4. Tiling | Two-scale tile extraction (5x, 20x) |\n",
    "| 5. Feature Extraction | Extract tile-level features |\n",
    "| 6. Pretraining | BYOL self-supervised learning |\n",
    "| 7. Evaluation | TCGA, CAMELYON, PANDA benchmarks |\n",
    "\n",
    "## Benchmarks\n",
    "\n",
    "### TCGA Pan-Cancer Classification\n",
    "\n",
    "| Model | Parameters | Accuracy | Hardware |\n",
    "|-------|-----------|----------|----------|\n",
    "| UNI | 307M | 81.2% | 8× A100 |\n",
    "| CONCH | 307M | 79.8% | 8× A100 |\n",
    "| **OpenSlideFM** | **42M** | **80.1%** | **1× RTX 4090** |\n",
    "\n",
    "### CAMELYON16\n",
    "\n",
    "| Model | AUC | Accuracy |\n",
    "|-------|-----|----------|\n",
    "| UNI | 0.942 | 89.3% |\n",
    "| **OpenSlideFM** | **0.938** | **88.7%** |\n",
    "\n",
    "## Repository Structure\n",
    "\n",
    "```\n",
    "OpenSlideFM/\n",
    "├── README.md\n",
    "├── LICENSE\n",
    "├── requirements.txt\n",
    "└── notebooks/\n",
    "    └── OP_FM.ipynb      ← Main pipeline notebook (22 cells)\n",
    "```\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@article{zafar2025openslidefm,\n",
    "  title={OpenSlideFM: A Resource-Efficient Foundation Model for \n",
    "         Computational Pathology on Whole Slide Images},\n",
    "  author={Zafar, Sanwal Ahmad and Qin, Wei},\n",
    "  journal={arXiv preprint},\n",
    "  year={2025},\n",
    "  institution={Shanghai Jiao Tong University}\n",
    "}\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "Apache License 2.0 - see [LICENSE](LICENSE)\n",
    "\n",
    "## Contact\n",
    "\n",
    "- **Sanwal Ahmad Zafar** - sanwal@sjtu.edu.cn\n",
    "- **Wei Qin** (Advisor) - Shanghai Jiao Tong University\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\">Made with ❤️ at SJTU Fuxi Lab</p>\n",
    "'''\n",
    "\n",
    "# requirements.txt\n",
    "REQUIREMENTS = '''# OpenSlideFM Requirements\n",
    "torch>=2.0.0\n",
    "torchvision>=0.15.0\n",
    "numpy>=1.21.0\n",
    "pandas>=1.3.0\n",
    "scipy>=1.7.0\n",
    "Pillow>=9.0.0\n",
    "openslide-python>=1.2.0\n",
    "opencv-python>=4.5.0\n",
    "scikit-learn>=1.0.0\n",
    "scikit-image>=0.19.0\n",
    "timm>=0.9.0\n",
    "tqdm>=4.62.0\n",
    "h5py>=3.6.0\n",
    "matplotlib>=3.5.0\n",
    "jupyter>=1.0.0\n",
    "'''\n",
    "\n",
    "# LICENSE\n",
    "LICENSE = '''Apache License 2.0\n",
    "\n",
    "Copyright 2025 Sanwal Ahmad Zafar, Shanghai Jiao Tong University\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "'''\n",
    "\n",
    "# .gitignore\n",
    "GITIGNORE = '''__pycache__/\n",
    "*.py[cod]\n",
    ".ipynb_checkpoints/\n",
    "*.svs\n",
    "*.tiff\n",
    "*.h5\n",
    "*.pt\n",
    "*.pth\n",
    "outputs/\n",
    "logs/\n",
    ".env\n",
    ".venv/\n",
    ".DS_Store\n",
    "'''\n",
    "\n",
    "files_to_upload = {\n",
    "    \"README.md\": README,\n",
    "    \"LICENSE\": LICENSE,\n",
    "    \"requirements.txt\": REQUIREMENTS,\n",
    "    \".gitignore\": GITIGNORE,\n",
    "    \"notebooks/OP_FM.ipynb\": notebook_json,\n",
    "}\n",
    "\n",
    "print(f\"Prepared {len(files_to_upload)} files:\")\n",
    "for f in files_to_upload:\n",
    "    print(f\"  • {f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: Get GitHub Token\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: GitHub Authentication\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nGitHub → Settings → Developer settings → Personal access tokens\")\n",
    "print(\"Generate token with 'repo' scope\\n\")\n",
    "\n",
    "GITHUB_TOKEN = getpass.getpass(\"Enter GitHub Token: \")\n",
    "if not GITHUB_TOKEN.strip():\n",
    "    raise ValueError(\"Token required!\")\n",
    "print(\"✓ Token received\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: Clean Up Old Files\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: Cleaning up old files\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Delete old root-level notebook\n",
    "old_files = [\"OP_FM.ipynb\", \"notebooks/OP_FM.ipynb\"]\n",
    "for old_file in old_files:\n",
    "    print(f\"  Checking {old_file}...\", end=\" \")\n",
    "    if delete_file(GITHUB_TOKEN, old_file):\n",
    "        print(\"✓ cleaned\")\n",
    "    else:\n",
    "        print(\"✗ failed\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: Upload All Files\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 5: Uploading files\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "success = 0\n",
    "failed = 0\n",
    "\n",
    "for filepath, content in files_to_upload.items():\n",
    "    print(f\"  {filepath}...\", end=\" \")\n",
    "    ok, response = upload_file(GITHUB_TOKEN, filepath, content, f\"Add {filepath}\")\n",
    "    if ok:\n",
    "        print(\"✓\")\n",
    "        success += 1\n",
    "    else:\n",
    "        print(f\"✗ {response.get('message', 'error')}\")\n",
    "        failed += 1\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if failed == 0:\n",
    "    print(\"✅ SUCCESS!\")\n",
    "else:\n",
    "    print(f\"⚠️  {success} uploaded, {failed} failed\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f'''\n",
    "🔗 https://github.com/{REPO_OWNER}/{REPO_NAME}\n",
    "\n",
    "📁 Repository Structure:\n",
    "   ├── README.md          (documentation)\n",
    "   ├── LICENSE            (Apache 2.0)\n",
    "   ├── requirements.txt   (dependencies)\n",
    "   ├── .gitignore\n",
    "   └── notebooks/\n",
    "       └── OP_FM.ipynb    ({len(cleaned_cells)} separate cells)\n",
    "\n",
    "The notebook now has {len(cleaned_cells)} properly separated cells that GitHub can render!\n",
    "''')\n",
    "\n",
    "del GITHUB_TOKEN"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}